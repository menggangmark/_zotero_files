Learning via Hilbert Space Embedding of Distributions
by
Le Song
A thesis submitted to The School of Information Technologies
The University of Sydney for the degree of
DOCTOR OF PHILOSOPHY
June 1, 2008

c 2007 Le Song All Rights Reserved

I hereby certify that the work embodied in this thesis is the result of original research and has not been submitted for a higher degree to any other University or Institution.
Le Song Sydney June 1, 2008

To My Parents

Acknowledgements
Many individuals and institutions contributed in many different ways to the completion of this thesis. I am deeply grateful for their support, and thankful for the unique chances they offered to me. These supports include scholarship from National ICT Australia, and travel funding and facilities from the University of Sydney and the Australian National University.
My research has proﬁted a lot from interacting with some of the best researchers in my ﬁeld. I am thankful to all of them: Yasemine Altun, Karsten Borgwardt, Tim Dwyer, Peter Eades, Kenji Fukumizu, Evian Gordon, Arthur Gretton, Seokhee Hong, Bernhard Scho¨lkopf, Alexander Smola, Vishy Vishwanasan and Lea Williams.
I’d also like to thank my fellow students for all the discussions and time. This has helped me gain deeper understanding of the subjects I studied. I am grateful to fellow students from the University of Sydney: Adel Ahmed, Kelvin Cheng, David Fung, Damian Merrick, Kathryn Merrick, Collin Murray, Xiaobin Shen, Yingxin Wu, Ben Yip and Lanbo Zheng; and to fellow students from the Australian National University: Justin Bedo, Quoc Viet Le, Dima Kamenesky, Choonhui Teo, Owen Thomas, Tim Sears, Qingfeng Shi, Jin Yu and Xinhua Zhang.
I especially thank Prof. Peter Eades, Dr. Masahiro Takatsuka and Dr. Soekhee Hong for their advice in the early stage of my PhD studies, and their later generosity to allow me to pursue my own interests.
I feel strongly indebted to my thesis supervisor Prof. Alex Smola. Without his visionary supervision I could never achieve what I have now.
More than to anyone else, I owe to the love and support of my families. Despite all the PhD studies, you are for ever the best of my life.

Contents

1 Introduction 1.1 Thesis Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Thesis Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2 Background 2.1 Kernel Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.1.1 Support Vector Machines . . . . . . . . . . . . . . . . . . . . . . . . 2.1.2 Kernel Tricks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.1 Properties of Kernels . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.2 Example Kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2.3 Reproducing Kernel Hilbert Spaces . . . . . . . . . . . . . . . . . . 2.2.4 Universal Kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Exponential Family Model of Distributions . . . . . . . . . . . . . . . . . . 2.3.1 Deﬁnitions and Basic Facts . . . . . . . . . . . . . . . . . . . . . . . 2.3.2 Examples of Exponential Families . . . . . . . . . . . . . . . . . . . 2.3.3 Marginal Polytope . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Statistical Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.1 Concentration Inequalities . . . . . . . . . . . . . . . . . . . . . . . 2.4.2 Rademacher Average . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5 U-Statistics and V-statistics . . . . . . . . . . . . . . . . . . . . . . . . . . .
3 Hilbert Space Embedding of Distributions 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Mean Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Properties of Mean Map . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Distance between Distributions . . . . . . . . . . . . . . . . . . . . 3.2.3 Choosing the Hilbert Space . . . . . . . . . . . . . . . . . . . . . . 3.3 Learning via Mean Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Two-Sample Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.2 Covariate Shift Correction and Local Learning . . . . . . . . . . . . 3.3.3 Kernels on Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.4 Density Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 Learning via Dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Dependence Measure . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.2 Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.3 Unsupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1 2 5 7
8 8 8 10 11 11 13 13 14 15 16 16 17 18 18 19 21
25 25 25 26 28 29 29 29 32 33 34 36 36 37 39 40

viii

4 Dependence Measure 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Measures of Dependence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 Pearson’s Correlation Coefﬁcient . . . . . . . . . . . . . . . . . . . 4.2.2 Mutual Information . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.3 Kernel Canonical Correlation . . . . . . . . . . . . . . . . . . . . . 4.2.4 Constrained Covariance . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Measure based on Hilbert Space Embedding of Distributions . . . . . . . . . 4.4 Hilbert-Schmidt Independence Criterion (HSIC) . . . . . . . . . . . . . . . . 4.5 Empirical Estimates of HSIC . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.1 Biased Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.2 Unbiased Estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.3 Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.4 HSIC is concentrated . . . . . . . . . . . . . . . . . . . . . . . . . . 4.6 Hypothesis Test using HSIC . . . . . . . . . . . . . . . . . . . . . . . . . . 4.6.1 Asymptotic Normality under H1 . . . . . . . . . . . . . . . . . . . . 4.6.2 Asymptotic Distribution under H0 . . . . . . . . . . . . . . . . . . . 4.7 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.7.1 Independence of Subspaces . . . . . . . . . . . . . . . . . . . . . . 4.7.2 Dependence between Text . . . . . . . . . . . . . . . . . . . . . . . 4.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

41 41 41 42 42 43 45 46 47 49 50 51 52 53 54 55 56 58 58 60 61

5 Feature Selection via Dependence Estimaton 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Criteria for Feature Selection . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Feature Selection Algorithms . . . . . . . . . . . . . . . . . . . . . 5.2 Feature Selection via HSIC . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Backward Elimination Using HSIC (BAHSIC) . . . . . . . . . . . . 5.2.2 Forward Selection Using HSIC (FOHSIC) . . . . . . . . . . . . . . . 5.2.3 Feature Weighting Using HSIC . . . . . . . . . . . . . . . . . . . . 5.3 Variants of BAHSIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.1 Kernels on Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.2 Kernels on Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Connections to Other Approaches . . . . . . . . . . . . . . . . . . . . . . . 5.5 Experiments on Benchmark Data . . . . . . . . . . . . . . . . . . . . . . . . 5.5.1 Artiﬁcial Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.5.2 Public Benchmark Data . . . . . . . . . . . . . . . . . . . . . . . . 5.6 Analysis of Brain Computer Interface Data . . . . . . . . . . . . . . . . . . 5.7 Analysis of Microarray Data . . . . . . . . . . . . . . . . . . . . . . . . . . 5.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

62 62 63 63 64 64 65 66 67 67 68 69 75 76 78 80 82 86

6 Clustering via Dependence Estimation 6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Clustering via HSIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 CLUHSIC Family . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.4 Relation to 0-Extension Problem . . . . . . . . . . . . . . . . . . . . . . . . 6.4.1 0-Extension as CLUHSIC . . . . . . . . . . . . . . . . . . . . . . . 6.4.2 Earthmover Distance . . . . . . . . . . . . . . . . . . . . . . . . . .

87 87 88 89 91 92 92

ix

6.4.3 CLUHSIC using Relaxed 0-Extension . . . . . . . . . . . . . . . . . 93 6.5 Algorithms for CLUHSIC . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
6.5.1 Iterative CLUHSIC . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 6.5.2 Spectral Method for CLUHSIC . . . . . . . . . . . . . . . . . . . . 94 6.5.3 Nonnegative Matrix Factorization . . . . . . . . . . . . . . . . . . . 96 6.6 Learning the Kernel on the Labels . . . . . . . . . . . . . . . . . . . . . . . 97 6.7 Stability under Perturbation . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 6.8 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 6.8.1 Kernel Choice and Stability . . . . . . . . . . . . . . . . . . . . . . 100 6.8.2 Kernel Matrix Approximation . . . . . . . . . . . . . . . . . . . . . 100 6.8.3 Clustering with Rich Label Kernels . . . . . . . . . . . . . . . . . . 102 6.8.4 Learning the Kernel on the Labels . . . . . . . . . . . . . . . . . . . 105 6.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105

7 Dimensionality Reduction via Dependence Estimation 7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Maximum Variance Unfolding . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Colored Maximum Variance Unfolding (MUHSIC) . . . . . . . . . . . . . . 7.4 MUHSIC in the Dual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4.1 Dual Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4.2 Duality and Optimality Conditions . . . . . . . . . . . . . . . . . . . 7.4.3 Ability for Dimensionality Reduction . . . . . . . . . . . . . . . . . 7.5 Efﬁcient Implementations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.5.1 Reduced Semideﬁnite Programming . . . . . . . . . . . . . . . . . . 7.5.2 Reﬁnement via Gradient Descent . . . . . . . . . . . . . . . . . . . 7.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.6.1 Visualization of Three Large Datasets . . . . . . . . . . . . . . . . . 7.6.2 Inﬂuence of the Adjacency Matrices . . . . . . . . . . . . . . . . . . 7.6.3 Inﬂuence of the Local Reﬁnement Step . . . . . . . . . . . . . . . . 7.6.4 Further Comparison to Other Methods . . . . . . . . . . . . . . . . . 7.6.5 Embedding a New Observation . . . . . . . . . . . . . . . . . . . . 7.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

107 107 108 109 110 110 111 112 113 113 114 115 115 119 119 120 121 122

8 Conclusion

124

A Mean and Variance of HSICb under H0

A-2

A.1 Mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-2

A.2 Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A-3

List of Figures

1.1 Structure of this thesis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3

2.1 Effects of kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

3.1 Illustration of the function maximizing the mean discrepancy in the case where a Gaussian is being compared with a Laplace distribution. Both distributions have zero mean and unit variance. The function f that witnesses the difference in feature means has been scaled for plotting purposes, and was computed empirically on the basis of 2 × 104 samples, using a Gaussian kernel with σ = 0.5. 30
3.2 Left: Empirical distribution of the MMD under H0, with Prx and Pry both Gaussians with unit standard deviation, using 50 samples from each. Right: Empirical distribution of the MMD under H1, with Prx a Laplace distribution with unit√standard deviation, and Pry a Laplace distribution with standard deviation 3 2, using 100 samples from each. In both cases, the histograms were obtained by computing 2000 independent instances of the MMD. . . . . . . . 32

4.1 Illustration of the function maximizing the discrepancy between ExEy[f (x, y)] and Exy[f (x, y)]. A sample from dependent scalar random variables x and y is shown in black, and the associated witness function f is plotted as a contour. The latter was computed empirically on the basis of 200 samples, using a Gaussian kernel with σ = 0.2. . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 The cumulative distribution function of HSICb (Emp) under H0 for m = 200, obtained empirically using 5000 independent draws of HSICb. The twoparameter Gamma distribution (Gamma) is ﬁt using α = 1.17 and β = 8.3 × 10−4 in (4.55), with mean and variance computed via Theorems 43 and 44. .
4.3 Top left plots: Example dataset for d = 1, m = 200, and rotation angles θ = π/8 (left) and θ = π/4 (right). In this case, both sources are mixtures
of two Gaussians (source (g) in [61, Table 3]). We remark that the random variables appear “more dependent” as the angle θ increases, although their correlation is always zero. Remaining plots: Rate of acceptance of H0 for the PD, HSICp, and HSICg tests. “Samp” is the number m of samples, and “dim” is the dimension d of x and y. . . . . . . . . . . . . . . . . . . . . . . . . .

48 58
60

xi

5.1 Artiﬁcial datasets and the performance of different methods when varying the number of observations. The ﬁrst row contains plots for the ﬁrst 2 dimension of the (a) binary (b) multiclass and (c) regression data. Different classes are encoded with different colors. The second row plots the median rank (yaxis) of the two relevant features as a function of sample size (x-axis) for the corresponding datasets in the ﬁrst row. The third row plots median rank (yaxis) of the two relevant features produced in the ﬁrst iteration of BAHSIC as a function of the sample size. (Blue circle: Pearson’s correlation; Green triangle: RELIEF; Magenta downward triangle: mutual information; Black triangle: FOHSIC; Red square: BAHSIC. Note that RELIEF only works for binary classiﬁcation.) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 The performance of a classiﬁer or a regressor (vertical axes) as a function of the number of selected features (horizontal axes). Note that the maximum of the horizontal axes are equal to the total number of features in each data set. (a-c) Balanced error rate by a SVM classiﬁer on the binary data sets Covertype (1), Ionosphere (2) and Sonar (3) respectively; (d-f) balanced error rate by a oneversus-the-rest SVM classﬁer on multiclass data sets Satimage (22), Segment (23) and Vehicle (24) respectively; (g-i) percentage of variance not-explained by a SVR regressor on regression data set Housing (25), Body fat (26) and Abalone (27) respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 HSIC, encoded by the colour value for different frequency bands. The x-axis corresponds to the upper cutoff and the y-axis denotes the lower cutoff (clearly no signal can be found where the lower bound exceeds the upper bound). Red corresponds to strong dependence, whereas blue indicates that no dependence was found. The ﬁgures are for subject (a) ‘aa’, (b) ‘al’, (c) ‘av’, (d) ‘aw’ and (e) ‘ay’. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4 Nonlinear kernels (MUL and dis) select genes that discriminate subtypes (red dots and green diamonds) where the linear kernel fails. The two genes in the ﬁrst row are representative of those selected by the linear kernel, while those in the second row are produced with a nonlinear kernel for the corresponding datasets. Different colors and shapes represent data from different classes. (a,d) dataset 18; (b,e) dataset 28; and (e,f) dataset 27. . . . . . . . . . . . . . . . .

77
79 81 84

6.1 Three artiﬁcial datasets and the ﬁrst 2 principal eigenvectors computed from various kernels matrices. Left column, top to bottom: I) Collinear dataset, III) Ring dataset, and V) XOR dataset (data from the same cluster scatter diagonally around the origin). Data points with identical colors and shapes belong to the same class. Right column: eigenvectors computed for the corresponding datasets on the left. The ﬁrst principal eigenvector is colored blue and the second in red. For Colinear, results are for linear and RBF kernels (II). For Ring, results are for RBF and graph kernels (IV). For XOR, results are for graph and polynomial kernels (VI). . . . . . . . . . . . . . . . . . . . . . . . 101
6.2 γ as a function of the amount of perturbation applied to the artiﬁcial datasets in Figure 6.1 using three kernels: RBF kernel (blue circle), graph kernel (red square) and polynomial kernel (green diamond). Results are for Colinear (I), Ring (II), and XOR (III) datasets. . . . . . . . . . . . . . . . . . . . . . . . . 103

xii
6.3 (a) Facial expression images embedded into 3 dimensional space; different marker shape/colour combinations represent the true identity/expression clusters. (b) The two-level hierarchy recovered from the data. . . . . . . . . . . . 103
6.4 Teapot images, 360◦ rotation and clustered (a) by k-means and (b) by CLUHSIC with the label kernel. (c) The schematic diagram showing that the teapot images live in a manifold with a ring structure. . . . . . . . . . . . . . . . . 104
6.5 Learning the kernel for the labels. (a) The kernel matrix for the data. (b) Manually designed kernel matrix for the labels. (c) Learned kernel matrix for the labels. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
7.1 Embedding of 2007 USPS digits produced by MUHSIC, MVU and PCA respectively. Colors of the dots are used to denote digits from different classes. The color bar below each ﬁgure shows the eigenspectrum of the learned kernel matrix K. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
7.2 Embedding of 2000 newsgroup articles produced by MUHSIC, MVU and PCA respectively. Colors and shapes of the dots are used to denote articles from different newsgroups. The color bar below each ﬁgure shows the eigenspectrum of the learned kernel matrix K. . . . . . . . . . . . . . . . . . . . . . . . . 116
7.3 Embedding of 1735 NIPS papers produced by MUHSIC, MVU and PCA. Papers by some representative (combinations of) researchers are highlighted as indicated by the legend. The color bar below each ﬁgure shows the eigenspectrum of the learned kernel matrix K. The yellow triangle in the graph denotes the embedding of a new paper [128] as submitted to NIPS. . . . . . . . . . . 118
7.4 Adjacency matrices of the nearest neighbor graphs for the three datasets. We used the Euclidean distance between the vector space representations of the USPS digits, and the TF.IDF representations of the Newsgroups and NIPS papers datasets. 1% of the data were chosen as nearest neighbors and the graphs were symmetrized subsequently. . . . . . . . . . . . . . . . . . . . . . . . . 120
7.5 Embedding of the three datasets produced by MUHSIC without the reﬁnement via gradient descent (MUHSIC−). Colors of the dots are used to denote digits from different classes. The color bar below each ﬁgure shows the eigenspectrum of the learned kernel matrix K. . . . . . . . . . . . . . . . . . . . . . . 121
7.6 Embedding of the three datasets produced by MVU without the reﬁnement via gradient descent (MVU−). Colors of the dots are used to denote digits from different classes. The color bar below each ﬁgure shows the eigenspectrum of the learned kernel matrix K. . . . . . . . . . . . . . . . . . . . . . . . . . . 121
7.7 Embedding of 2007 USPS digits produced by LDA, RCA and NCA methods. The same color scheme is used as that for MUHSIC. These methods directly learn a 2 dimensional projection, so no eigenspectrum of K is shown. . . . . 122
7.8 The embeddings of the DNA dataset produced by various methods. . . . . . . 123

List of Tables

2.1 Example kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13

4.1 Desirable properties of a dependence measure . . . . . . . . . . . . . . . . . 4.2 Biased estimator of the terms in HSIC. . . . . . . . . . . . . . . . . . . . . . 4.3 Independence tests for cross-language dependence detection. Topics are in the
ﬁrst column, where the total number of 5-line extracts for each dataset is in parentheses. BOW(10) denotes a bag of words kernel and m = 10 sample size, Spec(50) is a k-spectrum kernel with m = 50. The ﬁrst entry in each cell is the null acceptance rate of the test under H0 (i.e. 1 − (Type I error); should be near 0.95); the second entry is the null acceptance rate under H1 (the Type II error, small is better). Each entry is an average over 300 repetitions. . . . .

42 50
61

5.1 Classiﬁcation error (%) or percentage of variance not-explained (%). The best result, and those results not signiﬁcantly worse than it, are highlighted in bold (one-sided Welch t-test with 95% conﬁdence level). 100.0±0.0∗: program is not ﬁnished in a week or crashed. -: not applicable. . . . . . . . . . . . . . .
5.2 Classiﬁcation errors (%) on BCI data after selecting a frequency range. . . . . 5.3 Two-class datasets: classiﬁcation error (%) and number of common genes
(overlap) for 10-fold cross-validation using the top 10 selected features. Each row shows the results for a dataset, and each column is a method. Each entry in the table contains two numbers separated by “|”: the ﬁrst number is the classiﬁcation error and the second number is the number of overlaps. For classiﬁcation error, the best result, and those results not signiﬁcantly worse than it, are highlighted in bold (one-sided Welch t-test with 95% conﬁdence level; a table containing the standard errors is provided in the supplementary material). For the overlap, largest overlaps for each dataset are highlighted (no signiﬁcance test is performed). The second last row summarizes the number of times a method was the best. The last row contains the 2 distance of the error vectors between a method and the best performing method on each dataset. We use the following abbreviations: pc - Pearson’s correlation, snr - signal-to-noise ratio, pam - shrunken centroid, t - t-statistics, m-t - moderated t-statistics, lods - Bstatistics, lin - centroid, dis - ( x − x + )−1, rfe - svm recursive feature elimination) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Multiclass datasets: in this case columns are the datasets, and rows are the methods. The remaining conventions follow Table 5.3. . . . . . . . . . . . .

80 82
85 85

6.1 Clustering error and speed before and after performing incomplete Cholesky decomposition. err1, t1: clustering error and time using the full kernel matrix; err2, t2: clustering error and time using the incomplete Cholesky factor. col#: number of columns in the incomplete Cholesky factor. (m, d): sample size and dimension. c: number of clusters. . . . . . . . . . . . . . . . . . . . . . . . . 101

xv
6.2 Perturbing the images from opposite views such that they confuses k-means clustering. The perturbation is carried out pixel by pixel. The perturbed view is computed from the pixel value of the main view, am, and that of the opposite view, ao, by 0.75 × am + 0.25 × ao. Notice the blurred edge in the perturbed view. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
7.1 Nearest neighbor scores in % for various multiclass datasets produced by various methods. The sizes of the datasets are listed as triples: (size of dataset, number of dimensions, number of classes). We typically used k =1% of the data points as nearest neighbor for MVU and MUHSIC. In the case that the resulting nearest neighbor graph is not connected, we increase the neighbor size to 2%. Furthermore, we typically used the top n =10 eigenvectors of the graph Laplacian as the bases for optimizing MVU and MUHSIC. In the case that the dimension of the data is small (≤ 100), we decrease the number of bases used to 5. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122

Abstract
In this thesis, we propose a framework for learning based on Hilbert space embedding of distributions. By embedding the distributions via the kernel mean map, we are able to compare distributions by computing their distance in the reproducing Hilbert spaces. We show that learning via mean map leads to both good generalization ability and ﬁnite sample convergence.
Using this distance between distributions, we are able to tackle a wide range of learning problems under a common framework. Very often this new view leads to simpler and more effective algorithms in various learning problems. In particular, this thesis focuses on a measure of dependence based on the mean map, and we show that it can tackle a raft of learning problems from which we singled out four concrete examples and discussed in details in the thesis:
• Independence measure and test for structured and heterogeneous data.
• Feature selection via dependence for supervised learning scenario.
• Clustering via dependence with additional metric on labels.
• Dimensionality reduction via dependence with side information.
We also show that learning via Hilbert space embedding/dependence subsumes many existing algorithms as special cases. By elucidating the differences and connections of these algorithms, we are able to provide useful guidelines for practitioners in various applications.
This embedding approach for distribution analysis offers us a principled drop-in replacement for information theoretic approaches. We believe it will have wide applications in near future.

1C H A P T E R
Introduction
Information theoretic approaches [30, 4] have long been dominant in the context of analysis of probability distributions (eg. [123, 132, 83]). For instance, mutual information has been used in a wide range of applications such as independent component analysis [132], feature selection [156] and clustering [123]. Information theoretic approaches, however, have by and large a common issue: to estimate quantities such as entropy or mutual information, usually density estimation is needed as an intermediate step. For a direct estimation of these quantities, sophisticated space partitioning and/or bias correction strategies (eg. [99]) are usually needed. To a certain extent, these drawbacks have limited the practical applicability of information theoretic approaches, especially to data of high dimension and small sample size.
The goal of this thesis is to study kernel methods that sidestep these problems. We will discuss a framework of methods which allow us to compute distances between distributions without the need for intermediate density estimation. Moreover, these methods allow algorithm designers to specify which properties of a distribution are most relevant to their problems. This framework works by embedding distributions into the Hilbert spaces, and it often leads to algorithms which are simpler and more effective than information theoretic methods in a broad range of applications.
At the heart of this embedding approach is the mean map, µ[Prx] := Ex[k(x, ·)], via the kernel k(x, x ). The mean map is attractive since, with universal kernels [131], each distribution can be uniquely represented as an element in the RKHS, and then comparing distributions can be achieved by simply manipulating elements in the RKHS [124]. The major advantages of this embedding approach for distribution analysis are three-fold: ﬁrst, it does not requires intermediate density estimation; second, the empirical mean map has a good convergence guarantee to its population counterpart; and third, prior knowledge can be readily incorporated using kernels. This powerful concept has been exploited successfully for applications such as two-sample test and covariate shift correction [58, 73].
In this thesis, we will use this embedding approach to design a new measure of statistical dependence — we will use the RKHS embedding distance between the joint distribution, Prxy, and the product of its marginals, Prx Pry, as such a measure [124], ie.
2
∆ := µ[Pr] − µ[Pr Pr]
xy x y H
∆ can be used as a principled drop-in replacement for mutual information as a dependence measure and in a wide range of other learning problems.
As a dependence measure, ∆ has many nice properties besides those inherited from the embedding approach. For instance, under the assumption of iid. sampling, it is equivalent to the Hilbert-Schmidt norm of the cross-covariance operator and it has a simple empirical

2 Introduction
estimate m−2 tr(HKHL) which relies only on the kernel matrices K and L for observations x and y respectively. Furthermore, by formulating ∆ as U-statistics, we obtain a novel test statistic for independence. This new statistic is applicable to structured data such as strings and texts. Most interestingly, it also works for data of heterogeneous sources, such as independence between images and text captions.
As a tool for learning, dependence/relevance is also a natural objective to be optimized in many situations. For instance, we may want to pinpoint several genes from a large pool of candidates such that they are most relevant to cancer diagnosis; we may want to label a set of images such that the labels have good relevance to the underlying categories; or we may want to construct a low dimensional vector presentation of documents such that it is most relevant to the coauthor information in the documents. More speciﬁcally, we show that learning via dependence, as expressed by ∆, not only leads to new algorithms but also subsumes many existing algorithms as special cases:
• Feature selection can be formulated as maximizing ∆ between the chosen features and the given labels. Here we propose a greedy backward elimination procedure for this task (BAHSIC) [130]. This procedure is applicable to regression as well as binary and multiclass datasets. A plethora of feature selectors in microarray analysis, such as Pearson’s correlation and signal-to-noise ratio, are special cases of BAHSIC. By showing their connections, BAHSIC provides guidelines on how to choose them (for instance, it helps bioinformaticians on gene selection from microarray data [127]).
• Clustering can be formulated as maximizing ∆ between the generated cluster labels and the data. Here we propose an iterative procedure (CLUHSIC) [129] which applies to novel clustering problems with structured labels (for instance, a hierarchy of facial images). Furthermore, it provides a unifying theory for the geometric (eg. k-means), spectral and statistical view of clustering. This connection also leads to a new perturbation bound for k-means clustering, which has practical implications on how to choose a kernel for clustering tasks.
• Dimensionality reduction can be formulated as maximizing ∆ between the reduced representation of the data and the external goal. Here we propose an algorithm that takes side information into account: it reduces the dimensionality of the data via semideﬁnite programming, while maximizes the dependence between the reduced presentation of the data and the side information (MUHSIC) [128]. Such situation arises, for instance, in text documents where coauthorship is the side information. MUHSIC recovers the popular maximum variance unfolding technique [149] when no side information is available.
Besides the above successes of this embedding approach, we can actually replace information theoretic quantities in many other scenarios as well (for instance, in sensor placement problem [63]). Furthermore, treating various algorithms under a unifying framework and elucidating their connections also beneﬁts practitioners in their speciﬁc applications.
1.1 Thesis Structure
The rest of this thesis will be organized into seven chapters. The lineage of the chapters is shown in Figure 1.1. The main contents of each chapter are summarized below:

1.1 Thesis Structure

3

Figure 1.1: Structure of this thesis.
Chapter 2. Background In this chapter, we will cover background knowledge needed for our later theory development. This includes a brief introduction to kernel methods with Support Vector Machines as a motivating example. Then we collect a list of essential properties of kernels. In particular, we put emphasis on the reproducing kernel Hilbert spaces and universal kernels. These two aspects of kernels will play key roles in our Hilbert space embedding approach for distribution analysis. Then we will present basic concepts from exponential family model of distributions. The exponential families are closely connected to kernel methods: the sufﬁcient statistics in an exponential model can be identiﬁed as feature maps of kernels. This allows us to draw a connection between our embedding approach and the marginal polytope used in graphical models. Next we will present basic techniques from statistical learning theory for studying the concentration of statistics and the complexity of functions. These techniques are useful for analyzing the convergence of the empirical mean map to its population counterpart. Last we will describe two families of statistics, U-statistics and V-statistics. We will design our later tests for two-sample problem and independence based on U-statistics and Vstatistics. Many nice properties of U-statistics and V-statistics can then be transferred directly into our test statistics.
Chapter 3. Hilbert Space Embedding of Distributions In this chapter, we will discuss our Hilbert space embedding approach for distribution analysis in details. We will use the mean map as the key embedding for distribution. We show that the empirical mean map has many nice properties, such as fast convergence to its population counterpart, and one-to-one mapping for distributions when the universal kernels are used. Furthermore, the mean map has the ﬂexibility of choosing a kernel. This allows us to incorporate prior knowledge into the

4 Introduction
analysis of distributions and build a connection with the marginal polytope used in graphical models. Most interestingly, the mean map provides us a principled way for comparing the difference between distributions. This is achieved by computing the Hilbert space embedding distance between the distributions. Based on this distance measure, we can formulate many learning problems under a common framework. For instance, we can perform two-sample tests, covariate shift correction, density estimation and independence tests using the mean maps. In particular, we propose a new dependence measure which spawns a whole branch of methods for various supervised and unsupervised learning problems. In chapter 4, 5, 6 and 7, we will focus on learning via this dependence measure.
Chapter 4. Dependence Measure In this chapter, we will ﬁrst discuss the strengths and weaknesses of four dependence measures from the literature, ie. Pearson’s correlation coefﬁcient, mutual information, kernel canonical correlation and constrained covariance. Then we propose our new dependence measure based on the Hilbert space distance between distributions as a better alternative. This new dependence measure is very general. By choosing the kernels, it can be used for structured datasets and data of heterogenous types. Furthermore, under the assumption of iid. sampling, we recover the Hilbert-Schmidt norm of the cross-covariance operator (HSIC) as a special case. We also derive an unbiased estimator and concentration results for HSIC. By formulating HSIC in term of V-statistics we are able to design powerful yet efﬁcient statistics for testing statistical independence. Our experiments on artiﬁcial data and text documents demonstrate its superior performance.
Chapter 5. Feature Selection via Dependence Estimation In this chapter, we propose a framework for feature selection based on maximizing HSIC. In principle, selecting features using HSIC can be carried out using either forward selection, backward elimination or feature weighting. In this chapter, we will focus on backward elimination (BAHSIC), since it provides the best performance among the three. Selecting features using HSIC has many additional advantages. First, it has a good statistical basis; second, it can be directly applied to various data types such as binary, multiclass and regression by simply using a different kernel; and third, it also subsumes many existing feature selectors as special cases. For instance, Pearson’s correlation, t-statistic and quadratic mutual information are all special cases of HSIC. In a way, we also provide a unifying theory for a raft of feature selectors: they differ only in their respective preprocessing and kernels used. By elucidating their similarity and difference, we are able to provide guidelines on how to choose feature selectors. We have applied BAHSIC to various benchmark datasets, and compared various members of BAHSIC in a large scale microarray studies. We provide rules of thumb for gene selection from microarray data and biological meaningful explanations for gene expression proﬁles.
Chapter 6. Clustering via Dependence Estimation In this chapter, we propose a framework for clustering based on HSIC (CLUHSIC): clustering is to generate the labels for the data such that the dependence as measured by HSIC between the generated labels and the data are maximized. This formulation subsumes k-means clustering as a special case, and hence it provides a connection between the geometric and statistical views of clustering. A distinctive feature of our approach is that a kernel can also be applied on the labels. We show that CLUHSIC in its most general form is equivalent to the 0-Extension problem studied in theoretical computer science. This connection offers the 0-Extension problem a statistical interpretation. Furthermore, it suggests that we can borrow discrete approximation algorithms for 0-Extension

1.2 Thesis Contribution

5

into the clustering community. In this chapter, we also design several heuristics for solving the CLUHSIC problem as well. This includes a greedy approach, a spectral relaxation method and a nonnegative matrix factorization method. By viewing clustering as a dependence maximization process, we are able to derive a new perturbation bound for k-means, which has practical implications such as how to choose a kernel for clustering. In our experiments, we demonstrate interesting clustering results for hierarchical and manifold datasets.

Chapter 7. Dimensionality Reduction via Dependence Estimation In this chapter, we propose an algorithm for dimensionality reduction based on HSIC (MUHSIC). The idea is to maximize the dependence between the reduced data and the side information while at the same time preserve the local distances between the observations. The advantage of this new algorithm is that it can take side information into account in a principled way. When there is no side information, our algorithm reduces to a maximum variance unfolding algorithm. In a way we also provide statistical interpretation for the excellent performance of maximum variance unfolding. Furthermore, we study MUHSIC using convex duality and show that the dual of MUHSIC is to learn the weights of a graph with constraints on the graph Laplacian. This dual problem offers us insights into the ability of MUHSIC for dimensionality reduction. We run our algorithm on various datasets ranging from images to text documents. In either case, by incorporating side information we obtain better interpretable visualizations. The comparisons with other dimensionality reduction methods also show favorable results for MUHSIC.

Chapter 8. Conclusion In this chapter, we summarize the main results in this thesis. We also discuss some future directions for our Hilbert space embedding approach for distribution analysis. These include further connection to graphical models and learning via dependence for non-iid. data.

1.2 Thesis Contribution
This thesis includes conceptual and algorithmic contributions to the ﬁeld of machine learning. These contributions are:
1. A Hilbert space embedding approach for distribution analysis. 2. Measuring statistical dependence based on the Hilbert space distance between the joint
distribution and the product of the marginal distributions. 3. A framework of learning via dependence estimation. 4. Feature selection via dependence. 5. Clustering via dependence. 6. Dimensionality reduction via dependence. The set of publications related to this thesis are listed below: 1. A. Smola, A. Gretton, L. Song and B. Scho¨lkopf. A Hilbert space embedding for distri-
butions. 18th International Conference on Algorithmic Learning Theory, 2007.

6 Introduction
2. A. Gretton, K. Fukumizu, C.H. Teo, L. Song, B. Scho¨lkopf and A. Smola. A kernel statistical test of independence. Advances in Neural Information Processing Systems 20, 2007.
3. L. Song, A. Smola, A. Getton, J. Bedo and K. Borgwardt. Feature selection via dependence maximization. Journal of Machine Learning Researches. (accepted)
4. L. Song, J. Bedo, K. Borgwardt, A. Getton and A. Smola. Gene selection via the BAHSIC family of algorithms. 15th Annual International Conference on Intelligent Systems for Molecular Biology, 2007.
5. L. Song, A. Smola, Arthur Gretton, K. Borgwardt and J. Bedo. Supervised feature selection via dependence estimation. 24th International Conference on Machine Learning, 2007.
6. L. Song, A. Smola, Arthur Gretton and K. Borgwardt. A dependence maximization view of clustering. 24th International Conference on Machine Learning, 2007.
7. L. Song, A. Smola, K. Borgwardt and A. Getton. Colored maximum variance unfolding. Advances in Neural Information Processing Systems 20, 2007.
8. L. Song, X. Zhang, A. Smola, A. Gretton and B. Schoelkopf. Tailoring density estimation via reproducing kernel moment matching. 25th International Conference on Machine Learning, 2008.
9. N. Quadrianto, L. Song and A. Smola. Kernelized sorting. Submitted.
10. X. Zhang, L. Song, A. Gretton and A. Smola. Kernel measures of independence for non-iid data. Submitted.
Besides the above contributions, the author has also carried out research in other areas such as computational neuroscience and information visualization. For the reason of consistency, these contents are not included in this thesis. The corresponding research contributions for neuroscience community include:
1. S. Kuan, J. Gatt, C. Dobson-Stone, D. Palmer, R. Paul, L. Song, E. Gordon, P. Schoﬁeld and L. Williams. A polymorphism of the MAOA gene is associated with emotional brain and behaviour markers of antisocial and psychopathic personality traits. (submitted to the Journal of Neuroscience)
2. L. Williams, D. Palmer, B. Liddell, L. Song and E. Gordon. The ‘when’ and ‘where’ of perceiving signals of threat versus non-threat. NeuroImage, vol 31, pp. 458–467, 2006.
3. L. Song, and J. Epps. Classifying EEG for brain-computer interfaces: learning optimal ﬁlters for dynamical system features. 23rd International Conference on Machine Learning, 2006.
4. L. Song, and J. Epps. Improving the separability of EEG signals during motor imagery with an efﬁcient circular Laplacian. 31st IEEE International Conference on Acoustics, Speech, and Signal Processing, 2006.
5. L. Song, E. Gordon, and E. Gysels. Phase synchrony rate for the recognition of motor imagery in brain-computer interface. Advances in Neural Information Processing Systems 18, 2005.

1.3 Notation

7

6. L. Song. Desynchronization network analysis for the recognition of imagined movement. 27th IEEE International Conference of the Engineering in Medicine and Biology Society, 2005.
and those for information visualization community include:
1. W. Huang, C. Murray, X. Shen, L. Song, Y. X. Wu, and L. Zheng. Visualization and analysis of network motifs. 9th International Conference on Information Visualization, 2005.
2. A. Ahmed, T. Dywer, S.H. Hong, C. Murray, L. Song, and Y.X. Wu. Visualization and analysis of large and complex scale-free networks. 7th IEEE VGTC Symposium on Visualization, 2005.
3. L. Zheng, L. Song and P. Eades. Crossing minimization problems of drawing bipartite graphs in two clusters. 4th Asian-Paciﬁc Symposium on Information Visualization, 2005.
4. A. Ahmed, T. Dywer, S.H. Hong, C. Murray, L. Song, and Y. X. Wu. Wilmascope graph visualization. 10th IEEE Symposium on Information Visualization, 2004.

1.3 Notation

In this thesis, we will deal mainly with vectorial data in Rd where d ∈ N is the dimension of

the space. However, many theories developed in this thesis apply to general Hilbert spaces H.

We will use bold lower case character to denote both a vector in Rd and an element in a Hilbert space. For instance, x ∈ Rd and x ∈ H. For scalars, we will use lower case character. For

instance, m ∈ R and α ∈ R. We will use upper case character X = {x1, . . . , xm} to denote

a set of m observations, and bold upper case character X = (x1, . . . , xm) to denote a matrix formed from m observations. Likewise, we bundle the labels into a matrix Y for yi ∈ Rd, ie.
Y = (y1, . . . , ym) , or y for yi ∈ R, ie. y = (y1, . . . , ym) . We will index the ijth entry in

X by Xij. Also, we will refer to the ith row and column of X as Xi and X i respectively. For

a vector x, we will use x(i) to denote its ith dimension.

Furthermore, we denote the mean and standard deviation of the jth dimension of X by

µj

=

1 m

m i=1

Xij

and s2j

=

1 m

mi=1(Xij − µj)2. For binary classiﬁcation problems we

denote by m+ and m− the numbers of positive and negative observations. Moreover, µj+

and µj− correspond respectively to the means of the positive and negative classes at the jth

dimension (and correspondingly for standard deviations sj+ and sj−). More generally, let

my be the number of samples with class label equal to y (this notation is also applicable to

multiclass problems). Finally, let 1m be a vector of all ones with length m and 0m be a vector

of all zeros. In many cases, we will omit the subscript m. The exact dimension of the 1 and 0

will be clear from the context. When the labels are scalars, we use µy and sy to denote their

mean and standard deviation respectively.

2C H A P T E R
Background
In this chapter, we will introduce essential background knowledge necessary for the development of our later theory: Hilbert space embedding for distribution analysis. Our approach draws resources from kernel methods, functional analysis, exponential family of distributions, statistical learning theory and classical statistical literature.
2.1 Kernel Methods
Many machine learning algorithms can be expressed in term of inner products between observations, x, x , or inner products between observation matrices, XX . For instance, a linear classiﬁer, f (x), can be expressed as i αi xi, x ; A cluster assignment, Π, can be optimized over the objective tr(ΠXX Π ); and dimensionality reduction can be achieved from the principle eigenvectors of XX . A key idea arising from this observation is the so-called kernel trick [115]: wherever inner products are used, they are replaced by kernel functions (or kernels, for simplicity). Kernels can be viewed as nonlinear similarity measures between two observations. This is in contrast to a normal inner product which is a linear similarity measure. The kernel trick can readily convert a linear algorithm into a non-linear one by simply using kernels. Non-linear mappings coded in kernels transform the observations into elements in a higher-dimensional space; thus a linear algorithm in the new feature space is equivalent to a non-linear one in the original space. In other words, kernel methods modularize a learning algorithms. While the kernel components can be changed to incorporate different nonlinearity in the data, the same algorithm remains unchanged irrespective of the kernel used. Such reusability has made kernel methods favorable from a software engineering point of view. This has also led to the popularity of kernel methods in a wide range of applications.
2.1.1 Support Vector Machines
Before we introduce the theory of kernels, we would like to use the Support Vector Machine (SVM) as a motivating example for kernel methods. This simple example aim to reveal the basic principle underlying a large family of kernel learning algorithms. The key message is that a linear algorithm can be lifted to handle nonlinear cases by upgrading it with kernels. For a more extensive treatment of various kernel methods, we refer the reader to [115] and the references therein.
Support Vector Machines (SVMs) deal with the following binary classiﬁcation problem: given a set of observations X = {x1, . . . , xm} (eg. xi ∈ X = Rd, d ∈ N) and their associated labels Y = {y1, . . . , ym} (eg. yi ∈ Y = {−1, 1}) drawn i.i.d. from the space X × Y, the task is to learn a classiﬁer f : X −→ Y that predicts the labels of new observations. A linear SVM parameterizes the classiﬁer as a hyperplane and an offset, ie. f = sign( w, x + b). It

2.1 Kernel Methods

9

tries to separate the observations using this hyperplane while at the same time ensure that the minimum distance between the hyperplane and the observations (the margin γ) is maximized. These constraints lead to the following large margin algorithm for learning f

maximize γ
w,b,γ
subject to ∀i, yi( w, xi − b) ≥ γ, and w 2 = 1.

(2.1)

To see how this constrained optimization problem motivates the kernel method, we ﬁrst derive its dual problem using the Lagrangian below

L = −γ − αi [yi( w, xi − b) − γ] + λ( w 2 − 1), ∀i, αi ≥ 0.
i

(2.2)

The dual problem is equal to infw,b,γ {L}. Thus we differentiate L with respect to the primal variables w, b, γ, and set the derivatives to zeros

∂L = − ∂w

αiyixi + 2λw = 0,

i

∂L = − ∂b

αiyi = 0, and

i

∂L = −1 + ∂γ

αi = 0.

i

(2.3) (2.4) (2.5)

Substituting these relations into (2.2) and then simplifying, we obtain the dual problem

minimize
α

αiαjyiyj xi, xj

i,j

subject to yiαi = 0, αi = 1 and ∀i, αi ≥ 0.
ii

(2.6) (2.7)

Note that this is a convex quadratic optimization problem with respect to the dual variables

αi. To see this, we can express the dual objective in a matrix format, ie. α Aα, where

α = (α1, . . . , αm) and Aij = yiyi xi, xj . Matrix A is positive semi-deﬁnite and equal

to the Hessian of the optimization problem; furthermore, all constraints are afﬁne. Hence the

optimization problem is convex [21].

An interesting characteristic of the dual problem is that the observations are completely

encapsulated inside the inner products x, x . First, the dual objective depends on the observa-

tions only through their inner products. Second, the hyperplane in the classiﬁer is a linear com-

bination

of

the

observations,

ie.

w

=

1 2λ

i αiyixi. Thus both

w, x

=

1 2λ

i αiyi xi, x

and the offset b = yi − w, xi can be expressed as inner products between observations. This

encapsulation provides us nice basis for the kernel trick: we can replace the inner product with

a nonlinear similarity measure between observations, ie. x, x ⇒ k(x, x ). In this case, ma-

trix A has entries Aij = yiyjk(xi, xj). As long as this new measure maintains the convexity

of the underlying optimization problems, ie. A 0, we can use the same algorithm for both

linear and nonlinear problems. As we will discuss later, A 0 is also a sufﬁcient condition

for deﬁning a valid kernel.

10 Background

(a) Original space

(b) Feature space

Figure 2.1: Effects of kernels

2.1.2 Kernel Tricks

In this section, we will illustrate how a kernel affects the behavior of an SVM. As shown in Figure 2.1(a), we have a dataset with observations x ∈ R2. Different colors of the dots represent observations from different classes. A linear classiﬁer (or a line) is unable to perfectly separate the positive classes (red) and the negative classes (blue). One way to separate the classes is to use the ellipsoidal dash line showed in the ﬁgure. The question is how can we obtain the ellipsoidal boundary but still using the same optimization routine? This can be achieved by ﬁrst transforming the observations into linearly separable data and then performing linear classiﬁcation in the new feature space. To il√lustrate this, we apply a nonlinear transformation on x, ie. φ(x) : x −→ z = (x(1)2, x(2)2, 2x(1)x(2)) ∈ R3. In the new feature space (Figure 2.1(b)), the observations from different classes are now linearly separable, and therefore we can use the algorithm for a linear classiﬁer.
As we discussed in section 2.1.1, an SVM applied in the new feature space can be expressed in term of the inner products φ(x), φ(x ) R3. If we simplify this expression, we have

φ(x), φ(x ) R3 = x(1)2x (1)2 + x(2)2x (2)2 + 2x(1)x(2)x (1)x (2)

=

x, x

2 R2

.

(2.8)

The interesting thing is that to compute inner products φ(x), φ(x ) R3, we do not actually need

to construct the nonlinear feature map φ explicitly. This can be done implicitly by simply com-

puting

x, x

2 R2

instead.

This

may

lead

to

signiﬁcant

computational

savings

especially

when

the feature space is large or of inﬁnite dimension. In this example, we call

x, x

2 R2

a

kernel.

It

is a nonlinear similarity measure between two observations. Equivalently a kernel transforms

the observations nonlinearly into a new feature space φ and then compare transformed obser-

vations linearly in the new space. From this example, we also see that a kernel endows a linear

algorithm with extra capacity to handle nonlinear situations. In the next section, we will deﬁne

kernels formally and introduce some related theories.

2.2 Kernels

11

2.2 Kernels

Kernel functions (or kernels) k(x, x ) : X × X −→ R can be viewed as a special type of similarity measure between two observations. Formally, a kernel k(·, ·) is an inner product ·, · H in a feature space H. Given a general set X and two observations x, x ∈ X ,

k(x, x ) := φ(x), φ(x ) H ,

(2.9)

where φ(x) : X −→ H is a nonlinear mapping from an observation to its feature space
representation. In many cases, φ(x) seldom needs to be computed explicitly. This may lead to signiﬁcant computational savings, especially when the feature space H is of much higher dimensions than X . Note that, for a given kernel, the feature map φ(x) is usually not unique. For instance, the kernel x, x 2 can be decomposed as either

√√ (x(1)2, x(2)2, 2x(1)x(2)) , (x (1)2, x (2)2, 2x (1)x (2)) or

(2.10)

(x(1)2, x(2)2, x(1)x(2), x(1)x(2)) , (x (1)2, x (2)2, x (1)x (2), x (1)x (2)) . (2.11)

A natural question is what functions are qualiﬁed as kernels? From the deﬁnition of kernels, we have so far only one way of verifying this: explicitly construct a feature map φ and then test whether the inner product between two images equals to the kernel value. In some cases, however, it may simply be difﬁcult to construct a feature map explicitly. This happens especially when the structure of the data and prior knowledge of a particular application suggest a way of comparing two observations. We want to know whether the function that makes this comparison is a kernel.

2.2.1 Properties of Kernels
Before we describe alternative methods for demonstrating a candidate function is a kernel, we ﬁrst introduce the following two concepts.

Deﬁnition 1 (Gram Matrix) Given a symmetric function k : X × X −→ R and observations X = {x1, . . . , xm} ⊆ X , then the real symmetric m × m matrix K with elements

Kij := k(xi, xj)

(2.12)

for all 1 ≤ i, j ≤ m is called the Gram matrix of k with respect to X.

Deﬁnition 2 (Positive Semi-Deﬁnite Matrix) A real symmetric m × m matrix K satisfying

m
cicjKij ≥ 0
i,j=1

(2.13)

for all ci ∈ R is called positive semi-deﬁnite.

Theorem 3 (Positive Semi-Deﬁnite Kernel) A symmetric function k : X × X −→ R is a positive deﬁnite kernel (or a kernel), if for any ﬁnite set of observations (∀m ∈ N and ∀x1, . . . , xm ∈ X ) it gives rise to a positive deﬁnite Gram matrix.

12 Background

Equivalently this theorem says that, as long as the function k satisﬁes the ﬁnite positive deﬁnite property, it has a decomposition of k(x, x ) = φ(x), φ(x ) H and hence a kernel. Furthermore, kernels have a set of attractive closure properties. This means that certain operations on kernels still yield kernels. These closure operations will not only help us to verify whether a function is a kernel, but also allow us to create new kernels from simple building blocks. Four primitive closure operations are (1) nonnegative offset, (2) positive linear combination, (3) tensor product and (4) limit of kernels, ie.

(1) If k1 is a kernel, and b ≥ 0, then k1 + b is a kernel.

(2) If k1 and k2 are kernels, and α1, α2 ≥ 0, then α1k1 + α2k2 is a kernel.

(3) If k1 and k2 are kernels, then k(x, x ) := k1(x, x )k2(x, x ) is a kernel.

(4) If k1, k2, . . . are kernels, and k(x, x ) := limn→∞ kn(x, x ) exists for all x, x , then k is a kernel.

Based on these primitives, we can derive more complicated closure operations. For instance, a polynomial function f : R −→ R with positive coefﬁcients, ie.

d
f (r) = airi, ∀d ∈ N and ai ≥ 0
i=0

(2.14)

is such an operation, since it only involves positive combination of the products of r. From this operation, we can derive the polynomial kernel k(x, x ) = ( x, x + c)d (c ≥ 0, d ∈ N).
A second example is the exponential kernel k(x, x ) = exp(σ x, x ) (σ > 0). We can verify this by expanding the exponential function exp(σr) around zero, where we obtain an
inﬁnite sum of polynomials with positive coefﬁcients, ie.

exp (σr) = ∞ σi ri. i!
i=0

(2.15)

A third example is to combine Theorem 3 and the closure operations to deﬁne new kernels. For instance,

exp (2σ x, x ) k(x, x ) :=
exp (2σ x, x ) exp (2σ x , x ) = exp −σ x − x 2

(2.16)

is a kernel. First, we know that k1(x, x ) = exp (2σ x, x ) is a kernel according to (2.15); Second, according to Theorem 3, there must exist certain feature space H and the associated feature map φ(x) such that k1(x, x ) = φ(x), φ(x ) H. If we scale the feature map by
k1(x, x) = φ(x) , we have the new feature map φ˜(x) := φ(x)/ φ(x) which is the feature map for k. Since we can explicitly construct a feature map for k, then k is a kernel by
deﬁnition.

2.2 Kernels

13

A fourth example is the inverse square distance kernel deﬁned as

∞
k(x, x ) := exp −σ
0
1 = x−x 2+ ,

x−x 2+ > 0.

dσ

(2.17)

We show that k is a kernel by using the argument that limit of kernels remains a kernel. First, we construct the following sequence k∆ indexed by ∆ > 0

∆
k∆(x, x ) := exp −σ
0 n
= lim exp
n→∞ i=1

x−x 2+ − i∆ x − x
n

dσ 2+

∆ .
n

(2.18)

Note that each k∆ is a kernel, since k∆ by itself is the limit of a sequence of kernels. Second, we take the limit with respect to ∆, then we have k = lim∆→∞ k∆ is also a kernel.

2.2.2 Example Kernels
Besides vectors, kernels have been deﬁned over various domains, such as strings, graphs and dynamical systems (for recent reviews see [119, 117, 71]). These kernels are very often based on prior knowledge or special structure of a particular domain. These non-conventional kernels play a key role in extending the applicability of kernel algorithms to a large variety of data type. Since we will mainly deal with vectorial data in this thesis, we will list a battery of commonly used vectorial kernels in Table 2.1 for our later reference.

Name

Table 2.1: Example kernels Deﬁnition

linear kernel polynomial kernel exponential kernel Gaussian kernel

x, x ( x, x + c)d , c ≥ 0, d ∈ N
exp (σ x, x ) exp −σ x − x 2

Laplace kernel inverse distance kernel inverse square distance kernel
delta kernel

exp (−σ x − x )

1 x−x

+

=

∞ 0

exp

(−σ

(

x−x

+ )) dσ,

>0

1 x−x 2+

=

∞ 0

exp

−σ

x−x 2+

dσ, > 0

1, if x = x , δ(x, x ) =
0, otherwise.

2.2.3 Reproducing Kernel Hilbert Spaces
As we discussed at the beginning of section 2.2, the feature space for a given kernel is usually not unique. There is, however, a special feature space — the reproducing kernel Hilbert spaces

14 Background

(RKHS) — which is unique to a given kernel. We will ﬁrst quote the theorem below before we formally deﬁne the RKHS.

Theorem 4 (Moore-Aronszajn) (Theorem 1.1.1 [59]) Given a kernel k : X × X −→ R, we can construct a unique RKHS with k as its reproducing kernel. Conversely, to every RKHS there corresponds to a unique reproducing kernel k.
In the theorem, the RKHS is simply a Hilbert space H of functions f : X −→ R spanned by the function map k(x, ·), i.e. H = span {k(x, ·)|x ∈ X }. Furthermore, the dot product ·, · H in this space satisﬁes the following reproducing properties

f, k(x, ·) H = f (x) for all f ∈ H and x ∈ X , and consequently k(x, ·), k(x , ·) H = k(x, x ) for all x, x ∈ X .

(2.19) (2.20)

The one-to-one correspondence between k and its associated RKHS is important. This means that by choosing a particular kernel, we uniquely deﬁne the class of functions for subsequent analysis.
The concept of the RKHS is essential in characterizing a special class of kernel, called universal kernels. Universal kernels are intimately related to the consistency of an SVM classiﬁer [131] and is commonly referenced in the rest of the thesis.

2.2.4 Universal Kernels
Now we formally deﬁne continuous kernels and universal kernels below.

Deﬁnition 5 (Continuous Kernel [131]) Let k be a kernel on X with feature map k(x, ·). Then k is called a continuous kernel if and only if k(x, ·) is continuous.

Deﬁnition 6 (Universal Kernel [131]) Let C(X ) be the space of continuous bounded functions on compact domain X . A continuous kernel k on domain X is called universal if the space of all functions induced by k is dense in C(X ), i.e. for every function f ∈ C(X ) and every > 0 there exists a function g induced by k with f − g ∞ ≤ .
Sometimes, it may not be convenient to check the universality of a kernel using the deﬁnition alone. There are other ways to determine whether a kernel is universal according to the following theorems presented in [131].

Theorem 7 A kernel k(x, x ) = k( x, x ) deﬁned on Rd, with power series expansion

∞
k(r) = airi,
i=0

(2.21)

is a universal kernel if and only if for all i, we have ai strictly positive.

Theorem 8 A kernel k(x, x ) = k(x − x ) deﬁned on Rd, with a Fourier transformation

F

[k]

(ω)

=

(2π

)−

d 2

ei ω,r k(r)dr

r∈Rd

(2.22)

is a universal kernel if and only if for all ω, we have the Fourier coefﬁcients strictly positive.

2.3 Exponential Family Model of Distributions

15

Theorem 9 Let k1 be a universal kernel on X . Then the following normalization

k(x, x ) :=

k1(x, x )

k1(x, x)k1(x , x )

(2.23)

deﬁnes a universal kernel on X .

From Theorem 7, we see that the exponential kernel k(x, x ) = exp (σ x, x ) is universal.
Gaussian kernel is a normalized exponential kernel, and therefore it is also universal due to Theorem 9. Note that the above results on universal kernels focus on compact subsets of Rd. For general domain, there is a simple condition [19].

Theorem 10 (Positive Deﬁnite Kernel is Universal) A kernel is universal, if for arbitrary sets of distinct points it induces strictly positive deﬁnite kernel matrices.
Using this theorem, we can show that the following string kernel is universal [19].

Theorem 11 (Universal String Kernel) Let X be a ﬁnite set of strings. Then any string kernel of the form k(x, x ) = a∈X wa#a(x)#a(x ) is universal, where #a(·) computes the number of occurrence of substring a and wa > 0 for all a ∈ X .
For graphs unfortunately no universal kernel exists which are efﬁciently computable. This can be understood via a self-contradiction, as appeared in the reasoning of [52]: ﬁrst, a necessary condition for a kernel to be universal is that its feature map φ(x) has to be injective; second, if such a kernel exists, we can detect the isomorphism between two graphs x and x by simply computing their Hilbert space distance φ(x) − φ(x ) 2 = k(x, x) + k(x , x ) − 2k(x, x ). However, graph isomorphism is an NP-hard problem [51], which means that computing a universal kernel on graphs is also NP-hard.
Universal kernels are important since many results regarding the analysis of distributions are stated with respect to C(X ) and we would like to translate them into results on Hilbert spaces. Two most important properties of universal kernels are [131]
(1) Every universal kernel separates all compact subsets;
(2) Every feature map of a universal kernel is injective.
The ﬁrst property means that, given any ﬁnite subset X = {x1, . . . , xm} ⊆ X , the classes corresponding to every possible label assignment can always be correctly separated by a function in the RKHS. The second property means that a unique observation will be represented as a unique element in the RKHS. In another word, universal kernels preserve the identity of an observation in the RKHS. This second property plays a key role in our later approach for Hilbert space embedding of distribution.

2.3 Exponential Family Model of Distributions
There is a family of distributions that are intimately related to the kernels, called the exponential family. In this family, the potential functions or sufﬁcient statistics can be identiﬁed with the feature maps of the kernels. Furthermore, this family has some other nice properties pertaining to our later theory for Hilbert space analysis of distributions.

16 Background

2.3.1 Deﬁnitions and Basic Facts
Formally, given a kernel k : X × X −→ R with associated reproducing kernel Hilbert space H, the corresponding exponential family model of distributions can be parameterized by θ as

Prx(θ) := exp ( θ, k(x, ·) H − A(θ)) .

(2.24)

Here k(x, ·) is called potential functions or sufﬁcient statistics, and θ ∈ H is the canonical or natural parameter. The quantity A(θ) is called log-partition function or moment generating function. It is deﬁned as

A(θ) := log exp ( θ, k(x, ·) H) dx ,
X

(2.25)

which has the effect of normalizing the distribution properly. Furthermore, the derivatives of A(θ) generate the moments of the random variable k(x, ·). For instance,

∂A(θ) ∂θ

=

Eθ

[k(x, ·)]

:=

k(x, ·) Prx(θ)dx,
X

∂2A(θ) ∂θ2

=

Vθ

[k(x, ·)]

:=

Eθ

[k(x, ·)

⊗

k(x, ·)]

−

Eθ

[k(x, ·)]

⊗

Eθ

[k(x, ·)] ,

(2.26) (2.27)

where ⊗ is the tensor product. Since the Hessian in (2.27) is the variance matrix and hence positive semi-deﬁnite, we know that A(θ) is convex in θ. Very often, A(θ) is also strictly convex: there exists no θ ∈ H and θ = 0 such that θ, k(x, ·) H equal to a constant. In this case, the representation of the exponential family model is called minimal, and each θ is
uniquely associated with a distribution in the family. Based on the log-partition function, we
can also deﬁne the set of valid parameters for an exponential family model

Θ := θ ∈ H A(θ) < ∞ .

(2.28)

Similarly we can also deﬁne conditional distribution of y ∈ Y given x ∈ X , ie. Pry|x(θ). Here we use a joint kernel k : (X × Y) × (X × Y) −→ R to generate the reproducing kernel
Hilbert space H and the sufﬁcient statistics k((x, y), ·), and we have

Pry|x(θ) := exp ( θ, k((x, y), ·) H − Ax(θ)) .

(2.29)

Note that the log-partition function in this case is an integral over the domain Y only, ie.

Ax(θ) := log exp ( θ, k((x, y), ·) H) dy .
Y

(2.30)

2.3.2 Examples of Exponential Families

Many commonly used distributions, such as Bernoulli and Gaussian distribution, are from exponential families. A comprehensive list of exponential families can be found in [27]. Here we use a univariate Gaussian distribution with domain x ∈ R as an example

Prx

=

√1 2πσ

exp

−

(x

− µ)2 2σ2

.

(2.31)

2.3 Exponential Family Model of Distributions

17

Its corresponding potential function is

x,

−

1 2

x2

which gives us the natural parameter θ =

(θ1, θ2) ∈ R × (0, +∞) and the exponential family Prx(θ)

Prx(θ) = exp

θ, x, − 1 x2 2

− A(θ) .

(2.32)

In

this

model,

the

natural

parameter

θ1

and

θ2

will

correspond

to

µ σ2

and

1 σ2

respectively.

The

log-partition function can be computed as

A(θ) = log

+∞
exp
−∞

θ, x, − 1 x2 2

dx

=

1 2

log (2π)

−

1 2

log (θ2)

+

1 2

θ12 . θ2

(2.33)

A second example is the Bernoulli distribution, which corresponds to tossing a coin once with probability p for heads and probability 1 − p for tails. The probability of the outcome can be modeled as

Prx = px(1 − p)1−x, x ∈ {0, 1} .

(2.34)

The exponential family presentation of (2.34) has a simple potential function x with natural parameter θ ∈ R. Thus we have

Prx(θ) = exp ( θ, x − A(θ)) ,

where θ corresponds to log p(1 − p) and the log-partition function is



A(θ) = log 

exp ( θ, x ) = log 1 + eθ .

x∈{0,1}

(2.35) (2.36)

2.3.3 Marginal Polytope
The marginal polytope is the image of the space of all probability distributions P under the expectation of a particular potential function k(x, ·) ∈ H. Formally it is deﬁned as

M := µ[Prx] ∃ Prx ∈ P, µ[Prx] := k(x, ·) Prx dx = Ex [k(x, ·)] .
X

(2.37)

Since the space P is a convex set, then the marginal polytope, constructed from a linear map of Prx ∈ P, is also convex. Furthermore, we also deﬁne a mapping by restricting the expectation to a family of exponential distributions with the same potential function as k(x, ·)

Λ(θ) := k(x, ·) Prx(θ)dx = Eθ [k(x, ·)] ,
X

(2.38)

where Prx(θ) = exp ( θ, k(x, ·) H − A(θ)). Since Prx(θ) ∈ P, then Λ(θ) is also a member of the marginal polytope, ie. Λ(θ) ∈ M. Also note that Λ(θ) is equal to the derivative of the
log partition function, ie. Λ(θ) = ∂A(θ)/∂θ in (2.26). Furthermore, the mapping Λ(θ) has the
following interesting properties [145].

18 Background
Theorem 12 The mapping Λ(θ) : H −→ M is onto the relative interior of M, ie. Λ(θ) = ri(M).
Theorem 13 The mapping Λ(θ) : H −→ M is one-to-one if and only if the exponential representation is minimal.
The signiﬁcance of Theorem 12 is that for any µ ∈ ri(M), there exists an exponential family distribution which is the preimage of µ. Note that normally an exponential family describes only a subset of all distributions, for a given µ there may exist distributions other than a member of the exponential family which are also mapped to µ. The signiﬁcance of Theorem 13 is that for each µ ∈ ri(M), we can ﬁnd a unique member of the exponential family as its preimage. In a sense the mapping µ[Prx] partitions the space of distributions P into sets of equivalent classes, with each class being indexed by a member of the exponential family and mapped to the same µ ∈ M. Due to these nice properties and its dual relation to the entropy functions, marginal polytopes have become a standard tool in deriving efﬁcient algorithms for approximate inference in graphical models [145, 108]. We refer the reader to [145] for more details.

2.4 Statistical Stability
We would like to introduce some useful tools for studying the statistical stability of learning algorithms. It is desirable for a learning algorithm to capture statistically stable patterns from training observations and generalize to future observations. Such behavior can be quantiﬁed by the concentration of random variables characterizing the detected pattern. Basically, a random variable that is concentrated is very likely to assume values close to its expectation and values away from the expectation become exponentially unlikely. We will describe several commonly used concentration inequality in this section for our later use.
Besides concentration inequalities, we will also introduce some basic concepts of a family of statistics, called U-statistics. We can think of U-statistics as a generalization of the sample mean. Typically, U-statistics have nice consistency properties and asymptotical distributions. We will use U-statistics to design statistical tests based on our Hilbert space embedding approach for distribution analysis.

2.4.1 Concentration Inequalities
A common assumption in analyzing the stability of learning algorithms is that observations are drawn independently and identically (iid.) according to a ﬁxed distribution. One of the best-known concentration theorem is due to McDiarmid [92].

Theorem 14 (McDiarmid [92]) Let X = {x1, . . . , xm} be independent random variables taking values in X , and assume that f : X m −→ R satisﬁes

sup |f (x1, . . . , xi, . . . , xm) − f (x1, . . . , xˆi, . . . , xm)| ≤ ci
x1,...,xi,...,xm;xˆi
for all 1 ≤ i ≤ m. Then for all > 0

(2.39)

22

Pr {f (x1, . . . , xm) − EX [f (x1, . . . , xm)] ≥ } ≤ exp −

m i=1

c2i

.

(2.40)

2.4 Statistical Stability

19

Basically, this theorem says that if we can upper bound the variation of a function f with respect to its parameter xi, then function value evaluated at a random sample will not deviate too far away from its expectation. More speciﬁcally, the probability for a deviation of decays at exponential rate, ie. O(exp(− )). Alternatively, given a small probability δ ∈ (0, 1), we know that with probability 1 − δ, the deviation of f from its expectation is bounded by

f (x1, . . . , xm) − EX [f (x1, . . . , xm)] ≤

ln(1/δ)

m i=1

c2i

.

2

(2.41)

As a special case of McDiarmid’s theorem, Hoeffding derived the concentration inequality for the mean of a set of scalar random variables [70].

Theorem 15 (Hoeffding [70]) Let X = {x1, . . . , xm} be independent random variables sat-

isfying

xi

∈

[a, b]

⊂

R,

and

sm

:=

1 m

m i=1

xi

,

then

it

follows

that

Pr {sm − EX [sm] ≥ } ≤ exp

−

2m (b −

2
a)2

.

(2.42)

U-statistics, which we are to introduce in section 2.5, are generalization of the mean statistic. Therefore Theorem 15 also provides a basis for the concentration of U-statistics.

2.4.2 Rademacher Average
Note that both Theorem 14 and 15 apply only to the case of a ﬁxed function f . Learning algorithms typically optimize over a class of functions. Therefore, we also need to characterize their stability when we are allowed to choose from a class of functions. Clearly the more ﬂexible the function class, the more likely we will learn a spurious pattern from the observations. In other words, when we consider the stability, we need to take the complexity of the function class into account. In particular, we will introduce such a complexity measure called Rademacher average [14].

Deﬁnition 16 (Rademacher Average) For a sample X = {x1, . . . , xm} drawn iid. from a distribution Prx on X , and a real-valued function class F with domain X , the empirical Rademacher average of F is the random variable

Rˆm(F ) = Eσ

2m

sup
f ∈F

m σif (xi)
i=1

,

(2.43)

where σ = {σ1, . . . , σm} are independent uniform {±1}-valued (Rademacher) random variables. The corresponding population Rademacher average of F is

Rm(F ) = EX

Rˆm(F )

= EX Eσ

2m

sup
f ∈F

m σif (xi)
i=1

.

(2.44)

The Rademacher average measures the ability of a function class to ﬁt random labels. For instance, for a reproducing kernel Hilbert space H where f H ≤ 1 for all f ∈ H, we have

20 Background

the Rademacher average

Rm(H) = EX Eσ

2m

sup
f H≤1

m σif (xi)
i=1

= EX Eσ

sup
f H≤1

2 f,
m

m

σik(xi, ·)

i=1

2 = m EX Eσ

m
σik(xi, ·)
i=1 H

 

1/2

2m

≤ m EX Eσ 

σiσjk(xi, xj)

 

i,j=1

2 = m EX tr (K) ,

(2.45) (2.46) (2.47) (2.48)

where K is the kernel matrix corresponding to sample X. In step 2.45, we use the reproducing properties of the kernel. In step 2.46, we use the deﬁnition of dual norm. In step 2.47, we use the concavity of the square root. In step 2.48, we use the fact that Eσ [σiσj] vanishes except when i = j.
Intuitively, choosing a function class with low Rademacher average lowers the chance of detecting spurious pattern. Hence the stability of learning algorithms is intimately related to the Rademacher average. For instance, let f be a bounded function, then its empirical average and expected value are tied together via the following inequality [14].

Theorem 17 Let a sample X = {x1, . . . , xm} be drawn from a distribution Prx on X , and F be a function class mapping from X to [0, 1]. Then for all δ > 0, with probability at least 1 − δ, every f ∈ F satisﬁes

Ex

[f (x)]

≤

1 m

m

f (xi) + Rm(F ) +

ln(2/δ) 2m

i=1

≤1 m

m

f (xi) + Rˆm(F ) + 3

ln(2/δ) .
2m

i=1

(2.49) (2.50)

We will include the proof of this theorem in this thesis, since the techniques used in the proof can help us better understand the theory we develop later. The proof is similar as it appears in [14]. Proof For a ﬁxed function f ∈ F, we have

Ex

[f (x)]

≤

1 m

m i=1

f (xi)

+

sup
h∈F

Ex

[h(x)]

−

1 m

m

h(xi)

i=1

.

(2.51)

First we bound the second term in the right hand side of (2.51) using McDiarmid’s inequal-

ity. Since the range of f is within [0, 1], the variation of this term is at most 1/m if we only

vary the value of a single observation. We can then apply the bound we presented in (2.41)

and

set

ci

=

1 m

.

Hence,

given

a

small

probability

δ/2

∈

(0, 1),

we

know

that

with

probability

2.5 U-Statistics and V-statistics

21

1 − δ/2, its deviation from the expectation is bounded by

sup
h∈F

Ex

[h(x)]

−

1 m

m

h(xi)

i=1

≤ EX sup
h∈F

Ex

[h(x)]

−

1 m

m

h(xi)

i=1

ln(2/δ) +,
2m (2.52)

which also means for any f ∈ F

Ex

[f (x)]

≤

1 m

m

f (xi) + EX

i=1

sup
h∈F

Ex

[h(x)]

−

1 m

m

h(xi)

i=1

ln(2/δ) + . (2.53)
2m

Now we bound the middle term in (2.53) by introducing an independent ghost sample X˜ and Rademacher variables σ

EX

sup
h∈F

Ex

[h(x)]

−

1 m

m

h(xi)

i=1

= EX

sup EX˜
h∈F

1 m

m

h(x˜i)

−

1 m

m

h(xi)

i=1 i=1

≤ EX EX˜

sup
h∈F

1 m

m i=1

(h(x˜i)

−

h(xi))

= EX EX˜ Eσ

sup
h∈F

1 m

m i=1

σi

(h(x˜i)

−

h(xi))

≤ 2EX Eσ

1m

sup
h∈F

m σih(xi)
i=1

= Rm(F ).

(2.54) (2.55) (2.56) (2.57)

Note that the ghost sample X˜ is applied in step (2.54). In step (2.55), we use the convexity of sup function. In step (2.56), we use the fact that X and X˜ are samples of the same size drawn
iid. from the same distribution. Hence if we randomly exchange the observations between X and X˜ and average over all possible exchanges, the overall expectation remains the same. This
proves the result in (2.49).
Last, we apply McDiarmid’s inequality again and bound the difference between Rm(F) and Rˆm(F) for probability δ/2. Then we obtain the result in (2.50).

2.5 U-Statistics and V-statistics

Most of the materials in this section can be found in [118]. Suppose we have a sample X = {x1, . . . , xr} of size r drawn iid. from a distribution Prx. U-statistics concern an unbiased estimation of a parameter θ of Prx using X. Suppose there is some function h(x1, . . . , xr) which is an unbiased estimator of θ, ie.

θ = EX [h(x1, . . . , xr)] .

(2.58)

22 Background

h is called a kernel of the estimator.1 For a kernel h that is not symmetric in its arguments, we can always make it symmetric by the following average

1 r! πr h(xi1 , . . . , xir ),

(2.59)

where the summation ranges over all permutations πr of (1, . . . , r). Furthermore, the smallest integer r for which (2.58) holds is called the degree of θ.
When we have a sample X = {x1, . . . , xm} of size m larger than r, we can then construct a U-statistic in the following way.

Deﬁnition 18 (U-statistic) Given a kernel h of degree r and a sample X of size m ≥ r, the corresponding U-statistic for estimation of θ is obtained by averaging the kernel h symmetrically over the observations

1

U := (m)r

imr

h(xi1 , . . . , xir ),

(2.60)

where the summation ranges over r indices drawn without replacement from (1, . . . , m), and (m)r is the Pochhammer symbol

m! m (m)r := (m − r)! = r! r .

(2.61)

Note that in this deﬁnition, we do not require the kernel h to be symmetric in its arguments. The symmetrization is absorbed into the overall summation. If the kernel h is already symmetric, the U-statistics can be equivalently written as

m −1 U= r

h(xi1 , . . . , xir ),

(2.62)

where the summation now is only over all combination of indices drawn from (1, . . . , m). In this case we can drop the normalization by r! due to the symmetrization of the kernel h. Corresponding to a U-statistic, there is the following associated V-statistic.

Deﬁnition 19 (V-statistic) Given a kernel h of degree r and a sample X of size m ≥ r, the V-statistic is

1m

m

V := mr · · · h(xi1, . . . , xir ).

i1=1 ir=1

(2.63)

The difference in a V-statistic is that the summation here is over all indices drawn with replacement from (1, . . . , m). V-statics usually lead to biased estimator of θ.
Many well-known statistics can be expressed as U-statistics. The ﬁrst example is the mean parameter µ of a scalar variable x with distribution Prx. In this case, µ = xd Prx has a

1Note that the kernels for the U-statistics are different from the kernels we used in kernel methods.

2.5 U-Statistics and V-statistics

23

kernel h(x) = x, and the corresponding U-statistic is

1m

U= m

xi = x¯.

i=1

(2.64)

Another example is the variance, ie. σ2 = (x − µ)2d Prx. In this case, the kernel is

h(x1, x2)

=

x21 +x22 −2x1 x2 2

and

the

corresponding

U-statistic

is

2 U = m(m − 1)

1 h(xi, xj) = m − 1

m
x2i − mx¯2 = s2.

1≤i<j≤m

i=1

(2.65)

While the corresponding V-statistic is the biased estimator of the variance, ie.

1 mm

1

V = m2

h(xi, xj) = m

i=1 j=1

m
x2i − mx¯2 .
i=1

(2.66)

As we mentioned in section 2.4.1, U-statistics are generalization of the mean statistic. Therefore, we can also derive the following concentration inequality for U-statistics similarly to Theorem 15 [70].

Theorem 20 (Hoeffding [70]) Let U be a U-statistic as deﬁned in equation (2.60) with kernel h of degree r. Let X be a sample of size m ≥ r. If h ∈ [a, b], then for all > 0

2 m/r 2 Pr {U − EX [U ] ≥ } ≤ exp − (b − a)2 .

(2.67)

Another property of a U-statistic is that it has minimum variance [118].

Theorem 21 Let S be an unbiased estimator of θ based on a sample X = {x1, . . . , xm} drawn iid. from the distribution Prx. Then the corresponding U-statistic is unbiased and has minimum variance, ie.

V[U ] ≤ V[S].

(2.68)

We can further characterize the variance of a U-statistic. Suppose that the kernel h satisfying EX h2(X) < ∞. For 1 ≤ c ≤ r, we deﬁne

hc(x1, . . . , xc) = Exc+1 · · · Exr [h(x1, . . . , xc, xc+1, . . . , xr)]

(2.69)

by taking the expectation of h with respect to the last r − c variables. Furthermore, deﬁne ζ0 = 0, and for 1 ≤ c ≤ r, deﬁne

ζc = V [hc(x1, . . . , xc)] .

(2.70)

Then we have the following characterization of the variance of the U-statistics.

24 Background

Lemma 22 The variance of U is given by

m −1 r r

V [U ] = r

c

c=1

m−r r − c ζc.

(2.71)

With this characterization of the variance, we can examine the asymptotic distribution of a U-
statistic. Depending on whether ζ1 is vanishing or not, the asymptotic distribution is different. For ζ1 > 0, the asymptotic distribution is a Gaussian; while for ζ1 = 0, the asymptotic distribution is an inﬁnite sum of χ2(1) random variables. These results are stated formally in the
following two theorems [118].

Theorem 23 If E h2 < ∞ and ζ1 > 0, then U is asymptotically normal with mean θ and

variance

r2 m

ζ1

,

i.e.

(U

−

θ)

−→

N

(0,

r2 m

ζ1)

a.s.

m −→ ∞.

(2.72)

Theorem 24 If E h2 < ∞ and ζ1 = 0 < ζ2, then

(U − θ) −→

r(r − 1) 2m

∞

λi(χ2i (1) − 1)

a.s.

m −→ ∞.

i=1

(2.73)

where χ2i (1) are iid. χ2(1) random variables, and λi are the solution to the eigenvalue problem

h2(x, x ) − θ ψi(x)d Prx = λiψi(x)
X

(2.74)

and ψi are the corresponding eigenfunctions.

In practice, the inﬁnite sum of the χ2(1) is hard to compute. In this case, the distribution can be approximated via resampling the observations, ﬁtting Pearson curves [58], or ﬁtting a Gamma distribution [60].

3C H A P T E R
Hilbert Space Embedding of Distributions

In this chapter, we will explain a framework for distribution analysis via the mean map. The mean map has attractive properties such as injectivity and fast sample convergence. Furthermore, learning via the mean map also leads to interesting new algorithms.

3.1 Introduction
Kernel methods have become widespread and have gained popularity in the context of supervised learning [115, 107]. However, in the context of testing, estimation, and analysis of probability distributions, information theoretic approaches have long been dominant [30, 4, 123, 99, 83]. Their recent applications include, for instance, construction of graphical models [83], feature extraction [123], and independent component analysis [132]. These methods, however, have by and large a common issue: to estimate quantities such as the mutual information, entropy, or Kullback-Leibler divergence, sophisticated space partitioning and/or bias correction strategies are required [99, 132].
In this section we will introduce kernel methods for estimating distances between distributions without the need for intermediate density estimation. Moreover, they allow algorithm designers to specify which properties of a distribution are most relevant to their problems by using kernels. Very often this embedding approach to distribution representation and analysis leads to algorithms which are simpler and more effective than information theoretic methods in a broad range of applications.

3.2 Mean Map

Suppose we have a kernel k : X × X −→ R with associated reproducing kernel Hilbert space H. At the heart of our approach are the following two mappings

µ[Prx] := Ex [k(x, ·)]

(3.1)

and its empirical estimate

1 µ[X] :=
m

m

k(xi, ·).

i=1

(3.2)

Here X = {x1, . . . , xm} is assumed to be drawn independently and identically distributed (iid.) from Prx. If the (sufﬁcient) condition Ex [k(x, x)] < ∞ is satisﬁed, then µ[Prx] is an element of the Hilbert space (as is, in any case, µ[X]). By virtue of the reproducing property

26 Hilbert Space Embedding of Distributions

of H, we have

f, µ[Prx] = Ex [f (x)] and

1m

f, µ[X] = m

f (xi).

i=1

(3.3) (3.4)

That is, we can compute expectations and empirical means of f with respect to Prx and X, respectively, by taking its inner products with µ[Prx] and µ[X] in the RKHS.
These two mappings embed distributions and their samples into the RKHS. Then natural questions arise: what is the advantage of this approach to distribution representation? what is the ﬁnite sample convergence of the mean map to its population counterpart? and how can we use this approach in the context of machine learning? We will answer these questions in the following sections.

3.2.1 Properties of Mean Map
The representations µ[Prx] and µ[X] are attractive for the following reasons [49, 58].
Theorem 25 If the kernel k is universal, then the mean map µ : Prx −→ µ[Prx] is one-to-one.
A formal proof for this theorem can be found in [58]. Here we will present some intuitive arguments instead. Basically the proof builds upon the connection between the space of continuous functions and the reproducing kernel Hilbert spaces of universal kernels. It contains three key ingredients: ﬁrst, the space of continuous functions separate all distributions [43, Lemma 9.3.2]

Lemma 26 Let Prx and Prx˜ be two probability distribution deﬁned on X . Then Prx = Prx˜ if and only if Ex [f (x)] = Ex˜ [f (x˜)] for all f ∈ C(X ), where C(X ) is the space of continuous bounded functions on X .
Second, this result is translated into one in the reproducing kernel Hilbert spaces H. To do this, we express Lemma 26 as the following form

0 ≡ Ex [f (x)] − Ex˜ [f (x˜)] , ∀f ∈ C(X )

(3.5)

Using the reproducing properties of H in (3.3) and the fact that H is dense in C(X ), we have

0 ≡ f, Ex [k(x, ·)] − Ex˜ [k(x˜, ·)] , ∀f ∈ H

(3.6)

Third, since the optimum is identically zero for all f ∈ H, we know that the two mean maps Ex [k(x, ·)] and Ex˜ [k(x˜, ·)] have to be identical if and only if Prx = Prx˜.
Besides the one-to-one mapping of the mean map, we also have fast convergence of empiri-
cal mean map µ[X] to its population quantity [3, Theorem 15]. Denote by Bm the Rademacher average [14] associated with Prx and f H ≤ 1 via

Bm := EX Eσ

2m

sup
f H≤1

m σif (xi)
i=1

.

(3.7)

The following theorem ensures that µ[X] is a good proxy for µ[Prx], provided the Rademacher average is well behaved [3].

3.2 Mean Map

27

Theorem 27 Assume that f ∈ [0, 1] for all f ∈ H with f H ≤ 1. Given a sample X = {x1, . . . , xm} drawn iid. from the distribution Prx, with probability at least 1 − δ, we have

µ[Prx] − µ[X]

H

≤

2 m Ex

√ tr K

+

ln(2/δ) .
2m

(3.8)

Proof The proof relies on a dual relation between the norm and the following optimization problem

1m

sup Ex
f H≤1

[f (x)]

−

m

i=1

f (xi)

=

=

sup f, µ[Prx] − µ[X]
f H≤1
µ[Prx] − µ[X] H .

(3.9)

While for (3.9), we use the bound presented in Theorem 17. Suppose the optimal solution of (3.9) is denoted as f ∗, then we have

sup Ex
f H≤1

[f (x)]

−

1 m

m i=1

f (xi)

=

Ex

[f ∗(x)]

−

1 m

m i=1

f ∗(xi)

log(2/δ) 2 √

≤ Bm +

2m = m Ex tr K +

(3.10) log(2/δ) .
2m

Note that there is a strong connection between Theorem 27 and uniform convergence results commonly used in Statistical Learning Theory [82, 58]. This is captured in the theorem below

Theorem 28 Let F be a unit ball in the reproducing kernel Hilbert space H, and x1, . . . , xm be drawn iid. from Prx. Then the deviation between empirical means and expectations for any f ∈ F is bounded

sup
f ∈F

Ex

[f (x)]

−

1 m

m

f (xi)

i=1

=

µ[Prx] − µ[X] H .

Bounding the probability that this deviation exceeds some threshold is one of the key problems of statistical learning theory. This means that we have at our disposition a large range of tools typically used to assess the quality of estimators. The key difference is that while those bounds are typically used to bound the deviation between empirical and expected means under the assumption that the data are drawn from the same distribution, we will use the bounds to motivate new learning algorithms. For instance, we can use it to test whether two distributions are the same in Section 3.3.1, and in Sections 3.3.2 and 3.3.4 to motivate strategies for approximating particular distributions.
This is analogous to what is commonly done in the univariate case: the Glivenko-Cantelli lemma allows one to bound deviations between empirical and expected means for functions of bounded variation, as generalized by the work of Vapnik and Chervonenkis [143]. However, the Glivenko-Cantelli lemma also leads to the Kolmogorov-Smirnov statistic comparing distributions by comparing their cumulative distribution functions.

28 Hilbert Space Embedding of Distributions

3.2.2 Distance between Distributions

There are many notions of distance between distributions in the statistical literature (eg. [106, 54]). However, in the machine learning community, the most commonly used measure is the Kullback-Leibler (KL) divergence [30]. The KL divergence between two distributions Prx and Prx˜ is the expected value of their log ratio with respect to Prx, ie.

KL (Prx

Prx˜) := log
X

d Prx d Prx˜

d Prx .

(3.11)

Note that KL divergence is not a metric between distributions, since it is not symmetric in its arguments. Exchanging the order of the arguments results in different values of the KL divergence. Another drawback of KL divergence is that either parametric or nonparametric density estimation has to be performed before its value can be estimated. Recent efforts on estimating the KL divergence have been focused on sophisticated space partitioning and/or biased correction strategies (eg. [99, 103]).
Here we deﬁne a distance measure between distributions Prx and Prx˜, simply by letting

D(Prx, Prx˜) := µ[Prx] − µ[Prx˜] H .

(3.12)

This measure is not new although it has been derived in different ways and comes under differ-

ent names (eg. integral probability metric and probability metric with ζ-structure [159, 97, 96]).

The novelty of our deﬁnition is that we derive the distance from the view of the Hilbert space

embedding of distributions. This new view also allows us to study further properties of the

measure and its applications in machine learning which have not been previously explored.

Theorem 27 tells us that we do not need to have access to actual distributions in order

to

compute

D(Prx, Prx˜)

approximately

—

as

long

as

Bm

=

O(m−

1 2

),

a

ﬁnite

sample

from

the

distributions

will

yield

error

of

O(m−

1 2

).

The

following

theorem

shows

that

the

deviation

between the empirical estimate denoted as D(X, X˜ ) and the population quantity D(Prx, Prx˜)

is bounded.

Theorem 29 Let X and X˜ be samples of size m drawn from distribution Prx and Prx˜ respectively. Then for all δ ∈ (0, 1), with probability at least 1 − δ, we have

D(Prx, Prx˜) ≤ D(X, X˜ ) + Rm(H, Prx) + Rm(H, Prx˜) + 2

ln(2/δ) 2m

≤

D(X,

X˜ )

+

2 m EX

2 tr (K) + m EX˜

tr(K˜ ) + 2

ln(2/δ) .
2m

(3.13) (3.14)

Proof First we simply make use of the deﬁnition of D(Prx, Prx˜) and the triangle inequality

µ[Prx] − µ[Prx˜] H ≤

µ[Prx] − µ[X] H +

µ[X] − µ[X˜ ] +
H

µ[Prx˜] − µ[X˜ ] .
H

(3.15)

Then we apply the bound in Theorem 17 and we prove inequality (3.13) in the theorem. For the second part, we apply bound on the Rachemacher average as presented in (2.48). This completes the proof.

3.3 Learning via Mean Map

29

The above characterization of D(Prx, Prx˜) allows us to use it as a drop-in replacement wherever information theoretic quantities would have been used instead, e.g. for the purpose
of determining whether two sets of observations have been drawn from the same distribution. The advantage is that D(X, X˜ ) requires no density estimation and it is a true distance measure.
The latter advantage follows readily from the fact we are using the RKHS distance to deﬁne
the measure.

3.2.3 Choosing the Hilbert Space
Identifying probability distributions with elements of Hilbert spaces is not new: see e.g. [68]. However, this leaves the question of which Hilbert space to employ. We could informally choose a space with a kernel equal to a Delta function k(x, x ) = δ(x, x ), in which case the operator µ would simply be the identity map (which restricts us to probability distributions with square integrable densities).
The latter is in fact what is commonly done on ﬁnite domains (hence the L2 integrability condition is trivially satisﬁed). For instance, [145] effectively use the Kronecker Delta δ(x, x ) as their feature map. The use of kernels has additional advantages: we need not deal with the issue of representation of the sufﬁcient statistics or whether such a representation is minimal (i.e. whether the sufﬁcient statistics actually span the space).
Whenever we have knowledge about the class of functions F we would like to analyze, we should be able to trade off simplicity in F with better approximation behavior in P. For instance, assume that F contains only linear functions, then we only need to use linear kernels for the analysis. In this case, µ maps P into the space of all expectations of k(x, ·) = x, · . Consequently, one may expect better convergence (in terms of the constants in the deviation bound) of µ[X] to µ[Prx].

3.3 Learning via Mean Map
While the previous description may be of interest on its own, it is in application to areas of statistical machine learning that its relevance becomes apparent. In the following sections, we will provide overviews over several learning algorithms based on Hilbert space embedding of distributions. These algorithms include two-sample test, covariate shift correction, set kernels and density estimation. Furthermore, we will use this embedding approach for distribution analysis to design a new measure of statistical dependence in the next section. This new dependence measure allows us to provide a further framework for various supervised and unsupervised learning problems.

3.3.1 Two-Sample Test

Since we know that µ[X] → µ[Prx] with a fast rate (given appropriate behavior of Bm), we
may compare data drawn from two distributions Prx and Pry, with associated samples X and
Y , to test whether both distributions are identical; that is, whether Prx = Pry. For this purpose, recall that we deﬁned D(Prx, Pry) = µ[Prx] − µ[Pry] H. Using the reproducing property of an RKHS we may show [58] that

D2(Prx, Pry) = Ex,x k(x, x ) − 2Ex,y [k(x, y)] + Ey,y k(y, y ) ,

(3.16)

30 Hilbert Space Embedding of Distributions

Prob. density and f

1 0.8 0.6 0.4 0.2
0 −0.2 −0.4
−6

Witness f for Gauss and Laplace densities
f Gauss Laplace
−4 −2 0 2 4 6
X

Figure 3.1: Illustration of the function maximizing the mean discrepancy in the case where a Gaussian
is being compared with a Laplace distribution. Both distributions have zero mean and unit variance. The function f that witnesses the difference in feature means has been scaled for plotting purposes, and was computed empirically on the basis of 2 × 104 samples, using a Gaussian kernel with σ = 0.5.

where x is an independent copy of x, and y an independent copy of y. An unbiased empirical estimator of D2(Prx, Pry) can be written as a U-statistic [118],

Dˆ 2(X, Y ) :=

1

m(m − 1)

h((xi, yi), (xj, yj)),

i=j

(3.17)

where the kernel of the U-statistic is deﬁned as

h((x, y), (x , y )) := k(x, x ) − k(x, y ) − k(y, x ) + k(y, y ).

(3.18)

An equivalent interpretation, also in [58], is that we ﬁnd a function that maximizes the difference in expectations between probability distributions. The resulting problem may be written

D(Prx, Pry) = sup Ex[f (x)] − Ey[f (y)].
f H≤1

(3.19)

This latter setting in (3.19) lends D(Prx, Pry) the name maximum mean discrepancy (MMD). Furthermore, the maximizer f (witness function) provides us extra insights on how the difference between Prx and Pry is detected. To illustrate this, we plot the witness function f in Figure 3.1, when Prx is Gaussian and Pry is Laplace, using a Gaussian kernel. This function is straightforward to obtain, since the solution to Eq. (3.19) can be written f (x) = µ[Prx] − µ[Pry], k(x, ·) .
The following two theorems give uniform convergence and asymptotic results, respectively. The ﬁrst theorem is a straightforward application of the concentration inequality for U-statistics in Theorem 20 [70, p. 25].

Theorem 30 Assume that the kernel k is nonnegative and bounded by 1. Then with probability

3.3 Learning via Mean Map

31

at least 1 − δ the deviation

|D2(Prx, Pry) − Dˆ 2(X, Y )| ≤ 4 log(2/δ)/m.

(3.20)

Proof In Dˆ 2(X, Y ), the kernel h for the U-statistic has a degree r = 2 and h is bounded between [−2, 2]. Plug these conditions into Theorem 20 we prove the claim.
Note that an alternative uniform convergence bound is provided in [58], based on McDiarmid’s inequality [92]. The second theorem appeared as [58, Theorem 8], and describes the asymptotic distribution of Dˆ 2(X, Y ). When Prx = Pry, this distribution is given by [118, Section 5.5.1]; when Prx = Pry, it follows from [118, Section 5.5.2] and [7, Appendix].

Theorem 31 We assume E h2 < ∞. When Prx = Pry, Dˆ 2(X, Y ) converges in distribution to a Gaussian according to

Dˆ 2(X, Y ) − D2(Prx, Pry)

−→ N

0, σu2 m

a.s. m −→ ∞,

(3.21)

where √

σu2

=

4

Ez (Ez [h(z, z )])2 − Ez,z [h(z, z )] 2

and z := (x, y), uniformly at rate

1/ m. When Prx = Pry, the U-statistic is degenerate, meaning Ez [h(z, z )] = 0. In this

case, Dˆ 2(X, Y ) converges in distribution according to

Dˆ 2(X, Y ) −→ 1 m

∞

λi

gi2 − 2

i=1

a.s. m −→ ∞,

(3.22)

where gi ∼ N (0, 2) i.i.d., λi are the solutions to the eigenvalue equation

k˜(x, x )ψi(x)d Prx = λiψi(x ),
X

(3.23)

and k˜(xi, xj) := k(xi, xj) − Ex [k(xi, x)] − Ex [k(x, xj)] + Ex,x [k(x, x )] is the centered RKHS kernel.

We illustrate the MMD density by approximating it empirically for both Prx = Pry (also called the null hypothesis, or H0) and Prx = Pry (the alternative hypothesis, or H1). Results are plotted in Figure 3.2. We may use this theorem directly to test whether two distributions are identical, given an appropriate ﬁnite sample approximation to the (1 − α)th quantile of (3.22). In [58], this was achieved via two strategies: by using the bootstrap [10], and by ﬁtting Pearson curves using the ﬁrst four moments [76, Section 18.8].
While uniform convergence bounds have the theoretical appeal of making no assumptions on the distributions, they produce very weak tests. We ﬁnd the test arising from Theorem 31 performs considerably better in practice. In addition, [19] demonstrates that this test performs very well in circumstances of high dimension and low sample size (i.e. when comparing microarray data), as well as being the only test currently applicable for structured data such as distributions on graphs. Moreover, the test can be used to determine whether records in databases may be matched based on their statistical properties. In a later chapter of this thesis, we will also apply it to extract features with the aim of maximizing discrepancy between sets of observations.

32 Hilbert Space Embedding of Distributions

Empirical MMD density under H0
50

Empirical MMD density under H1
10

40 8

Prob. density Prob. density

30 6

20 4

10 2

0 −0.04 −0.02 0 0.02 0.04 0.06 0.08 0.1
MMD

0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4
MMD

Figure 3.2: Left: Empirical distribution of the MMD under H0, with Prx and Pry both Gaussians with unit standard deviation, using 50 samples from each. Right: Empirical distribution of the MMD under H1, with Prx a Lapl√ace distribution with unit standard deviation, and Pry a Laplace distribution with standard deviation 3 2, using 100 samples from each. In both cases, the histograms were obtained by computing 2000 independent instances of the MMD.

3.3.2 Covariate Shift Correction and Local Learning

A second application of the mean operator arises in situations of supervised learning where the training and test sets are drawn from different distributions, i.e. X = {x1, . . . , xm} is drawn from Prx and X = x1, . . . , xm is drawn from Prx . We assume, however, that the labels y ∈ R are drawn from the same conditional distribution Pry|x on both the training and test sets. The goal in this case is to ﬁnd a weighting of the training set such that minimizing a reweighted
empirical error on the training set will come close to minimizing the expected loss on the test
set. That is, we would like to ﬁnd weights (β1, . . . , βm) for X with i βi = 1. If Pry|x is a rapidly changing function of x, or if the loss measuring the discrepancy be-
tween y and its estimate is highly non-smooth, this problem is difﬁcult to solve. However,
under regularity conditions spelled out in [73], one may show that by minimizing

m

∆ :=

βik(xi, ·) − µ[X ]

i=1 H

(3.24)

subject to βi ≥ 0 and i βi = 1, we will obtain weights which achieve this task. The idea here is that the expected loss with the expectation taken over y|x should not change too quickly as

a function of x. In this case we can use points xi “nearby” to estimate the loss at location xj on the test set. Hence we are re-weighting the empirical distribution on the training set X such

that the distribution behaves more like the empirical distribution on X .

Note

that

by

re-weighting

X

we

will

assign

some

observations

a

higher

weight

than

1 m

.

This means that the statistical guarantees can no longer be stated in terms of the sample size m.

One may show [73], however, that

β

−2 2

now behaves like the effective sample size.

Instead

of minimizing ∆, it pays to minimize ∆2 + λ

β

2 2

subject

to

the

above

constraints.

It

is

easy

to show using the reproducing property of H that this corresponds to the following quadratic

3.3 Learning via Mean Map

33

program

1 minimize β

(K + λI) β − β

l

β2

subject to βi ≥ 0 and βi = 1.
i

(3.25)

Here

Kij

:=

k(xi, xj)

denotes

the

kernel

matrix

and

li

:=

1 m

m j=1

k(xi,

xj

)

is

the

expected

value of k(xi, ·) on the test set X , i.e. li = k(xi, ·), µ[X ] .

Experiments show that solving (3.25) leads to sample weights which perform very well in

covariate shift. Remarkably, the approach can even outperform “importance sampler” weights,

i.e. weights βi obtained by computing the ratio βi = Prx (xi)/ Prx(xi). This is surprising,

since the latter provide unbiased estimates of the expected error on X .

In the case where X contains only a single observation, i.e. X = {x }, the above proce-

dure leads to estimates which try to ﬁnd a subset of observations in X and a weighting scheme

such that the error at x is approximated well. In practice, this leads to a local sample weighting

scheme, and consequently an algorithm for local learning [20]. The key advantage here, how-

ever, is that we do not need to deﬁne the shape of the neighborhood in which we approximate

the error at x . Instead, this is automatically taken care of via the choice of the Hilbert space H

and the location of x relative to X.

3.3.3 Kernels on Sets
Up to now we used the mapping X −→ µ[X] to estimate the distance between two distributions (or their samples). However, since µ[X] itself is an element of an RKHS we can deﬁne a kernel on sets of objects, X = {x1, . . . , xm} and X = x1, . . . , xm , directly via

1 m,m

k(X, X ) := µ[X], µ[X ] = mm

k(xi, xj).

i,j

(3.26)

In other words, k(X, X ), and by analogy k(Prx, Prx ) := µ[Prx], µ[Prx ] , deﬁne kernels on sets and distributions respectively, and also between sets and distributions analogously. The intuition here is that the unordered objects in a set possesses some common property and can be treated as observations drawn iid. from a distribution. If we have multisets and sample weights for instances we may easily include this in the computation of µ[X]. It turns out that (3.26) is exactly the set kernel proposed by [53], when dealing with multiple instance learning. This can be useful, for instance, for medical applications where an examination or test has been carried out multiple times for a certain patient, and we want to group these data together into a set and treat them as a whole.
We can also deﬁne universal kernels on sets by using universal kernels on the set elements. The key idea here is that each set is embedded as a unique element in the RKHS and hence the kernel on sets inherits all properties of the underlying RKHS. Formally, we have the following corollary:

Corollary 32 If k is universal, given n sets of objects, X1, . . . , Xn, the kernel matrix K where Kij := µ[Xi], µ[Xj] has full rank as long as the sets are not identical.
Similar statement also holds for kernels on distributions. Note, however, that the set/distribution

34 Hilbert Space Embedding of Distributions
kernel may not be ideal for all multiple instance problems: in the latter one assumes that at least a single instance has a given property, whereas for the use of (3.26) one needs to assume that at least a certain fraction of instances have this property.

3.3.4 Density Estimation
We may also use the connection between mean operators and empirical means for the purpose of estimating densities. In fact, [41, 3, 42] show that this may be achieved in the following fashion

maximize H(Prx)
Prx
subject to µ[X] − µ[Prx] H ≤ .

(3.27)

Here H is an entropy-like quantity (e.g. Kullback-Leibler divergence, Csiszar divergence, Bregmann divergence, Entropy, Amari divergence) that is to be maximized subject to the constraint that the expected mean should not stray too far from its empirical counterpart. In particular, one may show that this approximate maximum entropy formulation is the dual of a maximum-a-posteriori estimation problem.
In the case of conditional probability distributions, it is possible to draw connections with many popular estimation algorithms, such as Gaussian Process classiﬁcation, regression, and conditional random ﬁelds. The key idea in this context is to identify the sufﬁcient statistics in generalized exponential families with the map x −→ k(x, ·) into a reproducing kernel Hilbert space.
In problem (3.27) we try to ﬁnd the optimal Prx over the entire space of probability distributions on X . This can be an exceedingly costly optimization problem, in particular in the nonparametric setting. For instance, computing the normalization of the density itself may be intractable, in particular for high-dimensional data. In this case we may content ourselves with ﬁnding a suitable mixture distribution such that µ[X] − µ[Prx] H is minimized with respect to the mixture coefﬁcients. The diagram below summarizes our approach

density Prx −→ sample X −→ empirical mean µ[X] −→ estimate via µ[Prx]. (3.28)

The connection between µ[Prx] and µ[X] follows from Theorem 27. To obtain a density estimate from µ[X] assume that we have a set of candidate densities Prix on X . We want to
use these as basis functions to obtain Prx via

M
Prx = βiPrix,
i

(3.29)

where

M i=1

βi

=

1

and

βi

≥

0.

In

other

words

we

wish

to

estimate

Prx

by

means

of

a

mixture

model with mixture densities Prix. The goal is to obtain good estimates for the coefﬁcients βi

and to obtain performance guarantees which specify how well Prx is capable of estimating Prx

3.3 Learning via Mean Map

35

in the ﬁrst place. This is possible using a very simple optimization problem

2
minimize µ[X] − µ[Prx]
βH
M
subject to βi = 1 and βi ≥ 0.
i=1

(3.30)

where β = (β1, . . . , βM ) . To ensure good generalization performance we can add a regular-

izer

Ω[β]

to

the

optimization

problem,

such

as

1 2

β 2. It follows using the expansion of Prx

in (3.29) that the resulting optimization problem can be reformulated as a quadratic program

via

1 minimize β

(Q + λI) β − l β

β2

M
subject to βi = 1 and βi ≥ 0.

i=1

(3.31)

Here λ > 0 is a regularization constant, and the quadratic matrix Q ∈ RM×M and the vector l ∈ RM are given by

Qij = lj =

µ[Prix], µ[Prjx]

=

Ex∼Prix Ex

∼Prj
x

k(x, x )

and

µ[X], µ[Prjx]

1m = m i=1 Ex ∼Prjx

k(xi, x ) .

(3.32) (3.33)

By construction Q 0 is positive semideﬁnite, hence the quadratic program (3.31) is convex. For a number of kernels and mixture terms Prix we are able to compute Q, l in closed form.
Since Prx is an empirical estimate it is quite unlikely that Prx = Prx. This raises the question of how well expectations with respect to Prx are approximated by those with respect to Prx. This can be answered by an extension of the Koksma-Hlawka inequality [102].

Lemma 33 Let := µ[X] − µ[Prx] . Under the assumptions of Theorem 27 we have that,
H
given δ ∈ (0, 1), with probability at least 1 − δ,

sup
f H≤1

Ex∼Prx [f (x)] − Ex∼Prx [f (x)]

≤ Bm +

Proof Using the reproducing property of H, we have

ln(2/δ) +.
2m

(3.34)

Ex∼Prx[f (x)] = f, µ[Prx] and Ex∼Prx[f (x)] = f, µ[Prx] .

(3.35) (3.36)

Hence the left hand side of (3.34) is equal to sup f H≤1 f, µ[Prx] − µ[Prx] , which is given by the norm µ[Prx] − µ[Prx] . Using triangle inequality, the assumption on µ[Prx],
H
and Theorem 27, we complete the proof.
This means that we have good control over the behavior of expectations of random variables, as

36 Hilbert Space Embedding of Distributions
long as they belong to “smooth” functions on X — the uncertainty increases with their RKHS norm.
The above technique is useful when it comes to representing distributions in message passing and data compression. Rather than minimizing an information theoretic quantity, we can choose a Hilbert space which accurately reﬂects the degree of smoothness required for any subsequent operations carried out by the estimate. For instance, if we are only interested in linear functions, an accurate match of the ﬁrst order moments will sufﬁce, without requiring a good match in higher order terms.

3.4 Learning via Dependence
Another branch of applications of the embedding approach for distribution analysis arises from measuring whether two random variables x and y are independent. Assume that pairs of observations (xi, yi) are jointly drawn from some distribution Prxy. We wish to determine whether this distribution factorizes.
Having a measure of (in)dependence between random variables is a very useful tool in data analysis. One application is in independent component analysis [29], where the goal is to ﬁnd a linear mapping of the observations xi to obtain mutually independent outputs. One of the ﬁrst algorithms to gain popularity was InfoMax, which relies on information theoretic quantities [86]. Recent developments using cross-covariance or correlation operators between Hilbert space representations have since improved on these results signiﬁcantly [11, 59, 61]; in particular, a faster and more accurate quasi-Newton optimization procedure for kernel ICA is given in [120]. In the following we will see that one of the above kernel independence measures can be derived from the mean operators instead.
Besides independent component analysis, a measure of independence can be used for many other learning tasks, such as feature selection, clustering and dimensionality reduction. This connection relies on the fact that many existing learning tasks can be cast into problems of dependence maximization/minimization. We will show that this new view not only provides a unifying framework for many existing algorithms, but also suggests interesting new algorithms such as colored maximum variance unfolding. In the next few sections, we will present overview of this learning via dependence framework. It will serve as a road map for the current and future work. In the following chapters of this thesis, we will single out several examples from this framework and discuss them in details.

3.4.1 Dependence Measure
We begin by deﬁning the Hilbert space embedding of the joint distribution Prxy and the product of its marginals Prx Pry

µ[Prxy] := Exy [v((x, y), ·)] and µ[Prx Pry] := ExEy [v((x, y), ·)] .

(3.37) (3.38)

Here we assumed that V is an RKHS over X × Y with kernel v((x, y), (x , y )). If x and y are dependent, the equality µ[Prxy] = µ[Prx Pry] will not hold. Hence we may use the distance between the two embeddings in the RKHS as a measure of dependence, ie.

∆ := µ[Prxy] − µ[Prx × Pry] V .

(3.39)

3.4 Learning via Dependence

37

Now assume that joint kernel decomposes v((x, y), (x , y )) = k(x, x )l(y, y ), i.e. the RKHS V is a direct product H ⊗ G of the RKHSs on X and Y. In this case it is easy to see that

∆2 =

Exy [k(x, ·)l(y, ·)] − Ex [k(x, ·)] Ey [l(y, ·)]

2 V

= ExyEx y k(x, x )l(y, y ) − 2ExEyEx y k(x, x )l(y, y )

+ ExEyEx Ey k(x, x )l(y, y ) .

(3.40)

The latter, however, is exactly what [59] show to be the Hilbert-Schmidt norm of the crosscovariance operator between RKHSs: this is zero if and only if x and y are independent, for universal kernels. Furthermore, we have the following two results which we are going to discuss in detail in the next chapter:

• Denote by Cxy the cross-covariance operator between random variables x and y with
joint distribution Prxy. Let the functions on X and Y be draw from the reproducing
kernel Hilbert spaces F and G respectively. Then the Hilbert-Schmidt norm Cxy HS of the cross-covariance operator equals ∆.

• Denote by K and L the kernel matrices on X and Y respectively. Moreover, denote by

H

=

I

−

1 m

11

the projection matrix onto the subspace orthogonal to the vector with

all

entries

set

to

1.

Then

1 m2

tr (HKHL)

is

an

estimate

of

∆2

with

bias

O(m−1).

With

high

probability

the

deviation

from

∆2

is

O(m−

1 2

).

Note that if v((x, y), ·) does not factorize we obtain a more general measure of dependence. In particular, we might not care about all types of interaction between x and y to an equal extent, and use an ANOVA kernel (a computationally efﬁcient recursions for computing ANOVA kernel can be found from [25, 142]). More importantly, this representation will allow us to deal with structured random variables which are not drawn independently and identically distributed, such as time series. However, in the following sections, we will focus on the case where observations are drawn iid. and we will show learning via the quantity tr (HKHL) already gives us many useful insights.

3.4.2 Supervised Learning
(Kernel) Fisher Discriminant Analysis We try to project the observations X via Xw such that the dependence between the projected data and the binary labels y are maximized

maximize tr Xww X Hyy H
w
subject to w = 1,

(3.41) (3.42)

where we use linear kernel on the labels and set y as

yi =

1 m+

,

xi

from

positive

class,

−

1 m−

,

otherwise.

(3.43)

In this case, it is equivalent to maximize the mean between positive class and negative class of the projected data, ie. (µ+ −µ−)2. This is nothing but Fisher discriminant analysis. If we carry out the projection in reproducing kernel Hilbert spaces H instead, w can be written as a linear
combination of the observations, ie. w = X α. We then obtain the following kernelized

38 Hilbert Space Embedding of Distributions

version of Fisher discriminant analysis

maximize tr α KHyy HKα
α
subject to α Kα = 1.

(3.44)

(Kernel) Regression The difference between regression and binary case is that regression usually deals with continuous-valued labels. In this case, we can still use the formulation in (3.41), except that we can also apply more complicated kernel on the labels. For instance we can apply a Gaussian kernel on the labels.

Supervised Feature Selection We try to ﬁnd a subset of features in x such that the depen-
dence between the selected features and the labels are maximized. Suppose the full set of feature be T and a subset be S ⊆ T . Let xS be x restricted to the features in S, and the corresponding kernel and kernel matrix be kS(xS, xS) and KS respectively, then feature selection can be cast in the dependence maximization framework in the following way

maximize tr (KSHLH) .
S⊆T

(3.45)

For binary feature selection problem, we can simply use linear kernel and set y as in (3.43). For multiclass problem we can use the delta kernel δy,y for the labels. For regression case, we can simply apply a Gauss kernel on the continuous-valued labels.

Kernelized Sorting problem We try to ﬁnd the relative ordering between two sets of observations such that the dependence between the re-ordered observations is maximized. This is equivalent to maximize the correspondence between two sets of objects, by permuting the order of the objects in one set. Suppose the permutation matrix is Π, the problem can be formulated as

maximize tr Π KΠHLH
Π
subject to Π1 = Π 1 = 1, Πij ∈ {0, 1} .

(3.46)

This is a cleaner way of formulating kernelized sorting in [75]. The optimization problem in this case, however, is a hard quadratic assignment problem in general.

Information Bottleneck We try to recode the observations x into new presentations x˜, by
channel all information through a third variables y [122]. In other words, we try to extract
certain amount of information x˜ from x and this information is maximally relevant to y. Let K, K˜ and L be the corresponding kernel matrix for x, x˜ and y respectively, we can ﬁrst learning the kernel matrix K˜ via the following optimization problem

maximize tr K˜ HLH
K˜
subject to tr K˜ HKH = c, K˜

0.

(3.47)

Then the compressed representation can be calculated from the eigendecomposition of K˜ .

3.4 Learning via Dependence

39

Supervised Dimensionality Reduction In some cases, we have side information for our disposal when we perform dimensionality reduction. Furthermore, we also try to approximately keep the distance between neighboring observations in the original space and reduced space. Suppose the side information is coded in kernel matrix L, the reduced presentation is coded in kernel matrix K, and the distances between the original observations are dij. We can use the following dependence maximization scheme to perform dimensionality reduction

maximize tr (KHLH)
K
subject to Kii − 2Kij + Kjj = d2ij, ∀(i, j) ∈ N ,

(3.48)

where N denotes a nearest neighbor graph and (i, j) ∈ N means observations xi and xj are neighbors (ie. an edge in graph N ).

3.4.3 Unsupervised Learning
(Kernel) Principal Component Analysis We try to project the data X via Xw such that the dependence between the projected data and the original data is maximized. Furthermore, we require that w be unit norm, then we have the following optimization problem

maximize tr w X HXX HXw
w
subject to w = 1,

(3.49)

where C := X HX is exactly the covariance matrix. Thus the solution of the above optimization problem is equal to ﬁnding the eigenvectors of C2. Since the eigenvectors of C2 is the same as those of C, the dependence maximization problem in (3.49) becomes principal component analysis. If we carry out the projection in the RKHS, we then have kernel principal component analysis as in [116].

(Kernel) k-means Clustering and Spectral Clustering We try to generate discrete labels for the observations, such that the dependence between the labels and the observations are maximized. Suppose we have c distinct labels, and the assignment of observations into labels is encoded in an assignment matrix Πm×c. Furthermore, suppose the kernel matrix for the distinct labels be A, then we can cluster the data by optimizing over the assignment

maximize tr HKHΠAΠ
Π
subject to Π 1 = 1, Πij ∈ {0, 1} .

(3.50)

We will show later that if we choose A properly we can actually recover many existing clus-

tering algorithms as special cases. For instance, if A = diag

1 m1

,

.

.

.

,

1 mc

, where mi is the

number of observations assigned to label i, then the problem (3.50) is equivalent to k-means

clustering. With other As, we have a more general clustering algorithm. Furthermore, for

spectral clustering, we basically apply graph kernels for kernel k-means [125].

Unsupervised Feature Selection We try to ﬁnd a subset of features in x such that the dependence between the selected features and the original data are maximized. This problem has been studied by combining Gaussian processes and information theoretic quantities [83].

40 Hilbert Space Embedding of Distributions

Here we will use our dependence measure to formulate this problem instead. To avoid select-
ing correlated features, we also need to encourage independence between the selected features. Suppose the full set of feature be T and a subset be S ⊆ T . Let xS be x restricted to the features in S, and the corresponding kernel and kernel matrix be kS(xS, xS) and KS respectively, then unsupervised feature selection can be cast in the dependence maximization framework in
the following way

maximize
S⊆T

tr (KSHKH)

−

λ

tr K{i}HK{j}H ,

i,j∈S

(3.51)

where λ controls the amount of penalty for correlated features. If we select features incrementally, we can then use simpler expression for such penalization. Suppose the current set of selected features be S, then the next feature to be included can be obtained from

argmax tr KS∪{i}HKH − λ tr K{i}HKSH .
i∈T −S

(3.52)

Dimensional Reduction First, we can use the setting in (3.48) for dimensionality reduction. In unsupervised case, we do not have label information; therefore we set L = I. This corresponds exactly to the maximum variance unfolding approach [149], which seeks a reduced representation that retains maximal diversity between observations.
Other dimensionality reduction techniques, such as locally linear embedding (LLE) and Isomap, are also intimately related to dependence maximization. This connection has been drawn in [66] which shows that LLE and Isomap essentially performs principal component analysis on particular kernel matrices.

3.5 Summary
We have shown that Hilbert space embeddings of distributions are a powerful tool to deal with a broad range of estimation problems, including two-sample tests, covariate shift, kernels on sets, and density estimation. Furthermore, we can also deﬁne useful dependence measure via Hilbert space embeddings. This leads to a unifying framework for various supervised and unsupervised learning problems. In the next few chapters, we will focus on the dependence measure and discuss several concrete examples of this new learning framework. These examples include feature selection, clustering and supervised dimensionality reduction algorithms.

4C H A P T E R
Dependence Measure
In this chapter, we will use the Hilbert space embedding approach to deﬁne a new measure of dependence. We show that under the assumption of iid. sampling, this new measure recovers Hilbert-Schmidt Independence Criterion (HSIC) as a special case. We will also design estimators for HSIC, and study their concentration and asymptotic distribution. Last, we will design statistical tests of independence based on HSIC. We demonstrate the advantages of this new test in our experiments.
4.1 Introduction
The concept of statistical dependence concerns the covariations between two random variables. Intuitively, a random variable is dependent on another random variable, if its variations can be explained by those of another. Measuring statistical dependence is useful in many real world applications. For instance, a dependence measure can help answer questions like whether the price of one stock is linked to another; whether the global temperature is connected to the amount of carbon dioxide emission; or whether the activities of one biological unit is synchronized with those of another.
Measuring statistical dependence has long been a ﬁeld of statistical analysis [110]. Mutual information, for instance, is a well-known dependence measure [30]. Recently, there has been considerable interest in using functions in the RKHS to deﬁne dependence measure. This was ﬁrst started by Bach and Jordan [11], who used a regularized estimate of the spectral norm of the correlation operator between two RKHS (KCC). They showed that this measure signiﬁcantly outperforms traditional measures in independence component analysis. Other work that builds on KCC includes the constrained covariance (COCO) [61]. Instead of the correlation operator, COCO employed the covariance operator. In this chapter, we will introduce a dependence measure based on the Hilbert space embedding distance between distributions. We will also show that, under the assumption of iid. sampling, this measure is equivalent to the squared Hilbert-Schmidt norm of the covariance operator (HSIC) [59]. This measure has many nice properties such as the simple empirical estimate for iid. data and its fast convergence to the population counterpart. Before we further explain these dependence measures, we will explain the concept of statistical dependence in more details.
4.2 Measures of Dependence
Two random variables x and y are statistically independent, if and only if their joint distribution (Prxy) factorizes into the product of their marginal distributions (Prx and Pry), ie. Prxy = Prx Pry. We will denote x ⊥ y if x and y are independent. When the joint distribution

42 Dependence Measure
does not factorize, we call x and y are dependent. To measure the strength of dependence, we usually construct a summary statistic, A(x, y), to quantify the difference between Prxy and Prx Pry. Renyi [110] gave a list of desirable properties for a measure of statistical dependence between random variables, as listed in Table 4.1. In practice, however, it is usually difﬁcult to ﬁnd a measure that satisﬁes all these properties while at the same easy to compute.
Note that, for measuring independence, normally we care most about property (4) and the relative amplitude of the test statistics. Very often we can drop the normalization requirement (3) and (5). For a general measure of dependence, however, all ﬁve properties are desirable. We will present several dependence measures before we deﬁne our new dependence measure.

Table 4.1: Desirable properties of a dependence measure

Property

Notes

(1) A(x, y) is well deﬁned

For all nonconstant variables

(2) A(x, y) = A(y, x)

Symmetric

(3) 0 ≤ A(x, y) ≤ 1

Normalized to range [0, 1]

(4) A(x, y) = 0 ⇔ x ⊥ y Necessary and sufﬁcient condition

(5) A(x, y) = 1 ⇔ y = f (x) or x = g(y) Maximum at deterministic relation

4.2.1 Pearson’s Correlation Coefﬁcient
Pearson’s correlation coefﬁcient measures linear dependence between two univariate random variables x and y. Assume that x and y are linearly related via

y=a·x+b+ ,

(4.1)

where a, b ∈ R and ∼ N (0, σ2) is Gaussian noise with unknown variance. Pearson’s correlation coefﬁcient is deﬁned as

cov(x, y) corr(x, y) =
var(x)var(y)

(4.2)

with the covariance and variance

cov(x, y) = Exy [(x − µx) (y − µy)] , var(x) = Ex [(x − µx) (x − µx)] ,

(4.3) (4.4)

where µx := E[x] and µy := E[y]. Pearson’s correlation is limited in many aspects. It only satisﬁes properties (1), (2) and (3) required for a dependence measure. In term of property (4) and (5), Pearson’s correlation can only measure linear relation between univariate Gaussian random variables, and it also relies heavily on the assumption of Gaussian noise. If these assumptions are violated, Pearson’s correlation may not be able to detect the dependence.

4.2.2 Mutual Information
Mutual information I(x, y) is a more general measure of dependence and does not rely on any model assumption. It is deﬁned via the Kullback-Leibler divergence between the joint

4.2 Measures of Dependence

43

distribution Prxy and the product Prx Pry of its marginals [30]

I(x, y) :=

log

X ×Y

d Prxy d Prx d Pry

d Prxy .

(4.5)

By convention, 0 · log

0 0

= 0 such that mutual information is well-deﬁned (property (1)).

Furthermore, mutual information is symmetric in its arguments, and it is a necessary and suf-

ﬁcient condition for independence (property (2) and (4)). Mutual information, however, is not

normalized to the range [0, 1]. It is maximized when there exists deterministic relation between

x and y. The maximum is the entropy of either x or y which is in general not equal to 1.

The deﬁnition of mutual information does not rely on any assumption on the type of distri-

bution or the type of dependence. We expect it to have wide applicability in theory. In practice,

however, such generality also brings problems. The distribution of the random variables are

usually not easy to estimate especially for high dimensional data. Furthermore, such generality

is usually not necessary in many cases. For instance, if we want to investigate whether second

order dependence exists between random variables x and y (suppose both variables are drawn

from Rd). For mutual information, we still need to estimate both the joint and marginal distri-

bution before we can assess the strength of the dependence. In this case, all information needed

for testing their dependence will be contained in the cross-covariance matrix

Cxy := Exy[xy ] − Ex[x]Ey[y ].

(4.6)

If there exists no second order dependence between x and y, then all entries in the crosscovariance matrix should be zero. In this case, we can simply use the Frobenius norm of Cxy to summarize the degree of dependence between x and y. Alternatively, given the singular values σi of Cxy, we can compute it via

Cxy

2 Frob

=

tr

Cxy Cxy

=

σi2.

i

(4.7)

This quantity is zero if and only if there exists no second order dependence between x and y. From this example, we see that, by incorporating prior knowledge, dependence estimation becomes much easier; we can sidestep density estimation and still obtain an evaluation of the dependence.
The statistic in (4.7) is limited in several respects, however, of which we mention two: ﬁrst, dependence can exist in forms other than that detectable via covariance (and even when a second order relation exists, the full extent of the dependence between x and y may only be apparent when nonlinear effects are included). Second, the restriction to subsets of Rd excludes many interesting kinds of variables, such as strings and class labels. Therefore, recent research on dependence measure tries to generalize the notion of covariance (or correlation) to nonlinear relationships, and to a wider range of data types. In the following sections, we will ﬁrst describe two kernel measures of dependence before we introduce our new measure based on Hilbert space embedding of distributions.

4.2.3 Kernel Canonical Correlation
The idea of kernel canonical correlation dates back to Renyi [110]. He states that x and y are independent if and only if cov(f (x), g(y)) = 0 for any bounded continuous function f and g, ie. f ∈ C(X ), g ∈ C(Y). Kernel canonical correlation (KCC) specializes this result by

44 Dependence Measure

considering functions in the RKHS. Formally, let k and l be kernels for random variables x and y respectively, and their associated reproducing kernel Hilbert spaces be F and G respectively. The kernel canonical correlation between x and y is deﬁned as

KCC := =

sup corr ( f, k(x, ·) , g, l(y, · )
f F ≤1, g G ≤1
cov ( f, k(x, ·) , g, l(y, ·) ) sup . f F ≤1, g G≤1 var ( f, k(x, ·) ) var ( g, l(y, ·) )

(4.8) (4.9)

KCC is well-deﬁned, symmetric with respect to x and y, and normalized to range [0, 1] (prop-

erty (1), (2) and (3)). The type of dependence that can be detected by KCC is intimately related

to the kernels chosen. As we discussed in the Chapter 2, the kernels uniquely determines the

RKHS and hence the class of induced functions. If we are only interested in second order

dependence, we can simply use linear kernels for both k and l. Linear kernels deﬁne the space

of linear functions which is a subspace of C(X ). If the set of functions induced by the RKHS

is dense in C(X ), KCC is guaranteed to detect dependence of any kind [11]. In particular,

universal kernels satisfy this condition and have the ﬂexibility to detect dependence of any

kind.

To compute KCC, we ﬁrst center the observations in the RKHS, ie. k˜(xi, ·) := k(xi, ·) −

m i=1

k(xi,

·),

and

observations, ie. f

then =

consider the functions in

m i=1

αik˜(xi,

·)

and

g

=

the RKHS which lie in the span of the ﬁnite

m i=1

βi˜l(yi,

·).

The

empirical

estimate

of

the

covariance and variance can be computed as

cov f, k˜(x, ·) , g, ˜l(y, ·)


mm
=  αj
i=1 j=1
=α K˜ L˜β,

k˜(xj, ·), k˜(xi, ·)

m
βj
j=1

 ˜l(yj, ·), ˜l(yi, ·) 

(4.10)

and similarly var f, k˜(x, ·) = α K˜ K˜ α. Note that K˜ and L˜ are the kernel matrices for
the centered observations. They can be readily computed from the kernel matrices K and L via

K˜ = HKH and L˜ = HLH,

(4.11)

where

H

is

the

centering

matrix

deﬁned

as

H

=

I

−

1 m

11

. Therefore, the empirical estimate

of KCC is equal to

KCC = sup
α,β∈Rm

α K˜ L˜β

.

α K˜ 2α β L˜ 2β

(4.12)

However, a drawback of KCC is that, for ﬁnite number of observations, KCC can poten-
tially over-ﬁt the data. To show this, we re-parameterize the KCC optimization problem using u = K˜ α ∈ U and v = L˜β ∈ V where U and V denote the subspaces of Rm generated by the

4.2 Measures of Dependence

45

column space of K˜ and L˜ respectively. Then we get

KCC = sup

uv = sup cos(u, v),

u∈U,v∈V (u u)(v v) u∈U,v∈V

(4.13)

which is exactly the cosine of the angle between the two subspaces U and V. If both K and L have full rank, the subspaces spanned by their columns become identical, ie. Rm. This situation
happens, for instance, when we use universal kernels for computing KCC. The centering
matrix only projects out the subspaces spanned by the vector of all ones, and the remaining subspaces spanned by K˜ and L˜ will still coincide. Therefore we will always obtain a value of
1 for KCC. To avoid this, we need to consider the following regularized version of KCC

KCC = sup
α,β∈Rm

α K˜ L˜β

.

α

K˜ +

m 2

I

2
α

β

L˜

+

m 2

I

2
β

(4.14)

KCC can be equivalently formulated as the following constrained optimization problem

KCC = maximize α K˜ L˜β
α,β∈Rm

subject to α

K˜ + m

I

2
α=1

and

β

2

L˜ + m

I

2
β = 1.

2

(4.15)

we can obtain the solution for α and β as the principal eigenvector of the following generalized eigenvalue problem

0 K˜ L˜ L˜K˜ 0

α β

 = λ

K˜

+

m 2

I

2

0



0α

L˜ +

m 2

I

2

β

.

(4.16)

Thus KCC is equal to the largest eigenvalue. The drawback of KCC is that it is not easy
to tune the regularization parameter . Normally, as the number of observations increases,
1
must approach zero at rate O(m 3 ) to ensure the consistency of the estimated KCC, and of the
associated maximizers f and g [48]. Furthermore, computation of the generalized eigenvalue
problem is costly especially when the number of observations are large.

4.2.4 Constrained Covariance
Constrained covariance (COCO) gears towards the test of independence, and it only satisﬁes properties (1) (2) and (4). For this purpose, COCO drops the normalization by variances and uses only the functional covariance. Keeping the notations for kernel canonical correlation, we have the deﬁnition for COCO [61]

COCO := sup cov ( f, k(x, ·) , g, l(y, ·) ) .
f F ≤1, g G ≤1

(4.17)

From the deﬁnition we can see that COCO is well-deﬁned and symmetric. Furthermore, COCO

is zero if and only if x and y are independent [61]. For ﬁnite number of observations, both f

and g lie in the span of the observations in the RKHS, ie. f =

m i=1

αik˜(xi,

·)

and

g

=

46 Dependence Measure

m i=1

βi˜l(yi,

·).

We

compute

the

empirical

estimate

of

COCO

as

[61]

COCO = maximize

1 α

K˜ L˜β

α K˜ α≤1,β L˜β≤1 m

= max

1 α

K˜ 1/2L˜ 1/2β

α α ≤1,β β≤1 m

1 =

K˜ 1/2L˜ 1/2

,

m2

(4.18)

where we have made a replacement of variables as α = K˜ 1/2α, β = L˜1/2β. The matrices K˜ 1/2 and L˜1/2 are the matrix square roots of K˜ and L˜ respectively, and · 2 returns the largest singular value of its argument.
The advantage of COCO over KCC is that COCO requires no regularization parameter.
This avoids the additional model selection problem in KCC. It has been shown that COCO
achieves better performance than KCC when used as a contrast function in ICA. The drawback of COCO is that we need to compute the square root of K˜ and L˜. If the number of observations is large, this operation is very costly since it requires full eigen-decomposition of K˜ and L˜.

4.3 Measure based on Hilbert Space Embedding of Distributions

In this section, we will introduce our new dependence measure based on Hilbert-space embedding of distributions. Like COCO, this new measure only satisﬁes property (1), (2) and (4). However, it is computationally much more efﬁcient than COCO. The key idea is that we embed both the joint distribution and the product of the marginal distributions into the RKHS and then compare their embedding distance in the RKHS. Given two random variables x and y, we begin by deﬁning the following two embeddings

µ[Prxy] := Exy [v((x, y), ·)] and µ[Prx Pry] := ExEy [v((x, y), ·)] .

(4.19) (4.20)

Here we assume that V is an RKHS over X × Y with joint kernel v((x, y), (x , y )). If x and
y are independent, the equality µ[Prxy] = µ[Prx Pry] should hold. Hence we may use their distance, ∆ := µ[Prxy] − µ[Prx Pry] V , as a measure of dependence. Expanding ∆2 using the kernel, we have

∆2 =

Exy [v((x, y), ·)] − ExEy [v((x, y), ·)]

2 V

= ExyEx y v((x, y), (x , y )) − 2ExEyEx y

+ ExEyEx Ey v((x, y), (x , y )) .

v((x, y), (x , y ))

(4.21)

Alternatively, ∆ can be deﬁned via an optimization problem in the RKHS

∆ = sup Exy[f (x, y)] − ExEy[f (x, y)],
f V ≤1

(4.22)

where we try to ﬁnd a function f inside a unit ball in the RKHS, such that the difference between the expected value of f under the joint distribution and that under the product of the marginals are maximized. In essence, f can be treated as a classiﬁer between matching pair of observations (x, y) and randomly assembled pairs (x, y ). If the classiﬁer can distinguish

4.4 Hilbert-Schmidt Independence Criterion (HSIC)

47

them better than random, then there exists some dependence between x and y. The maximizer f is called the witness function in resemblance to the two-sample test in section 3.3.1. An illustration of the witness function is provided in Figure 4.1. We observe that this is a smooth function which has large magnitude where the joint distribution is most different from the product of the marginals.
In some cases, the joint kernel v((x, y), (x , y )) factorizes into the product of two kernels, ie. v((x, y), (x , y )) = k(x, x )l(y, y ). In this case, the RKHS V is a direct product H ⊗ G of the RKHSs on X and Y. Then we can compute ∆2 as

∆2 =

Exy [k(x, ·)l(y, ·)] − Ex [k(x, ·)] Ey [l(y, ·)]

2 V

= ExyEx y k(x, x )l(y, y ) − 2Exy Ex k(x, x ) Ey

+ ExEx k(x, x ) EyEy l(y, y ) .

l(y, y )

(4.23)

We will show later that ∆2 is equal to the squared Hilbert-Schmidt norm of the cross covariance operator between space X and Y. Therefore, this measure is also called Hilbert-Schmidt Independence Criterion (HSIC) [59]. In some cases, such as using a Gaussian RBF kernel, the empirical estimate of HSIC can also be interpreted as a smoothed difference between the joint empirical characteristic function (ECF) and the product of the marginal ECFs [45, 77]. However, this interpretation does not hold in all cases which include kernels on strings, graphs, and other structured spaces.
Note that if v((x, y), ·) does not factorize we obtain a more general measure of independence. In particular, we might not care about all types of interaction between x and y to an equal extent, and use an ANOVA kernel [25, 142]. More importantly, this representation will allow us to deal with structured random variables which are not drawn independently and identically distributed, such as time series. For instance, we can deﬁne the following joint feature map for time series x and y which takes the dependency between adjacent observations into account

k(xt−1, ·) ⊗ k(xt, ·)  v((x, y), ·) =  k(xt, ·) ⊗ l(yt, ·) .
l(yt−1, ·) ⊗ l(yt, ·)

(4.24)

Another example is for the case of EEG (electroencephalogram) data where we have both spatial and temporal structure in the signal. Few algorithms take full advantage of this when performing independent component analysis [8]. The pyramidal kernel of [113] is one possible choice for measuring dependence in this structured case.
However, many theoretical questions related to dependence measure in structured case have not been settled yet: for instance, what is the estimator, whether we have good convergence of empirical estimator to population counterpart, and how to perform learning via dependence in structured case. In the following sections, we will only discuss the case for iid. data. In this case, we show that the dependence measure is equivalent to the Hilbert-Schmidt norm of the cross-covariance operator (HSIC). We will develop further theory for HSIC in the rest of this chapter and present examples of learning via HSIC in the following chapters.

4.4 Hilbert-Schmidt Independence Criterion (HSIC)
In this section, we will start with the cross-covariance operator, a generalization of the crosscovariance matrix. In accordance with [12, 49], this is a linear operator Cxy : G −→ F such

48 Dependence Measure

Figure 4.1: Illustration of the function maximizing the discrepancy between ExEy[f (x, y)] and Exy[f (x, y)]. A sample from dependent scalar random variables x and y is shown in black, and the associated witness function f is plotted as a contour. The latter was computed empirically on the basis of 200 samples, using a Gaussian kernel with σ = 0.2.

that1

Cxy := Exy [(k(x, ·) − µx) ⊗ (l(y, ·) − µy)] = Exy [k(x, ·) ⊗ l(y, ·)] − µx ⊗ µy,

(4.25)

where µx = Ex[k(x, ·)] and µy = Ey[l(y, ·)]. Here ⊗ denotes the tensor product. For f ∈ F and g ∈ G, a tensor product is a linear operator f ⊗ g : G −→ F formally deﬁned as

(f ⊗ g) h := f g, h G for all h ∈ G.

(4.26)

Earlier we used Frobenius norm of the cross-covariance matrix to measure second order dependence. Here we need to extend the notion of the Frobenius norm to operators. This leads us to the Hilbert-Schmidt (HS) norm of a linear operator C : G −→ F, ie. provided the sum converges,

C

2 HS

:=

Cvi, uj

2 F

,

ij

(4.27)

1Again we abuse the notation here by using the same subscript in the operator Cxy as in the covariance matrix of (4.6), even though we now refer to the covariance between feature maps.

4.5 Empirical Estimates of HSIC

49

where uj and vi are orthonormal bases of F and G respectively. For operators with discrete spectrum this amounts to computing the 2 norm of the singular values. A linear operator is called Hilbert-Schmidt operator if its Hilbert-Schmidt norm exists. The set of Hilbert-Schmidt operators C : G −→ F also form a Hilbert space H with inner product deﬁned as

C, D H :=

Cvi, uj F Dvi, uj F .

ij

(4.28)

Then we can compute the HS norm of a tensor product operator as

f ⊗g

2 HS

=

f ⊗ g, f ⊗ g H =

f, (f ⊗ g) g F

=

f, f F

g, g G =

f

2 F

g

2 G

.

(4.29)

We use the squared Hilbert-Schmidt norm of the cross-covariance operator (HSIC),

Cxy

2 HS

as our dependence measure. In terms of kernels, HSIC can be expressed as [59]

HSIC :=

Cxy

2 HS

=

Cxy, Cxy H

= Exy [ k(x, ·) ⊗ l(y, ·), k(x, ·) ⊗ l(y, ·) H] − 2Exy µx ⊗ µy, k(x, ·) ⊗ l(y, ·) H

+ µx ⊗ µy, µx ⊗ µy H = ExyEx y [k(x, x )l(y, y )] − 2Exy[Ex [k(x, x )]Ey [l(y, y )]]

+ ExEx [k(x, x )]EyEy [l(y, y )].

(4.30)

Note that the expression in (4.30) is the same as the one in (4.23). This means HSIC can also be interpreted as the squared distance between the Hilbert space embedding of Prxy and Prx Pry. Furthermore, the above expression involves only the expectations over two kernel functions k and l. This is in contrast to the estimation of mutual information where density estimation is usually needed.

HSIC detects arbitrary dependence A nice property of HSIC is that with suitable kernels HSIC = 0 if and only if x and y are independent. Hence HSIC is capable of detecting dependence of any kind. This results is proved in Theorem 4 of [59].
Theorem 34 (HSIC and Independence) Let F, G be RKHSs with universal kernels k, l on respective compact domains X and Y in the sense of [131], then HSIC = 0 if and only if x and y are independent.
Note that non-universal kernels can also be used for HSIC, although they may not guarantee that all dependence is detected. Different kernels incorporate distinctive prior knowledge into the dependence estimation, and they focus HSIC on dependence of a certain type. For instance, a linear kernel requires HSIC to seek only ﬁrst order dependence, whereas a polynomial kernel of degree b restricts HSIC to test for dependences of degree (up to) b. Clearly HSIC is capable of ﬁnding and exploiting dependence of a much more general nature using kernels on graphs, strings, or other discrete domains.

4.5 Empirical Estimates of HSIC
We denote by Z = (X, Y ) the set of observations {(x1, y1), . . . , (xm, ym)} drawn iid. from Prxy. Denote by EZ the expectation with respect Z. Moreover, denote by K, L ∈ Rm×m

50 Dependence Measure

kernel

matrices

obtained

via

Kij

=

k(xi,

xj )

and

Lij

=

l(yi, yj).

Finally

let

H

=

I−

1 m

11

∈

Rm×m be the centering matrix which is a projection onto the space orthogonal to the vector 1.

We can derive biased estimator of HSIC from these kernel matrices. Furthermore, by removing

the bias and forming the U-statistics, we also provide an unbiased estimator of HSIC.

4.5.1 Biased Estimator
We derive a biased estimator of HSIC by estimating the three terms in (4.23) respectively using the kernel matrices K and L (Table 4.2).

Table 4.2: Biased estimator of the terms in HSIC.

Population Quantity

Biased Estimator

ExyEx y [k(x, x )l(y, y )] Exy[Ex [k(x, x )]Ey [l(y, y )]] ExEx [k(x, x )]EyEy [l(y, y )]

1 m2

tr

(KL)

1 m3

1

KL1

1 m4

1

K11

L1

Combining the terms we have the following biased estimator of HSIC

1 21 HSICb = m2 tr (KL) − m3 1 KL1 + m4 1 K11 L1

1 = m2

tr (KL) − 1 tr KL11 m

− 1 tr LK11 m

(4.31) 1 + m2 tr L11 K11

1 = m2

tr

KL

I−

1 11

m

− 1 tr

K11 L

I−

1 11

mm

1 = m2 tr

K

I−

1 11

m

L

I−

1 11

m

1 = m2 tr (KHLH)

The bias of HSICb arises from the self-interaction terms, i.e. we still have O(m) terms of the form KiiLii, O(m2) terms of the form KiiLij and KijLii, and O(m3) terms of the form
KiiLjs and KjsLii in the sum. However, the bias decreases as the sample size increases, with a rate of O(m−1) [59].

Theorem 35 (Biased estimator of HSIC [59]) The estimator

1 HSICb := m2 tr (KHLH)

(4.32)

has bias O(m−1), i.e. HSIC − EZ HSICb = O(m−1).

To address this, we now devise an unbiased estimator by removing these self-interaction terms while ensuring proper normalization.

4.5 Empirical Estimates of HSIC

51

4.5.2 Unbiased Estimator

Our proposed unbiased estimator has the form

1 HSICu := m(m − 3)

tr(K˜ L˜)

−

m

2 −

1 2

K˜ L˜1

+

1 K˜ 11 L˜1 (m − 1)(m − 2)

,

(4.33)

where K˜ and L˜ are related to K and L by K˜ ij = (1 − δij)Kij and L˜ ij = (1 − δij)Lij (i.e. the diagonal entries of K˜ and L˜ are set to zero).

Theorem 36 (Unbiased estimator of HSIC) The estimator HSICu is unbiased, that is, we have EZ HSICu = HSIC.

Proof We prove the claim by constructing unbiased estimators for each term in (4.30). Note

that we have three types of expectations, namely ExyEx y , a partially decoupled expectation

ExyEx Ey , and ExEyEx Ey which takes all four expectations independently.

If we want to replace the expectations by empirical averages, we need to take care to avoid

using the same discrete indices more than once for independent random variables. In other

words, when taking expectations over n independent random variables, we need n-tuples of

indices where each index occurs exactly once. We deﬁne the sets imn to be the collections of

indices satisfying this property. By simple combinatorics one can see that their cardinalities

are

given

by

the

Pochhammer

symbols

(m)n

=

m! (m−n)!

.

Jointly

drawn

random

variables,

on

the other hand, share the same index.

For the joint expectation over pairs we have

ExyEx y

1

k(x, x )l(y, y )

= (m)2 EZ

Kij Lij
(i,j)∈im2

1 = (m)2 EZ

tr

K˜ L˜

.

(4.34)

Recall that we set K˜ ii = L˜ii = 0. In the case of the expectation over three independent terms ExyEx Ey [k(x, x )l(y, y )] we obtain

1

(m)3 EZ

Kij Liq
(i,j,q)∈im3

1 = (m)3 EZ

1 K˜ L˜1 − tr

K˜ L˜

.

(4.35)

For four independent random variables ExEyEx Ey [k(x, x )l(y, y )],

1

(m)4 EZ

Kij Lqr
(i,j,q,r)∈im4

1 = (m)4 EZ

1

K˜ 11

L˜1 − 41

K˜ L˜1 + 2 tr

K˜ L˜

. (4.36)

To obtain an expression for HSIC we only need to take linear combinations using (4.30). Collecting terms related to tr K˜ L˜ , 1 K˜ L˜1, and 1 K˜ 11 L˜1 yields

1 HSIC = m(m − 3) EZ

tr

K˜ L˜

−

m

2 −

1 2

K˜ L˜1

+

1 K˜ 11 L˜1 (m − 1)(m − 2)

.

(4.37)

This is the expected value of HSICu. Note that neither HSICb nor HSICu require any explicit regularization parameters, unlike ear-

52 Dependence Measure
lier work on kernel canonical correlation. The regularization is encapsulated in the choice of the kernels. Another key feature is that the computation for both HSICb and HSICu is simple: only the kernel matrices are needed. This is in contrast to information theoretic quantities, which requires density estimation and/or sophisticated bias correction strategies [e.g. 99].

4.5.3 Computation

Note that both HSICb and HSICu only need the kernel matrices K and L. Assume that computing an entry in K and L takes constant time, then computing the full matrix takes O(m2) time. In term of the sample size m, we have the following analysis of the time complexity of HSICb and HSICu (by considering summation and multiplication as atomic operations)
• HSICb: Centering L takes O(m2) time. Since tr(KHLH) is equivalent to 1 (K ◦ HLH)1, it also takes O(m2) time. Overall, computing HSICb takes O(m2) time.
• HSICu: Each of the three terms in HSICu, namely tr(K˜ L˜), 1 K˜ 11 L˜1 and 1 K˜ L˜1, takes O(m2) time. Overall, computing HSICu also takes O(m2) time.
Further speedup is also possible via a low rank approximation of the kernel matrices. Particularly, using incomplete Cholesky decomposition, [59] derive an efﬁcient approximation of HSICb. Formally, it can be summarized as the following lemma

Lemma 37 (Efﬁcient Approximation to HSICb) Let K ≈ AA and L ≈ BB , where A ∈ Rm×df and B ∈ Rm×dg . Then HSICb can be approximated in O(m(d2f + d2g)) time.
Note that in this case the dominant computation comes from the incomplete Cholesky decomposition, which can be carried out in O(md2f ) and O(md2g) time respectively [46].
The three terms in HSICu can be computed analogously. Denote by DK = diag(AA ) and DL = diag(BB ) the diagonal matrices of the approximating terms. The latter can be computed in O(mdf ) and O(mdg) time respectively. We have

1 K˜ 1 = 1 (AA − DK)1 = 1 A 2 + 1 DK1.

(4.38)

Computation requires O(mdf ) time. The same holds when computing 1 L˜1. To obtain the second term we exploit that

1 K˜ L˜1 = 1 (AA − DK)(BB − DK)1 = ((A(A 1)) − DK1) ((B(B 1)) − DL1).

(4.39)

This can be computed in O(m(df + dg)). Finally, to compute the third term we use

tr K˜ L˜ = tr(AA − DK)(BB − DL)

(4.40)

=

A

B

2 Frob

−

tr

B

DKB

− tr

A

DLA

+ tr (DKDL) .

This can be computed in O(mdf dg) time. It is the most costly part of all operations, since it takes all interactions between the reduced factorizations of K and L into account. Hence we may compute HSICu efﬁciently (note again that dominant computation comes from the incomplete Cholesky decomposition):

4.5 Empirical Estimates of HSIC

53

Lemma 38 (Efﬁcient Approximation of HSICu) Let K ≈ AA and L ≈ BB , where A ∈ Rm×df and B ∈ Rm×dg . Then HSICu can be approximated in O(m(d2f + d2g)) time.

4.5.4 HSIC is concentrated
HSICu, the estimator in (4.33), can be alternatively formulated using U-statistics [69]. This reformulation allows us to derive a uniform convergence bound for HSICu. Such a bound has practical importance. For instance, in term of feature selection, this means that the quality of features evaluated using HSICu closely reﬂects its population counterpart HSIC. Furthremore, the same features should consistently be selected to achieve high dependence if data are repeatedly drawn from the same distribution. We formalize these results below

Theorem 39 (U-statisic of HSIC) HSICu can be rewritten in terms of a U-statistic 1
HSICu = (m)4 (i,j,q,r)∈im4 h(i, j, q, r), where the kernel h of the U-statistic is deﬁned by

(4.41)

(i,j,q,r)

1 h(i, j, q, r) =
24

Kst (Lst + Luv − 2Lsu)

(s,t,u,v)

(i,j,q,r)

(i,j,q,r)

1 =
6

Kst

(Lst

+

Luv )

−

1 12

KstLsu.

(s≺t),(u≺v)

(s,t,u)

(4.42)

Here the ﬁrst sum represents all 4! = 24 quadruples (s, t, u, v) which can be selected without replacement from (i, j, q, r). Likewise the sum over (s, t, u) is the sum over all triples chosen without replacement. Finally, the sum over (s ≺ t), (u ≺ v) has the additional condition that the order imposed by (i, j, q, r) is preserved. That is (i, q) and (j, r) are valid pairs, whereas (q, i) or (r, j) are not. Proof Combining the three unbiased estimators in (4.34-4.36) we obtain a single U-statistic

HSICu

=

1 (m)4

(i,j,q,r)∈im4

(Kij Lij

+

Kij Lqr

−

2KijLiq) .

(4.43)

In this form, however, the kernel h(i, j, q, r) = KijLij +KijLqr −2KijLiq is not symmetric in its arguments. For instance h(i, j, q, r) = h(q, j, r, i). The same holds for other permutations of the indices. Thus, we replace the kernel with a symmetrized version, which yields

(i,j,q,r)

1

h(i, j, q, r) := 4!

(KstLst + KstLuv − 2KstLsu) ,

(s,t,u,v)

(4.44)

where the sum in (4.44) represents all ordered quadruples (s, t, u, v) selected without replacement from (i, j, q, r).
This kernel can be simpliﬁed, since Kst = Kts and Lst = Lts. The ﬁrst one only contains terms KstLst, hence the indices (u, v) are irrelevant. Exploiting symmetry we may impose (s ≺ t) without loss of generality. The same holds for the second term. The third term remains

54 unchanged, which completes the proof.

Dependence Measure

We now show that HSICu is concentrated and that it converges to HSIC with rate O(m−1/2).
This is a slight improvement over the convergence of the biased estimator HSICb proposed by [59]. We will use the a bound from Hoeffding [70] which applies to U-statistics of HSIC.

Theorem 40 (HSIC is Concentrated) Assume k, l are bounded almost everywhere by 1, and are non-negative. Then for m > 1 and all δ > 0, with probability at least 1 − δ for all Prxy

HSICu − HSIC ≤ 4 2 log(2/δ)/m.

(4.45)

Proof [Sketch] By virtue of (4.41) we see immediately that HSICu is a U-statistic of order 4, where each term is contained in [−2, 2]. Applying Hoeffding’s bound for U-statistics we set

2 exp

−

2 (b

2m/4 − a)2

=δ

(4.46)

and obtain = 4 2 log(2/δ)/m. If k and l were just bounded by 1 in terms of absolute value the bound of Theorem 40 would be worse by a factor of 2.
The concentration result means that if we estimate HSIC from training data we can guarantee with high probability its deviation from the population counterpart is bounded. Furthermore, the deviation drops at rate O(m1/2) as the sample size increase. This is desirable for learning, since we want to generalize the properties we obtain from the training data to future data. This concentration result provides us conﬁdence on how close our empirical estimate is to the true quantity we try to estimate.

4.6 Hypothesis Test using HSIC
We now describe tests of statistical (in)dependence for two random variables, based on the test statistics for HSIC. We begin with an introduction to the terminology of statistical hypothesis testing [28, Chapter 8], as it applies to independence testing. Given iid. sample Z deﬁned earlier, the statistical test, T (Z) : (X × Y)m → {0, 1} is used to distinguish between the null hypothesis H0 : Prxy = Prx Pry and the alternative hypothesis H1 : Prxy = Prx Pry.
This is achieved by comparing the test statistic, in our case HSICu or HSICb, with a particular threshold: if the threshold is exceeded, then the test rejects the null hypothesis (bearing in mind that a zero population HSIC indicates Prxy = Prx Pry). The acceptance region of the test is thus deﬁned as any real number below the threshold. Since the test is based on a ﬁnite sample, it is possible that an incorrect answer will be returned: the Type I error is deﬁned as the probability of rejecting H0 based on the observed sample, despite x and y being independent. Conversely, the Type II error is the probability of accepting Prxy = Prx Pry when the underlying variables are dependent. The level α of a test is an upper bound on the Type I error, and is a design parameter of the test, used to set the test threshold. A consistent test achieves a level α, and a Type II error of zero, in the large sample limit.
How, then, do we set the threshold of the test given α? The approach we adopt here is to derive the asymptotic distribution of the empirical estimate of HSIC under H1 and H0 respec-

4.6 Hypothesis Test using HSIC

55

tively. We then use the 1 − α quantile of this distribution as the test threshold.2 Our presentation in this section is therefore divided into two main parts. First, we obtain the distribution of
HSIC under both H1 and H0; the former distribution is also needed to ensure consistency of the test. We shall see, however, that the distribution under H0 has a complex form, and cannot be evaluated directly. Thus, in the second part of this section, we describe ways to accurately approximate the 1 − α quantile of this distribution.

4.6.1 Asymptotic Normality under H1

Theorem 40 gives worst case bounds on the deviation between HSIC and HSICu. However, in many cases more accurate bounds for the typical case are needed. In particular, we would like
to know the limiting distribution of HSICu for large sample sizes. We now show that, in fact, HSICu is asymptotically normal and we derive its variance. For the biased estimator HSICb, since the bias drops at rate O(m−1), we actually have exactly the same asymptotic distribution
for HSICu and HSICb. Therefore, in this section we will present the result for HSICu only.

Theorem 41 (Asymptotic Normality) If E[h2] < ∞, and two random variables x and y are
not independent, then their HSICu converges in distribution (a.s. m → ∞) to a Gaussian random variable with mean HSIC and estimated variance

σ2

16 =

HSICu m

2
R − HSICu

,

(4.47)

where

1m R=
m

(m − 1)−3 1

2
h(i, j, q, r) ,

i=1 (j,q,r)∈im3 \{i}

(4.48)

and imn \ {i} denotes the set of all n-tuples drawn without replacement from {1, . . . , m} \ {i}.

Proof [Sketch] This follows directly from [118, Theorem B, p. 193], which shows asymptotic normality of U-statistics.

Unfortunately (4.47) is expensive to compute by means of an explicit summation: even com-
puting the kernel h of the U-statistic itself is a nontrivial task. For practical purposes we need an expression which can exploit fast matrix operations. As we shall see, σ2 can be computed
HSICu
in O(m2), given the matrices K˜ and L˜. To do so, we ﬁrst form a vector h with its ith entry corresponding to (j,q,r)∈im3 \{i} h(i, j, q, r). Collecting terms in (4.42) related to matrices K˜ and L˜, h can be written as

h =(m − 2)2(K˜ ◦ L˜)1 + (m − 2) (tr K˜ L˜)1 − K˜ L˜1 − L˜K˜ 1 − m(K˜ 1) ◦ (L˜1)

+ (1 L˜1)K˜ 1 + (1 K˜ 1)L˜1 − (1 K˜ L˜1)1,

(4.49)

where ◦ denotes elementwise matrix multiplication. Then R in (4.47) can be computed as R = (4m)−1(m − 1)−3 2h h. Combining this with the the unbiased estimator in (4.33) leads to the matrix computation of σ2 .
HSICu
2An alternative would be to use a large deviation bound, as provided for instance by [59] based on Hoeffding’s inequality. It has been reported in [58], however, that such bounds are generally too loose for hypothesis testing.

56 Dependence Measure

Variance of HSICu To compute the variance of HSICu we also need to deal with (K˜ ◦ L˜)1.
For the latter, no immediate linear algebra expansion is available. However, we may use of the following decomposition. Assume that a and b are vectors in Rm. In this case

((aa ) ◦ (bb ))1 = (a ◦ b)(a ◦ b) 1,

(4.50)

which can be computed in O(m) time. Hence we may compute

df dg

((AA ) ◦ (BB ))1 =

((A i ◦ B j)(A i ◦ B j) )1,

i=1 j=1

(4.51)

which can be carried out in O(mdf dg) time. To take care of the diagonal corrections note that (AA − DK) ◦ DL = 0. The same holds for B and DK. The remaining term DKDL1 is also computable in O(m) time.

4.6.2 Asymptotic Distribution under H0
Under H0, however, the variance of HSICu is degenerate. It means that HSICu can no longer be approximated as a Gaussian distribution, and its variance σ2 as computed from 4.47
HSICu
becomes zero. In this case, the asymptotic distribution is an inﬁnite sum of χ2 distributions.

Theorem 42 Under H0, the U-statistic HSICu is degenerate, meaning Ei [h (i, j, q, r)] = 0. In this case, HSICu converges in distribution according to [118, Section 5.5.2]

1 HSICb −→ m

∞

λl(χ2l − 1)

a.s.

m −→ ∞,

l=1

(4.52)

where χ2l are iid. χ2(1) random variables, and λl are the solutions to the eigenvalue problem

h (i, j, q, r) ψl(zj)d Przizqzr = λlψl(zj),

(4.53)

where the integral is over the distribution of variables zi, zq, and zr.

Proof This follows from the discussion of [118, Section 5.5.2]. In this case, the asymptotic distribution for the corresponding V-statistic HSICb is also an inﬁnite sum of χ2 distributions. The difference is that the counterpart of (4.52) for the V-statistic is not centered, ie.

HSICb

−→

1 m

∞

λlχ2l

a.s.

m −→ ∞,

l=1

(4.54)

A hypothesis test using HSICb can be derived from Theorem 42 above by computing the (1 − α)th quantile of the distribution (4.52), where consistency of the test (that is, the convergence to zero Type II error for m → ∞) is guaranteed by the decay as O(m−1). The
distribution under H0 is complicated. The question then becomes how to accurately approximate its quantiles.

4.6 Hypothesis Test using HSIC

57

One approach, taken by [45], is to use a Monte Carlo resampling technique: the ordering

of the Y sample is permuted repeatedly while that of X is kept ﬁxed, and the 1 − α quantile is

obtained from the resulting distribution of HSICb values. This can be very expensive, however. A second approach, suggested in [77, p. 34], is to approximate the null distribution as a two-

parameter Gamma distribution [76, p. 343, p. 359]: this is one of the more straightforward approximations of an inﬁnite sum of χ2(1) variables (see [76, Chapter 18.8] for further ways

to approximate such distributions; in particular, we wish to avoid using moments of order

greater than two, since these can become expensive to compute). Speciﬁcally, we make the

approximation

xα−1e−x/β HSICb ∼ mβαΓ(α) ,

(4.55)

where

α = (E[HSICb])2 , β = V[HSICb] .

V[HSICb]

E[HSICb]

(4.56)

An illustration of the cumulative distribution function (CDF) obtained via the Gamma approximation is given in Figure 4.2, along with an empirical CDF obtained by repeated draws of HSICb. We note the Gamma approximation is quite accurate, especially in areas of high probability (which we use to compute the test quantile). The accuracy of this approximation will be further evaluated experimentally in Section 4.7.
To obtain the Gamma distribution from our observations, we need empirical estimates for
E[HSICb] and V[HSICb] under the null hypothesis. Expressions for these quantities are given in [77, pp. 26-27], however these are in terms of the joint and marginal characteristic functions, and do not always apply in our more general kernel setting (see also [80, p. 313]). In the following two theorems, we provide much simpler expressions for both quantities, in terms of norms of mean elements µx and µy, and the variance operators

Cxx := Ex[(k(x, ·) − µx) ⊗ (k(x, ·) − µx)]

(4.57)

and Cyy similarly, in feature space. The main advantage of our new expressions is that they are computed entirely in terms of kernels, which makes possible the application of the test to any domains on which kernels can be deﬁned, and not only Rd. Furthermore, we have the
following two theorems

Theorem 43 Under H0,

1 E[HSICb] = m (Exy [k(x, x)l(y, y)] +

µx

2 F

µy

2 G

−

µx

2 F

Ey

[l(y,

y)]

−

µy

2 G

Ex

[k(x,

x)])

+

O(m−2).

(4.58)

An empirical estimate of this statistic is obtained by replacing the norms above with

µx

2 F

=

(m)−2 1 (i,j)∈im2 k(xi, xj ).

Theorem 44 Under H0,

2(m − 4)(m − 5)

V[HSICb] =

(m)4

Cxx

2 HS

Cyy

2 HS

+

O(m−3).

(4.59)

58 Dependence Measure
Figure 4.2: The cumulative distribution function of HSICb (Emp) under H0 for m = 200, obtained empirically using 5000 independent draws of HSICb. The two-parameter Gamma distribution (Gamma) is ﬁt using α = 1.17 and β = 8.3 × 10−4 in (4.55), with mean and variance computed via Theorems 43 and 44.
Denote by ◦ the element wise matrix product, A·2 the element wise matrix power, and B = ((HKH) ◦ (HLH))·2. An empirical estimate with negligible bias may be found by replacing the product of covariance operator norms with 1 (B − diag(B)) 1: this is slightly more efﬁcient than taking the product of the empirical operator norms (although the scaling with m is unchanged). We remark that all the above computation can be carried out in O(m2) time. It is also possible to further improve the test speed by approximating the Gram matrices using incomplete Cholesky decomposition.
4.7 Experiments
General tests of statistical independence are most useful for data having complex interactions that simple correlation does not detect. We investigate two cases where this situation arises: ﬁrst, we test vectors in Rd which have a dependence relation but no correlation, as occurs in independent subspace analysis; and second, we study the statistical dependence between a text and its translation.
4.7.1 Independence of Subspaces
One area where independence tests have been applied is in determining the convergence of algorithms for independent component analysis (ICA), which involves separating random variables that have been linearly mixed, using only their mutual independence. ICA generally entails optimization over a non-convex function (including when HSIC is itself the optimization criterion [59]), and is susceptible to local minima, hence the need for these tests (in fact, for classical approaches to ICA, the global minimum of the optimization might not correspond to independence for certain source distributions). Contingency table-based tests have been applied [84] in this context, while the test of [77] has been used in [80] for verifying ICA outcomes when the data are stationary random processes (through using a subset of samples

4.7 Experiments

59

with a sufﬁciently large delay between them). Contingency tests may be less useful in the case of independent subspace analysis (ISA, see e.g. [135] and its bibliography), where higher dimensional independent random vectors are to be separated. Thus, characteristic function-based tests [45, 77] and kernel independence measures may work better for this problem.
In our experiments, we tested the independence of random vectors, as a way of verifying the solutions of independent subspace analysis. We assume for ease of presentation that our subspaces have respective dimension dx = dy = d, but this is not required. The data are constructed as follows. First, we generate m samples of two univariate random variables, each drawn at random from the ICA benchmark densities in [61, Table 3]: these include superGaussian, sub-Gaussian, multimodal, and unimodal distributions. Second, we mix these random variables using a rotation matrix parameterized by an angle θ, varying from 0 to π/4 (a zero angle means the data are independent, while dependence becomes easier to detect as the angle increases to π/4: see the two plots in Figure 4.3, top left). Third, we add d − 1 dimensional Gaussian noise of zero mean and unit standard deviation to each of the mixtures. Finally, we multiply the two resulting variables by a d-dimensional orthogonal matrix, such that the resulting vectors are dependent across all observed dimensions. We emphasize that classical approaches (such as Spearman’s ρ or Kendall’s τ ) are completely unable to ﬁnd this dependence, since the variables are uncorrelated; nor can we recover the subspace in which the variables are dependent using PCA, since this subspace has the same second order properties as the noise. We investigated sample sizes m = 128, 512, 1024, 2048, and d = 1, 2, 4.
We compared two different methods for computing the 1 − α quantile of the HSICb null distribution: repeated random permutation of the Y sample ordering as in [45] (HSICp), where we used 200 permutations; and Gamma approximation (HSICg), based on (4.55). We used a Gaussian kernel, with kernel size set to the median distance between points in input space. We also compared with the power-divergence based contingency table test of [109] (PD), which consisted in partitioning the space, counting the number of samples falling in each partition, and comparing this with the number of samples that would be expected under the null hypothesis (the test we used, described in [84], is more reﬁned than this short description would suggest). Rather than a uniform space partitioning, we divided our space into roughly equiprobable bins as in [84], using a Gessaman partition for higher dimensions [33, Figure 21.4].3 All remaining parameters were set according to [84]. Results are plotted in Figure 4.3. The y-intercept on these plots corresponds to the acceptance rate of H0 at independence, or 1 − (Type I error), and should be close to the design parameter of 1 − α = 0.95. Elsewhere, the plots indicate acceptance of H0 where the underlying variables are dependent, i.e. the Type II error.
As expected, we observe that dependence becomes easier to detect as θ increases from 0 to π/4, when m increases, and when d decreases. The PD approach performs poorly at m = 128, but approaches the performance of HSIC-based tests for increasing m (although it remains slightly worse at m = 512 and d = 1). PD also scales very badly with d, and never rejects the null hypothesis when d = 4, even for m = 2048. Although HSIC-based tests are unreliable for small θ, they generally do well as θ approaches π/4 (besides m = 128, d = 2). We also emphasise that HSICp and HSICg perform identically, although HSICp is far more costly (by a factor of around 100, given the number of permutations used).
3Ku and Fine did not specify a space partitioning strategy for higher dimensions, since they dealt only with univariate random variables.

60 Dependence Measure
Figure 4.3: Top left plots: Example dataset for d = 1, m = 200, and rotation angles θ = π/8 (left) and θ = π/4 (right). In this case, both sources are mixtures of two Gaussians (source (g) in [61, Table 3]). We remark that the random variables appear “more dependent” as the angle θ increases, although their correlation is always zero. Remaining plots: Rate of acceptance of H0 for the PD, HSICp, and HSICg tests. “Samp” is the number m of samples, and “dim” is the dimension d of x and y.
4.7.2 Dependence between Text
In this section, we demonstrate independence testing on text. Our data are taken from the Canadian Hansard corpus (http://www.isi.edu/natural-language/download/hansard/). These consist of the ofﬁcial records of the 36th Canadian parliament, in English and French. We used debate transcripts on the three topics of Agriculture, Fisheries, and Immigration, due to the relatively large volume of data in these categories. Our goal was to test whether there exists a statistical dependence between English text and its French translation. Our dependent data consisted of a set of paragraph-long (5 line) English extracts and their French translations. For our independent data, the English paragraphs were matched to random French paragraphs on the same topic: for instance, an English paragraph on ﬁsheries would always be matched with a French paragraph on ﬁsheries. This was designed to prevent a simple vocabulary check from being used to tell when text was mismatched. We also ignored lines shorter than ﬁve words long, since these were not always part of the text (e.g. identiﬁcation of the person speaking).
We used the k-spectrum kernel of [88], computed according to the method of [134]. We set k = 10 for both languages, where this was chosen by cross validating on an SVM classiﬁer for Fisheries vs National Defense, separately for each language (performance was not especially sensitive to choice of k; k = 5 also worked well). We compared this kernel with a simple kernel between bags of words [26, pp. 186–189]. Results are in Table 4.3.
Our results demonstrate the excellent performance of the HSICp test on this task: even for small sample sizes, HSICp with a spectral kernel always achieves zero Type II error, and a Type I error close to the design value (0.95). We further observe for m = 10 that HSICp with the spectral kernel always has better Type II error than the bag-of words kernel. This suggests that a kernel with a more sophisticated encoding of text structure induces a more sensitive test,

4.8 Summary

61

Table 4.3: Independence tests for cross-language dependence detection. Topics are in the ﬁrst column,

where the total number of 5-line extracts for each dataset is in parentheses. BOW(10) denotes a bag of

words kernel and m = 10 sample size, Spec(50) is a k-spectrum kernel with m = 50. The ﬁrst entry in

each cell is the null acceptance rate of the test under H0 (i.e. 1 − (Type I error); should be near 0.95);

the second entry is the null acceptance rate under H1 (the Type II error, small is better). Each entry is

an average over 300 repetitions.

Topic

BOW(10) HSICg HSICp

Spec(10) HSICg HSICp

BOW(50) HSICg HSICp

Spec(50) HSICg HSICp

Agriculture

1.00,

0.94,

1.00,

0.95,

1.00,

0.93,

1.00,

0.95,

(555)

0.99 0.18 1.00 0.00 0.00 0.00 0.00 0.00

Fisheries

1.00,

0.94,

1.00,

0.94,

1.00,

0.93,

1.00,

0.95,

(408)

1.00 0.20 1.00 0.00 0.00 0.00 0.00 0.00

Immigration

1.00,

0.96,

1.00,

0.91,

0.99,

0.94,

1.00,

0.95,

(289)

1.00 0.09 1.00 0.00 0.00 0.00 0.00 0.00

although for larger sample sizes, the advantage vanishes. The HSICg test does less well on this data, always accepting H0 for m = 10, and returning a Type I error of zero, rather than the design value of 5%, when m = 50. It appears that this is due to a very low variance estimate returned by the Theorem 44 expression, which could be caused by the high diagonal dominance of kernels on strings. Thus, while the test threshold for HSICg at m = 50 still fell
between the dependent and independent values of HSICb, this was not the result of an accurate modelling of the null distribution. We would therefore recommend the permutation approach for this problem. Finally, we also tried testing with 2-line extracts and 10-line extracts, which yielded similar results.

4.8 Summary
We have introduced a measure of dependence based on Hilbert space embedding of distributions. This new measure computes the Hilbert space distance between the embedded joint distribution Prxy and the product of the marginals Prx Pry. For iid. data, we recovered Hilbert-Schmidt norm of the cross covariance operator (HSIC) as a special case. We also derived unbiased estimator and concentration results for HSIC.
HSIC has the ﬂexibility of choosing kernels, and different kernels can be applied to the two domains being tested. Therefore, we can use HSIC for observations from structured domains. For instance, we can use it to detect dependence between texts and their translations. Furthermore, we can also use HSIC to detect dependence between data of completely different types, such as between images and captions.The choice of kernels also allow us to incorporate prior knowledge into the dependence estimation process. A good choice of kernels on the domains being tested can result in very sensitive tests at small sample sizes.
More interestingly, we can also use HSIC for various learning problems. As we discussed in Chapter 3, we can use dependence for feature selection, clustering and dimensionality reduction. In the next three chapters, we will use these several learning problems as examples to demonstrate how we can perform learning via dependence. In particular, when express these learning problems using HSIC, we not only design new algorithms, but also recover many existing algorithms as special cases.

5C H A P T E R
Feature Selection via Dependence Estimaton

In this chapter, we will use dependence as measured by HSIC for feature selection. This problem can be viewed as choosing a subset of features that are most relevant to the labels. We show that selecting features via HSIC subsumes many existing feature selectors as special cases. We also design an algorithm that combines HSIC and a backward elimination strategy for feature selection. We demonstrate its good performance in various real world datasets.

5.1 Introduction

In data analysis we are typically given a set of observations X = {x1, . . . , xm} ⊆ X which can be used for a number of learning tasks, such as novelty detection, low-dimensional representation, or a range of supervised problems. In the latter case we also have a set of labels Y = {y1, . . . , ym} ⊆ Y at our disposal. Supervised tasks include ranking, classiﬁcation, regression, or sequence annotation. While not always true in practice, we assume in the following, that the data X and Y have been generated identically independently distributed (iid.) from some underlying distribution Prxy.
We often want to reduce the dimension of the data (the number of features) before the actual learning [64]; a larger number of features can be associated with higher data collection cost, more difﬁculty in model interpretation, higher computational cost for the classiﬁer, and sometimes decreased generalization ability. In other words, often there exist ulterior motives than ﬁnding a well performing estimator which make feature selection a necessity for reasons of speed or deployment cost. It is therefore important to select an informative feature subset.
The problem of supervised feature selection can be cast as a combinatorial optimization problem. We have a full set of features, denoted by S (each element in S corresponds to one dimension of the data). It is our aim to select a subset T ⊆ S such that this subset retains the relevant information contained in X. Given m observations, suppose the relevance of a feature subset (to the outcome) is quantiﬁed by Qm(T ), and is computed by restricting the data to the dimensions in T . Feature selection can then be formulated as

T0 = arg max Qm(T )
T ⊆S
subject to |T | ≤ t,

(5.1)

where | · | computes the cardinality of a set and t is an upper bound on the number of selected features. Two important aspects of problem (5.1) are the choice of the criterion Qm(T ) and the selection algorithm.

5.1 Introduction

63

5.1.1 Criteria for Feature Selection
A number of quality functionals Qm(T ) are potential candidates to use for feature selection. For instance we could use a estimator of mutual-information or a Hilbert Space based estimator. In any case, the choice of Qm(T ) should respect the underlying tasks: supervised learning estimates functional dependence f from training data and guarantees f predicts well on test data. Therefore, good criteria should satisfy two conditions:
I: Qm(T ) is capable of detecting desired (linear or nonlinear) functional dependence between the data and the labels.
II: Qm(T ) is concentrated with respect to the underlying measure. This guarantees with high probability that detected functional dependence is preserved in test data.
While many criteria have been explored few take these two conditions explicitly into account. Examples include the leave-one-out error bound of SVM [154] and the mutual information [156]. Although the latter has good theoretical justiﬁcation, it requires density estimation, which is problematic for high dimensional and continuous variables. We sidestep these problems by employing a mutual-information like quantity — the Hilbert-Schmidt Independence Criterion (HSIC) we introduced in Chapter 4. HSIC uses kernels to measure the dependence between the feature and the labels. It detects nonlinear as linear dependence when universal kernels are used. Furthermore, empirical estimate of HSIC has good uniform convergence guarantees. Therefore HSIC satisﬁes conditions I and II, required for a good feature selection criterion.

5.1.2 Feature Selection Algorithms
Finding a global optimum for (5.1) is NP-hard in general [153], unless the criterion is easily decomposable or has properties which make optimization easier, e.g. submodularity [63]. Many algorithms transform (5.1) into a continuous problem by introducing weights on the dimensions [154, 22, 153, 98]. These methods perform well for linearly separable problems. For nonlinear problems, however, the optimization usually becomes non-convex and a local optimum does not necessarily provide good features. Greedy approaches, forward selection and backward elimination, are often used to tackle problem (5.1) directly. Forward selection tries to increase Qm(T ) as much as possible for each inclusion of features, and backward elimination tries to achieve this for each deletion of features [65]. Although forward selection is computationally more efﬁcient, backward elimination provides better features in general since the features are assessed within the context of all others. See Section 5.5 for experimental details.
In principle, the Hilbert-Schmidt independence criterion can be employed for feature selection using either a weighting scheme, forward selection or a backward selection strategy, or even a mix of several strategies. While the main focus of this paper is on the backward elimination strategy, we also discuss the other selection strategies. As we shall see, several speciﬁc choices of a kernel function will lead to well known feature selection and feature rating methods.
Note that backward elimination using HSIC (BAHSIC) is a ﬁlter method for feature selection. It selects features independent of a particular classiﬁer. Such decoupling not only facilitates subsequent feature interpretation but also speeds up the computation over wrapper and embedded methods.
We will see that BAHSIC is directly applicable to binary, multiclass, and regression problems. Most other feature selection methods are only formulated either for binary classiﬁcation

64 Feature Selection via Dependence Estimaton
or regression. Multi-class extension of these methods is usually accomplished using a oneversus-the-rest strategy. Still fewer methods handle classiﬁcation and regression cases at the same time. BAHSIC, on the other hand, accommodates all these cases and unsupervised feature selection in a principled way: by choosing different kernels, BAHSIC not only subsumes many existing methods as special cases, but also allows us to deﬁne new feature selectors. Such versatility of BAHSIC originates from the generality of HSIC.

5.2 Feature Selection via HSIC
We will use HSIC as the selection criterion, and we now describe algorithms that conduct feature selection on the basis of this dependence measure. Denote by S the full set of features, T a subset of features (T ⊆ S). We want to ﬁnd T such that the dependence between features in T and the labels is maximized. Moreover, we may choose between different feature selection strategies, that is, whether we would like to build up a catalog of features in an incremental fashion (forward selection) or whether we would like to remove irrelevant features from a catalog (backward selection). For certain kernels, such as a linear kernel, both selection methods are equivalent: the objective function decomposes into individual coordinates, and thus feature selection can be done without recursion in one go. Although forward selection is computationally more efﬁcient, backward elimination in general yields better features (especially for nonlinear features), since the quality of the features is assessed within the context of all other features [64].

5.2.1 Backward Elimination Using HSIC (BAHSIC)
BAHSIC works by generating a list S† which contains the features in increasing degree of relevance. At each step S† is appended by a feature from S which is not contained in S† yet by selecting the features which are least dependent on the reference set (i.e. Y or the full set X).
Once we perform this operation, the feature selection problem in (5.1) can be solved by simply taking the last t elements from S†. Our algorithm produces S† recursively, eliminating the least relevant features from S and adding them to the end of S† at each iteration. Note that
for feature selection, the kernel matrix for the labels remain unchanged. For convenience, we denote HSIC as HSIC(σ, S), where S are the features used in computing the data kernel matrix K, and σ is the parameter for the data kernel (for instance, this might be the size of a Gaussian kernel k(x, x ) = exp(−σ x − x 2)). The algorithm is presented in Algorithm 1.

Algorithm 1 BAHSIC

Input: The full set of features S Output: An ordered set of features S†

1: S† ← ∅
2: repeat 3: σ ← Ξ 4: I ← arg maxI 5: S ← S \ I 6: S† ← S† ∪ I 7: until S = ∅

j∈I HSIC(σ, S \ {j}), I ⊂ S

Step 3 of the algorithm denotes a policy for adapting the kernel parameters. Depending on

5.2 Feature Selection via HSIC

65

the availability of prior knowledge and the type of preprocessing, we explored three types of policies

1. If we have prior knowledge about the nature of the nonlinearity in the data, we can use a
ﬁxed kernel parameter throughout the iterations. For instance, we can use a polynomial kernel of ﬁxed degree, e.g. ( x, x + 1)2, to select the features for the XOR dataset
showed in Figure 5.1(a).

2. If we have no prior knowledge, we can optimize HSIC over a set of kernel parameters. In
this case, the policy corresponds to arg maxσ∈Θ HSIC(σ, S), where Θ is a set of parameters that ensure the kernel is bounded. For instance, σ can be the scale parameter of a Gaussian kernel, k(x, x ) = exp(−σ x − x 2). Optimizing over the scaling parameter
allows us to adapt to the scale of the nonlinearity present in the (feature-reduced) data.

3. Adapting kernel parameters via optimization is computational intensive. Alternatively we can use a policy that produces approximate parameters in each iteration. For instance, if we assume features to be independent of each other and normalize each feature separately to zero mean and unit variance, we know that the expected value of the distance between data points, ExEx (x − x )2 , is 2d (d is the dimension of the data). When using a Gaussian kernel, we can then use a policy that assigns σ to 1/(2d) as the dimension of the data is reduced.

Step 4 of the algorithm is concerned with the selection of a set I of features to eliminate.

While one could choose a single element of S, this would be inefﬁcient when there are a large

number of irrelevant features. On the other hand, removing too many features at once risks the

loss of relevant features. In our experiments, we found a good compromise between speed and

feature quality is to remove 10% of the current features at each iteration.

In BAHSIC, the kernel matrix L for the labels is ﬁxed through the whole process. It can

be precomputed and stored for speedup if needed. Therefore, the major computation comes

from repeated calculation of the kernel matrix K for the dimension-reduced data. Assume that

computing an entry in K requires constant time irrespective of the dimension of the data, then the ith iteration of BAHSIC takes O(βi−1dm2) time (d is the total number of features, and 1 − β is the portion of removed features in each iteration; therefore βi−1d features remain

after i − 1 iterations, and we have m2 elements in the kernel matrix in total). If we want to

reduce the number of features to t we need at most τ = logβ(t/d) iterations. This brings

the total time complexity to O

1−βτ 1−β

dm2

=O

d−t 1−β

m2

operations. When using incom-

plete Cholesky factorization we may reduce computational complexity somewhat further to

O

d−t 1−β

m(d2f

+

d2g )

time. This saving is signiﬁcant as long as df dg < m, which may hap-

pen, for instance whenever Y is a binary label matrix. In this case dg = 1, hence incomplete

factorizations may yield signiﬁcant computational gains.

5.2.2 Forward Selection Using HSIC (FOHSIC)
FOHSIC uses the converse approach to backward selection: it builds a list of features in decreasing degree of relevance. This is achieved by adding one feature at a time to the set of features S† obtained so far using HSIC as a criterion for the quality of the so-added features. For faster selection of features, we can choose a group of features (for instance, a ﬁxed proportion γ) at step 4 and add them in one shot at step 6. The adaptation of kernel parameters in step

66 Feature Selection via Dependence Estimaton

3 follows the same policies as those for BAHSIC. The feature selection problem in (5.1) can be solved by simply taking the ﬁrst t elements from S†.

Algorithm 2 FOHSIC

Input: The full set of features S, desired number of features t Output: An ordered set of features S†

1: S† ← ∅
2: repeat
3: σ ← Ξ
4: I ← arg maxI 5: S ← S \ I 6: S† ← S† ∪ I 7: until |S†| = t

j∈I HSIC(σ, S† ∪ {j}), I ⊂ S

Under the same assumption as BAHSIC, the ith iteration of FOHSIC takes O((1−γ)i−1dm2)

time. The total number of iterations τ to obtain t features is t = [1 − (1 − γ)τ ]d, that

is τ

=

log(d−t)−log d log(1−γ)

iterations.

Performing τ

steps will therefore take

τ −1 i=0

d(1

−

γ)i

=

d(1 − (1 − γ)τ )/γ = t/γ operations. This means that FOHSIC takes O(tm2/γ) time to extract

t features.

5.2.3 Feature Weighting Using HSIC

Besides backward elimination algorithm, feature selection using HSIC can also proceed by

converting problem (5.1) into a continuous optimization problem. By adding a penalty on the

number of nonzero terms, such as a relaxed L0 “norm” of a weight vector over the features we are able to solve the problem with continuous optimization methods. Unfortunately, this

approach does not perform as well as the the backward elimination procedure proposed in the

main text. For completeness and since related methods are somewhat popular in the literature,

the approach is described below. We introduce a weighting w ∈ Rn on the dimensions of the data: x −→ w ◦ x, where
◦ denotes element-wise product. Thus feature selection using HSIC becomes an optimization

problem with respect to w (for convenience we write HSIC as a function of w, HSIC(w)). To

obtain a sparse solution of the selected features, the zero “norm” w 0 is also incorporated into
our objective function (clearly . 0 is not a proper norm). w 0 computes the number of nonzero entries in w and the sparsity is achieved by imposing heavier penalty on solutions with

large number of non-zero entries. In summary, feature selection using HSIC can be formulated

as

w = arg max HSIC(w) − λ
w

w

0,

(5.2)

where w ∈ [0, ∞)n. The zero “norm” is not a continuous function. However, it can be

approximated well by a concave function [50] (α = 5 works well in practice):

w 0 ≈ 1 (1 − exp (−αw)).

(5.3)

While the optimization problem in (5.2) is non-convex, we may use relatively more efﬁcient optimization procedures for the concave approximation of the L0 norm. For instance, we may use the convex-concave procedure (CCCP) of [155]. For a Gaussian kernel HSIC can be

5.3 Variants of BAHSIC

67

decomposed into the sum of a convex and a concave function:
HSIC(w) − λ w 0 ≈ tr(K(I − m−111 )L(I − m−111 )) − λ1 (1 − e−αw). (5.4)
Depending on the choice of L we need to assign all terms involving exp with positive coefﬁcients into the convex and all terms involving negative coefﬁcients to the concave function.

5.3 Variants of BAHSIC
So far we discussed a set of algorithms to select features once we decided to choose a certain family of kernels k, l to measure dependence between two sets of observations. We now proceed to discussing a number of design choices for k and l. This will happen in two parts: in the current section we discuss generic choices of kernels on data and labels. Various combinations of such kernels will then lead to new algorithms that aim to discover different types of dependence between features and labels (or between a full and a restricted dataset whenever we are interested in unsupervised feature selection). After that (in section 5.4) we will study speciﬁc choices of kernels which correspond to existing feature selection methods.

5.3.1 Kernels on Data
There exists a great number of kernels on data. Different kernels induce distinctive similarity measure on the data and will correspond to a range of different assumptions on the type of dependence between random variables x and y. For instance

Linear kernel The simplest choice for k is to take a linear kernel k(x, x ) = x, x . This means that we are just using the underlying Euclidean space to deﬁne the similarity measure. Whenever the dimensionality d of x is very high, this may allow for more complexity in the function class than what we could measure and assess otherwise. An additional advantage of this setting is that the kernel decomposes into the sum of products between individual coordinates. This means that any expression of the type tr (KM) can be maximized with respect to the subset of available features via

d
tr (KM) = tr XX M = X jMX j.
j=1

(5.5)

This means that the optimality criterion decomposes into a sum over the scores of individual coordinates. Hence maximization with respect to a subset of size t is trivial, since it just involves ﬁnding the t largest contributors. In this case both FOHSIC and BAHSIC generate the optimal feature selection with respect to the criterion applied.

Polynomial kernel Clearly in some cases the use of linear features can be quite limiting. It
is possible, though, to use higher order correlations between data for the purpose of feature selection. This is achieved by using a polynomial kernel k(x, x ) = ( x, x + a)b (a ≥ 0, b ∈ N). This kernel incorporates all polynomial interactions up to degree b (provided that a > 0). For instance, if we wanted to take only mean and variance into account, we would only need to consider b = 2 and a = 1. Placing a higher emphasis on means is achieved by increasing the constant offset a.

68 Feature Selection via Dependence Estimaton

Radial Basis Function kernel Note that polynomial kernels only map data into a ﬁnite dimensional space: while potentially huge, the dimensionality of polynomials of bounded degree is ﬁnite, hence criteria arising from such kernels will not provide us with guarantees for detecting dependence of any kind. On the other hand, many radial basis function kernels, such as the Gaussian RBF kernel map x into an inﬁnite dimensional space. One may show that these kernels are in fact universal in the sense of [131]. That is, we use kernels of the form

k(x, x ) = κ( x − x ),

(5.6)

where κ(·) = exp(−·) and κ(·) = exp(−·2) to obtain Laplace and Gaussian kernels respectively. Since the spectrum of the corresponding matrices is rapidly decaying it is easy to compute incomplete Cholesky factorizations of the kernel matrix efﬁciently.

String and Graph kernel One of the key advantages of our approach is that it is not limited to vectorial data. For instance, we can perform feature selection on strings or graphs. For string kernels we have

k(x, x ) = wa#a(x)#a(x ),
ax

(5.7)

where a x is a substring of x [144, 89]. Similar decompositions can be made for graphs, where kernels on random walks and paths can be deﬁned. As before, we could use BAHSIC to remove or FOHSIC to generate a list of features such that only relevant ones remain. That said, given that such kernels are additive in their features, we can use the same argument as made above for linear kernels to determine meaningful features in one go.

5.3.2 Kernels on Labels
The kernels on the data described our inherent assumptions on which properties of x (e.g. linear, polynomial, or nonparametric) are relevant for estimation. We now describe the complementary part, namely a set of possible choices for kernels on labels. Note that these kernels can be just as general as those deﬁned on the data. This means that we may apply our algorithms to classiﬁcation, regression, ranking, etc. in the same fashion. This is a signiﬁcant difference to previous approaches which only apply to specialized settings such as binary classiﬁcation. For completeness we begin with the latter.

Binary Classiﬁcation The simplest kernel we may choose is

l(y, y ) = yy ,

(5.8)

where y, y ∈ {±1}. In this case the label kernel matrix L = yy has rank 1 and it is simply the outer product of the vector of labels. Note that we could transform l by adding a positive constant c to obtain l(y, y ) = yy + c which yields l(y, y ) = 2δy,y for c = 1. This transformation, however, is immaterial: once K has been centered it is orthogonal to constant matrices.
A second transformation, however, leads to nontrivial changes: we may change the relative weights of positive and negative classes. This is achieved by transforming y → cyy. For

5.4 Connections to Other Approaches

69

instance,

we

may

pick

c+

=

1 m+

and

c−

=

1 m−

.

That

is,

we

choose

11 y = m+ 1m+ , m− 1m−

,

(5.9)

which

leads

to

l(y, y

)

=

yy my my

That is, we give different weight to positive and negative class according to their sample

size. As we shall see in the next section, this corresponds to making the feature selection

independent of the class size and it will lead to criteria derived from the estimator of Maximum

Mean Discrepancy [58].

Multiclass Classiﬁcation Here we have a somewhat larger choice of options to contend with. Clearly the simplest kernel would be

l(y, y ) = cyδy,y ,

(5.10)

where cy > 0 and y, y

∈ {1, . . . , n} for n classes.

For cy =

1 my

we

obtain

a

per-class

normalization. Clearly, for n classes, the kernel matrix L can be represented by the outer

product of a rank-n matrix, where each row is given by cyey , where ey denotes the y-th unit vector in Rn. Alternatively, we may adjust the inner product between classes to obtain

l(y, y ) = ψ(y), ψ(y ) where

m 11 ψ(y) = my(m − my) ey − z and z = ( m − m1 , . . . , m − mn ) .

(5.11)

This corresponds to assigning a “one versus the rest” feature to each class and taking the inner

product between them. As before in the binary case, note that we may drop z from the expan-

sion, since constant offsets do not change the relative values of HSIC for feature selection. In

this

case

we

recover

(5.10)

with

cy

=

m2y

m2 (m−my

)2

.

Regression This is one of the situations where the advantages of using HSIC are clearly apparent: we are able to adjust our method to such situations simply by choosing appropriate kernels. Clearly, we could just use a linear kernel l(y, y ) = yy which would select simple correlations between data and labels. Another choice is to use an RBF kernel on the labels, such as

l(y, y ) = exp −σ¯ y − y 2 .

(5.12)

This will ensure that we capture arbitrary nonlinear dependence between x and y. The price is that in this case L will have full rank, hence computation of BAHSIC and FOHSIC are correspondingly more expensive.

5.4 Connections to Other Approaches
We now show that several feature selection criteria are special cases of BAHSIC by choosing appropriate preprocessing of data and kernels. We will directly relate these criteria to the biased estimator HSICb in (4.32). Given the fact that HSICb converges to HSICu with rate

70 Feature Selection via Dependence Estimaton
O(m−1) it follows that the criteria are well related. Additionally we can infer from this that by using HSICu these other criteria could also be improved by correcting their bias. In summary BAHSIC is capable of ﬁnding and exploiting dependence of a much more general nature (for instance, dependence between data and labels with graph and string values).

Pearson Correlation Pearson’s correlation is commonly used in microarray analysis [141, 44]. It is deﬁned as

1m Rj := m
i=1

Xij − µj sj

yi − µy . sy

(5.13)

This means that all features are individually centered by µj and scaled by their coordinatewise variance sj as a preprocessing step. Performing those operations before applying a linear
kernel yields the equivalent HSICu formulation:

2
tr (KHLH) = tr XX Hyy H = X Hy

(5.14)

d
=

m Xij − µj

j=1 i=1

sj

yi − µy sy

2d
= Rj2.
j=1

(5.15)

Hence HSICb computes the sum of the squares of the Pearson Correlation (pc) coefﬁcients. Since the terms are additive, feature selection is straightforward by picking the list of best performing features.

Mean Difference and its Variants The difference between the means of the positive and negative classes at the jth feature, (µj+ − µj−), is useful for scoring individual features. With different normalization of the data and the labels, many variants can be derived. In our experiments we compare a number of these variants. For example, the centroid (lin) [15], t-statistic (t), signal-to-noise ratio (snr), moderated t-score (m-t) and B-statistics (lods) [126] all belong to this family. In the following we make those connections more explicit.

Centroid [15] use vj := λµj+ −(1−λ)µj− for λ ∈ (0, 1) as the score for feature j.1 Features

are subsequently selected according to the absolute value |vj|. In experiments the authors

typically choose λ

yi yi myi myi

(yi, yi

∈

=

1 2

.

In

this

case,

we

can

achieve

the

same

goal

by

{±1}), in which case HLH = L, since the label

choosing Lii kernel matrix

= is

already centered. When we use a linear kernel on the data, ie. K = XX , we have

tr (KHLH)

=

m i,i =1

yiyi myi myi

xi

xi



=

dm


yiyi Xij Xi j  =

j=1 i,i =1 myi myi

d
(µj+ − µj−)2.
j=1

(5.16)

This proves that the centroid feature selector can be viewed as a special case of BAH-

SIC

in

the case

of

λ

=

1 2

.

From our

analysis

we

see

that

other

values of

λ

amount to

1The parametrization in [15] is different but it can be shown to be equivalent.

5.4 Connections to Other Approaches

71

effectively rescaling the patterns xi differently for different classes, which may lead to undesirable features being selected.

t-Statistic The normalization for the jth feature is computed as

1

s¯j =

s2j+ + s2j− m+ m−

2
.

(5.17)

In this case we deﬁne the t-statistic for the jth feature via tj = (µj+ − µj−)/s¯j.
Compared to the Pearson correlation, the key difference is that now we normalize each feature not by the overall sample standard deviation but rather by a value which takes each of the two classes separately into account.

Signal to noise ratio is yet another criterion to use in feature selection. The key idea is to normalize each feature by s¯j = sj+ + sj− instead. Subsequently the (µj+ − µj−)/s¯j are used to score features.

Moderated t-score is similar to t-statistic and is used for microarray analysis [126]. Its normalization for the jth feature is derived via a Bayes approach as

s˜j

=

ms¯2j + m0s¯20 , m + m0

(5.18)

where s¯j is from (5.17), and s¯0 and m0 are hyperparameters for the prior distribution on s¯j (all s¯j are assumed to be iid). s¯0 and m0 are estimated using information from all feature dimensions. This effectively borrows information from the ensemble of features to aid with the scoring of an individual feature. More speciﬁcally, s¯0 and m0 can be computed as [126]



m0

=

2Γ

−1

1 d

d
(zj − z¯)2 − Γ

j=1

s¯20 = exp

z¯ − Γ

m 2

+Γ

m0 2

 m 2 ,
− ln m0 m

,

(5.19) (5.20)

where Γ(·) is the gamma function,

denotes

derivative,

zj

=

ln(s¯2j )

and

z¯

=

1 d

d j=1

zj

.

B-statistic is the logarithm of the posterior odds (lods) that a feature is differentially expressed. [90, 126] show that, for large number of features, B-statistic is given by

Bj = a + bt˜2j ,

(5.21)

where both a and b are constant (b > 0), and t˜j is the moderated-t statistic for the jth feature. Here we see that Bj is monotonic increasing in t˜j, and thus results in the same
gene ranking as the moderated-t statistic.

The reason why these connections work is that the signal-to-noise ratio, moderated t-statistic, and B-statistic are three variants of the t-test. They differ only in their respective denominators, and are thus special cases of HSICb if we preprocess the data accordingly.

72 Feature Selection via Dependence Estimaton

Maximum Mean Discrepancy For binary classiﬁcation, an alternative criterion for selecting features is to check whether the distributions Pr(x|y = 1) and Pr(x|y = −1) differ and subsequently pick those coordinates of the data which primarily contribute to the difference between the two distributions.
More speciﬁcally, we could use Maximum Mean Discrepancy (MMD) [19], which is a generalization of mean difference for Reproducing Kernel Hilbert Spaces, given by

MMD =

Ex [k(x, ·)|y

= 1] − Ex [k(x, ·)|y

=

−1]

2 H

.

(5.22)

A biased estimator of the above quantity can be obtained simply by replacing expectations by

averages over a ﬁnite sample. We relate a biased estimator of MMD to HSICb again by setting

1 m+

as

the

labels

for

positive

samples

and

−

1 m−

for negative samples.

If we apply a linear

kernel on labels, L is automatically centered, i.e. L1 = 0 and HLH = L. This yields

tr (KHLH) = tr (KL)

1 m+

1 m−

2 m+ m−

= m2+ i,j k(xi, xj ) + m2− i,j k(xi, xj ) − m+m− i

k(xi, xj)
j

=

1 m+

m+ i

k(xi,

·)

−

1 m−

m−
k(xj, ·)
j

2
.
H

(5.23)

The quantity in the last line is an estimator of MMD with bias O(m−1) 4. This implies that
HSICb and the biased estimator of MMD are identical up to a constant factor. Since the bias of HSICb is also O(m−1), this effectively show that scaled MMD and HSICu converges to each other with rate O(m−1).

Kernel Target Alignment Alternatively, one could use Kernel Target Alignment (KTA) [31] to test directly whether there exists any correlation between data and labels. KTA has been used for feature selection in this context. Formally it is deﬁned as tr(KL)/ K Frob L Frob, that is, the cosine between the kernel matrix and the label matrix.
The nonlinear dependence on K makes it somewhat hard to optimize for. Indeed, for computational convenience the normalization is often omitted in practice [100], which leaves us with tr(KL), the corresponding estimator of MMD. Note the key difference, though, that normalization of L according to label size does not occur. Nor does KTA take centering into account. Whenever the sample sizes for both classes are approximately matched, such lack of normalization is negligible and we see that both criteria effectively check a similar criterion.
Hence in some cases in binary classiﬁcation, selecting features that maximizes HSIC also maximizes MMD and KTA. Note that in general (multiclass, regression, or generic binary classiﬁcation) this connection does not hold. Moreover, the use of HSIC offers uniform convergence bounds on the tails of the distribution of the estimators.

Shrunken Centroid The shrunken centroid (pam) method [136, 137] performs feature ranking using the differences from the class centroids to the centroid of all the data, ie.

(µj+ − µj)2 + (µj− − µj)2 ,

(5.24)

as a criterion to determine the relevance of a given feature. It also scores each feature separately.

5.4 Connections to Other Approaches

73

To show that this criterion is related to HSIC we need to devise an appropriate map for

the

labels

y.

Consider

the

feature

map

ψ(y)

with

ψ(1)

=

(

1 m+

,

0)

and

ψ(−1)

=

(0,

1 m−

)

.

Clearly, when applying H to Y we obtain the following centered effective feature maps

ψ¯(1) = ( 1

− 1 , − 1 ) and ψ¯(−1) = (− 1 ,

1

−

1 ).

m+ m m

m m− m

(5.25)

Consequently we may express tr(KHLH) via

tr (KHLH) =

1 m+

m+ i=1

xi

−

1 m

m i=1

xi

2
+

1 m−

m− i=1

xi

−

1 m

m i=1

xi

2


d
=
j=1

1 m+

1m

m+ i=1 Xij − m i=1 Xij

2
+

1 m−

1m

m− i=1 Xij − m i=1 Xij

2 

d
= (µj+ − µj)2 + (µj− − µj)2 .
j=1

(5.26)

This is the information used by the shrunken centroid method, hence we see that it can be seen

to be a special case of HSIC when using a linear kernel on the data and a speciﬁc feature map

on the labels. Note that we could assign different weights to the two classes, which would lead

to a weighted linear combination of distances from the centroid. Finally, it is straightforward

how this deﬁnition can be extended to multiclass settings, simply by considering the map ψ :

y

→

1 my

ey

.

Ridge Regression BAHSIC can also be used to select features for regression problems, except that in this case the labels are continuous variables. We could, in principle, use an RBF kernel or similar on the labels to address the feature selection issue. What we show now is that even for a simple linear kernel, interesting results can be obtained. More to the point, we show that feature selection using ridge regression can also be seen to arise as a special case of HSIC feature selection. We assume here that y is centered.
In ridge regression [67], we estimate the outputs y using the design matrix V and a parameter vector w by minimizing the following regularized risk functional

J = y − Vw 2 + λ w 2 .

(5.27)

Here the second term is known as the regularizer. If we choose V = X we obtain the family of linear models. In the general (nonlinear) case V may be an arbitrary matrix, where each row consists of a set of basis functions, e.g. a feature map φ(x). One might conclude that small values of J correspond to good sets of features, since there a w with small norm would still lead to a small approximation error. It turns out that J is minimized for w = (V V + λI)−1y. Hence the minimum is given by

J∗ = y y − y V(V V + λI)−1V y = constant − tr V(V V + λI)−1V yy .

(5.28)

74 Feature Selection via Dependence Estimaton

Whenever we are only given K = V V we have the following equality

J∗ = constant − tr K(K + λI)−1 yy .

(5.29)

This means that the matrices K¯ := V(V V + λI)−1V and K¯ := K(K + λI)−1

(5.30)

are equivalent kernel matrices to be used in BAHSIC. Note that instead of using yy as a kernel on the labels L we could use a nonlinear kernel in conjunction with the matrices arrived at from feature selection by ridge regression. It also generalizes the setting of [67] to situations other than regression.

Quadratic Mutual Information [138] introduces the quadratic mutual information for feature selection. That is, he uses the L2 distance between the joint and the marginal distributions on x and y as a criterion for how dependent the two distributions are:

QI(x, y) =

(d Prxy −d Prx d Pry)2 dxdy

X ×Y

(5.31)

In general, (5.31) is not efﬁciently computable. That said, when using a Parzen windows estimate of the joint and the marginals, it is possible to evaluate QI(x, y) explicitly. Since we only have a ﬁnite number of observations, one uses the estimates

1 pˆ(x) =
m

m

κx(xi − x),

i=1

1 pˆ(y) =
m

m

κy(yi − y),

i=1

1 pˆ(x, y) =
m

m

κx(xi − x)κy(yi − y).

i=1

(5.32a) (5.32b) (5.32c)

Here κx and κy are appropriate kernels of the Parzen windows density estimator. Denote by

Kij = κx(xi − x)κx(xj − x)dx and
X

(5.33)

Lij = κy(yi − y)κy(yj − y)dy
Y

(5.34)

inner products between Parzen windows kernels, and compile these entries into matrices K and L. Then we have

pˆ(x, y) − pˆ(x)pˆ(y)

2

=

1 m2

tr (KL) − 21

KL1 + 1

K11

L1

1 = m2 tr (KHLH) .

(5.35)

In other words, we obtain the same criterion as what can be derived from a biased estimator of HSIC. The key difference, though, is that this analogy only works whenever κx and κy can be seen to be arising from an inner product between Parzen windows kernel estimates. However,

5.5 Experiments on Benchmark Data

75

for graphs, trees or strings, the generalization of a density estimator can be difﬁcult, which poses a serious limitation. Moreover, since we are using a plug-in estimate of the densities, we inherit an innate slow-down of convergence due to the convergence of the density estimators. This issue is discussed in detail in [7].

Recursive Feature Elimination with Support Vectors Another popular feature selection algorithm is to use Support Vector Machines and to determine the relevance of features by the size of the induced margin as a solution of the dual optimization problem [65]. While the connection to BAHSIC is somewhat more tenuous in this context, it is still possible to recast this algorithm in our framework. Before we do so, we describe the basic idea of the method, using ν-SVM instead of plain C-SVMs: for ν-SVM without a constant offset b we have the following dual optimization problem [114].

1 minimize α
α2

(K ◦ L)α subject to α

1 = νm and αi ∈ [0, 1].

(5.36)

This problem is ﬁrst solved with respect to α for the full set of features. Features are then selected from (5.36) by removing coordinates such that the objective function decreases least (if at all). For computational convenience, α is not recomputed for a number of feature removals, since solving a quadratic program repeatedly is computationally expensive.
We now show that this procedure can be viewed as a special case of BAHSIC, where now the class of kernels, parameterized by σ is the one of conformal kernels. Given a base kernel k(x, x ) [5] propose the following kernel:

k¯(x, x ) = α(x)α(x )k(x, x ) where α(x) ≥ 0.

(5.37)

It is easy to see that

α (K ◦ L)α = y (diag α) K (diag α) y = y K¯ y,

(5.38)

where K¯ is the kernel matrix arising from the conformal kernel k¯(x, x ). Hence for ﬁxed α the objective function is given by a quantity which can be interpreted as a biased version of HSIC. Re-optimization with respect to α is consistent with the kernel adjustment step in Algorithm 1. The only difference being that here the kernel parameters are given by α rather than a kernel width σ. That said, it is also clear from the optimization problem that this style of feature selection may not be as desirable, since the choice of kernel parameters emphasizes only points close to the decision boundary.

5.5 Experiments on Benchmark Data
We analyze BAHSIC and related algorithms in an extensive set of experiments. The current section contains results on synthetic and real benchmark data, that is, data from Statlib, the UCI repository, and data from the NIPS feature selection challenge. Sections 5.6 and 5.7 then discusses applications to biological data, namely brain signal analysis and feature selection for microarrays.
Since the number of possible choices for feature selection within the BAHSIC family is huge, it is clearly impossible to investigate and compare all of them to all possible other feature selectors. In the present section we pick the following three feature selectors as representative

76 Feature Selection via Dependence Estimaton

examples. A wider range of kernels and choices is investigated in Section 5.6 and 5.7 in the context of bioinformatics applications.
In this section, we presents three concrete examples of BAHSIC which are used for our later experiments. We apply a Gaussian kernel k(x, x ) = exp(−σ x−x 2) on data, while varying the kernels on labels. These BAHSIC variants are dedicated respectively to the following setttings:

Binary classiﬁcation (BIN) Use the feature map in (5.9) and apply a linear kernel.

Multiclass classiﬁcation (MUL) Use the feature map in (5.10) and apply a linear kernel.

Regression problem (REG) Use the kernel in (5.12), i.e. a Gaussian RBF kernel on Y.

For the above variants a further speedup of BAHSIC is possible by updating entries in the

data kernel matrix incrementally. We use the fact that distance computation of a RBF kernel

decomposes into individual coordinates, i.e. we use that xi − xi 2 =

d j=1

Xij − Xi j

2.

Hence xi − xi 2 needs to be computed only once, and subsequent updates are effected by

subtracting Xij − Xi j 2.

We will use BIN, MUL and REG as the particular instances of BAHSIC in our experi-

ments. We will refer to them commonly as BAHSIC since the exact meaning will be clear

depending on the datasets encountered. Furthermore, we also instantiate FOHSIC using the

same kernels as BIN, MUL and REG, and we adopt the same convention when we refer to it in

our experiments.

5.5.1 Artiﬁcial Data
We constructed 3 artiﬁcial datasets, as illustrated in Figure 5.1, to illustrate the difference between BAHSIC variants with linear and nonlinear kernels. Each dataset has 22 dimensions — only the ﬁrst two dimensions are related to the prediction task and the rest are just Gaussian noise. These datasets are (i) Binary XOR data: samples belonging to the same class have multimodal distributions; (ii) Multiclass data: there are 4 classes but 3 of them are collinear; (iii) Nonlinear regression data: labels are related to the ﬁrst two dimension of the data by y = x(1) exp(−x(1)2 − x(2)2) + , where denotes additive Gaussian noise. We compare BAHSIC to FOHSIC, Pearson’s correlation, mutual information [156], and RELIEF (RELIEF works only for binary problems). We aim to show that when nonlinear dependencies exist in the data, BAHSIC with nonlinear kernels is very competent in ﬁnding them.
We instantiate the artiﬁcial datasets over a range of sample sizes (from 40 to 400), and plot the median rank, produced by various methods, for the ﬁrst two dimensions of the data. All numbers in Figure 5.1 are averaged over 10 runs. In all cases, BAHSIC shows good performance. More speciﬁcally, we observe:
Binary XOR Both BAHSIC and RELIEF correctly select the ﬁrst two dimensions of the data even for small sample sizes; while FOHSIC, Pearson’s correlation, and mutual information fail. This is because the latter three evaluate the goodness of each feature independently. Hence they are unable to capture nonlinear interaction between features.
Multiclass Data BAHSIC, FOHSIC and mutual information select the correct features irrespective of the size of the sample. Pearson’s correlation only works for large sample size. The collinearity of 3 classes provides linear correlation between the data and the labels, but due to the interference of the fourth class such correlation is picked up by Pearson’s correlation only for a large sample size.

5.5 Experiments on Benchmark Data

77

(a) (b) (c)
(d) (e) (f)
(g) (h) (i) Figure 5.1: Artiﬁcial datasets and the performance of different methods when varying the number of observations. The ﬁrst row contains plots for the ﬁrst 2 dimension of the (a) binary (b) multiclass and (c) regression data. Different classes are encoded with different colors. The second row plots the median rank (y-axis) of the two relevant features as a function of sample size (x-axis) for the corresponding datasets in the ﬁrst row. The third row plots median rank (y-axis) of the two relevant features produced in the ﬁrst iteration of BAHSIC as a function of the sample size. (Blue circle: Pearson’s correlation; Green triangle: RELIEF; Magenta downward triangle: mutual information; Black triangle: FOHSIC; Red square: BAHSIC. Note that RELIEF only works for binary classiﬁcation.)
Nonlinear Regression Data The performance of Pearson’s correlation and mutual information is slightly better than random. BAHSIC and FOHSIC quickly converge to the correct answer as the sample size increases.
In fact, we observe that as the sample size increases, BAHSIC is able to rank the relevant features (the ﬁrst two dimensions) correctly almost in the ﬁrst iteration. In the third row of Figure 5.1, we show the median rank of the relevant features produced in the ﬁrst iteration as a function of the sample size. It is clear from the pictures that BAHSIC effectively selects features in a single iteration when the sample size is large enough. For the regression case, we also see that BAHSIC with several iterations, indicated by the red square in Figure 5.1(f), slightly improves the correct ranking over BAHSIC with a single iteration, given by the blue

78 Feature Selection via Dependence Estimaton
square in Figure 5.1(i). While this does not prove BAHSIC with nonlinear kernels is always better than that with
a linear kernel, it illustrates the competence of BAHSIC in detecting nonlinear features. This is useful in a real-world situations. The second advantage of BAHSIC is that it is readily applicable to both classiﬁcation and regression problems, by simply choosing a different kernel on the labels.
5.5.2 Public Benchmark Data
Algorithms In this experiment, we show that the performance of BAHSIC can be comparable to other state-of-the-art feature selectors, namely SVM Recursive Feature Elimination (RFE) [65], RELIEF [81], L0-norm SVM (L0) [153], and R2W2 [154]. We used the implementation of these algorithms as given in the Spider machine learning toolbox, since those were the only publicly available implementations.2 Furthermore, we also include ﬁlter methods, namely FOHSIC, Pearson’s correlation (PC), and mutual information (MI), in our comparisons.
Datasets We used various real world datasets taken from the UCI repository,3 the Statlib repository,4 the LibSVM website,5 and the NIPS feature selection challenge6 for comparison. Due to scalability issues in Spider, we produced a balanced random sample of size less than 2000 for datasets with more than 2000 samples.
Experimental Protocol We report the performance of an SVM using a Gaussian kernel on a feature subset of size 5 and 10-fold cross-validation. These 5 features were selected per fold using different methods. Since we are comparing the selected features, we used the same SVM for all methods: a Gaussian kernel with σ set as the median distance between points in the sample [115] and regularization parameter C = 100. On classiﬁcation datasets, we measured the performance using the error rate, and on regression datasets we used the percentage of variance not-explained (also known as 1 − r2). The results for binary datasets are summarized in the ﬁrst part of Table 5.1. Those for multiclass and regression datasets are reported respectively in the second and the third parts of Table 5.1.
To provide a concise summary of the performance of various methods on binary datasets, we measured how the methods compare with the best performing one in each dataset in Table 5.1. We recorded the best absolute performance of all feature selectors as the baseline, and computed the distance of each algorithm to the best possible result. In this context it makes sense to penalize catastrophic failures more than small deviations. In other words, we would like to have a method which is almost always very close to the best performing one. Taking the 2 distance achieves this effect, by penalizing larger differences more heavily. It is also our goal to choose an algorithm that performs homogeneously well across all datasets. The 2 distance scores are listed for the binary datasets in Table 5.1. In general, the smaller the 2 distance, the better the method. In this respect, BAHSIC and FOHSIC have the best performance. We did not produce the 2 distance for multiclass and regression datasets, since the limited number of such datasets did not allow us to draw statistically signiﬁcant conclusions.
2http://www.kyb.tuebingen.mpg.de/bs/people/spider 3http://www.ics.uci.edu/ mlearn/MLSummary.html 4http://lib.stat.cmu.edu/datasets/ 5http://www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/ 6http://clopinet.com/isabelle/Projects/NIPS2003/

5.5 Experiments on Benchmark Data

79

Besides using 5 features, we also plot the performance of the learners as a function of the number of selected features for 9 datasets (Covertype, Ionosphere, Sonar, Satimage, Segment, Vehicle, Housing, Bodyfat and Abalone) in Figure 5.2. Generally speaking, the smaller the plotted number the better the performance of the corresponding learner. For multiclass and regression datasets, it is clear that the curves for BAHSIC very often lie along the lower bound of all methods. For binary classiﬁcation, however, SVM-RFE as a member of our framework performs the best in general. The advantage of BAHSIC becomes apparent when a small percentage of features is selected. For instance, BAHSIC is the best when only 5 features are selected from data set 1 and 2. Note that in these cases, the performance produced by BAHSIC is very close to that using all features. In a sense, BAHSIC is able to shortlist the most informative features.

(a) (b) (c)
(d) (e) (f)
(g) (h) (i) Figure 5.2: The performance of a classiﬁer or a regressor (vertical axes) as a function of the number of selected features (horizontal axes). Note that the maximum of the horizontal axes are equal to the total number of features in each data set. (a-c) Balanced error rate by a SVM classiﬁer on the binary data sets Covertype (1), Ionosphere (2) and Sonar (3) respectively; (d-f) balanced error rate by a one-versus-therest SVM classﬁer on multiclass data sets Satimage (22), Segment (23) and Vehicle (24) respectively; (g-i) percentage of variance not-explained by a SVR regressor on regression data set Housing (25), Body fat (26) and Abalone (27) respectively.

80 Feature Selection via Dependence Estimaton

Table 5.1: Classiﬁcation error (%) or percentage of variance not-explained (%). The best result, and
those results not signiﬁcantly worse than it, are highlighted in bold (one-sided Welch t-test with 95% conﬁdence level). 100.0±0.0∗: program is not ﬁnished in a week or crashed. -: not applicable.

Data BAHSIC FOHSIC PC

MI RFE RELIEF L0

R2W2

covertype 26.3±1.5 37.9±1.7 40.3±1.3 26.7±1.1 33.0±1.9 42.7±0.7 43.4±0.7 44.2±1.7

ionosphere 12.3±1.7 12.8±1.6 12.3±1.5 13.1±1.7 20.2±2.2 11.7±2.0 35.9±0.4 13.7±2.7

sonar 27.9±3.1 25.0±2.3 25.5±2.4 26.9±1.9 21.6±3.4 24.0±2.4 36.5±3.3 32.3±1.8

heart 14.8±2.4 14.4±2.4 16.7±2.4 15.2±2.5 21.9±3.0 21.9±3.4 30.7±2.8 19.3±2.6

breastcancer 3.8±0.4 3.8±0.4 4.0±0.4 3.5±0.5 3.4±0.6 3.1±0.3 32.7±2.3 3.4±0.4

australian 14.3±1.3 14.3±1.3 14.5±1.3 14.5±1.3 14.8±1.2 14.5±1.3 35.9±1.0 14.5±1.3

splice 22.6±1.1 22.6±1.1 22.8±0.9 21.9±1.0 20.7±1.0 22.3±1.0 45.2±1.2 24.0±1.0

svmguide3 20.8±0.6 20.9±0.6 21.2±0.6 20.4±0.7 21.0±0.7 21.6±0.4 23.3±0.3 23.9±0.2

adult 24.8±0.2 24.4±0.6 18.3±1.1 21.6±1.1 21.3±0.9 24.4±0.2 24.7±0.1 100.0±0.0∗

cleveland 19.0±2.1 20.5±1.9 21.9±1.7 19.5±2.2 20.9±2.1 22.4±2.5 25.2±0.6 21.5±1.3

derm 0.3±0.3 0.3±0.3 0.3±0.3 0.3±0.3 0.3±0.3 0.3±0.3 24.3±2.6 0.3±0.3

hepatitis 13.8±3.5 15.0±2.5 15.0±4.1 15.0±4.1 15.0±2.5 17.5±2.0 16.3±1.9 17.5±2.0

musk 29.9±2.5 29.6±1.8 26.9±2.0 31.9±2.0 34.7±2.5 27.7±1.6 42.6±2.2 36.4±2.4

optdigits 0.5±0.2 0.5±0.2 0.5±0.2 3.4±0.6 3.0±1.6 0.9±0.3 12.5±1.7 0.8±0.3

specft 20.0±2.8 20.0±2.8 18.8±3.4 18.8±3.4 37.5±6.7 26.3±3.5 36.3±4.4 31.3±3.4

wdbc 5.3±0.6 5.3±0.6 5.3±0.7 6.7±0.5 7.7±1.8 7.2±1.0 16.7±2.7 6.8±1.2

wine 1.7±1.1 1.7±1.1 1.7±1.1 1.7±1.1 3.4±1.4 4.2±1.9 25.1±7.2 1.7±1.1

german 29.2±1.9 29.2±1.8 26.2±1.5 26.2±1.7 27.2±2.4 33.2±1.1 32.0±0.0 24.8±1.4

gisette 12.4±1.0 13.0±0.9 16.0±0.7 50.0±0.0 42.8±1.3 16.7±0.6 42.7±0.7 100.0±0.0∗

arcene 22.0±5.1 19.0±3.1 31.0±3.5 45.0±2.7 34.0±4.5 30.0±3.9 46.0±6.2 32.0±5.5

madelon 37.9±0.8 38.0±0.7 38.4±0.6 51.6±1.0 41.5±0.8 38.6±0.7 51.3±1.1 100.0±0.0∗

2 11.2 14.8 19.7 48.6 42.2 25.9 85.0 138.3

satimage 15.8±1.0 17.9±0.8 52.6±1.7 22.7±0.9 18.7±1.3 - 22.1±1.8 segment 28.6±1.3 33.9±0.9 22.9±0.5 27.1±1.3 24.5±0.8 - 68.7±7.1 vehicle 36.4±1.5 48.7±2.2 42.8±1.4 45.8±2.5 35.7±1.3 - 40.7±1.4 svmguide2 22.8±2.7 22.2±2.8 26.4±2.5 27.4±1.6 35.6±1.3 - 34.5±1.7 vowel 44.7±2.0 44.7±2.0 48.1±2.0 45.4±2.2 51.9±2.0 - 85.6±1.0
usps 43.4±1.3 43.4±1.3 73.7±2.2 67.8±1.8 55.8±2.6 - 67.0±2.2

-

housing 18.5±2.6 18.9±3.6 25.3±2.5 18.9±2.7 - - bodyfat 3.5±2.5 3.5±2.5 3.4±2.5 3.4±2.5 - - abalone 55.1±2.7 55.9±2.9 54.2±3.3 56.5±2.6 - - -

-

5.6 Analysis of Brain Computer Interface Data
In this experiment, we show that BAHSIC selects features that are meaningful in practice. Here we use it to select frequency bands for a brain-computer interface (BCI) dataset from the Berlin BCI group [39]. The data contains EEG signals (118 channels, sampled at 100 Hz) from ﬁve healthy subjects (‘aa’, ‘al’, ‘av’, ‘aw’ and ‘ay’) recorded during two types of motor imaginations. The task is to classify the imagination for individual trials.
Our experiment proceeds in 3 steps: (i) A Fast Fourier transformation (FFT) is performed on each channel and the power spectrum is computed. (ii) The power spectra from all channels

