Knowledge-Based Systems 80 (2015) 14–23

Contents lists available at ScienceDirect

Knowledge-Based Systems
journal homepage: www.elsevier.com/locate/knosys

Transfer learning using computational intelligence: A survey
Jie Lu ⇑, Vahid Behbood, Peng Hao, Hua Zuo, Shan Xue, Guangquan Zhang
Decision Systems & e-Service Intelligence Lab, Centre for Quantum Computation & Intelligent Systems, Faculty of Engineering and Information Technology, University of Technology Sydney, PO Box 123, Broadway, NSW 2007, Australia

a r t i c l e

i n f o

a b s t r a c t
Transfer learning aims to provide a framework to utilize previously-acquired knowledge to solve new but similar problems much more quickly and effectively. In contrast to classical machine learning methods, transfer learning methods exploit the knowledge accumulated from data in auxiliary domains to facilitate predictive modeling consisting of different data patterns in the current domain. To improve the performance of existing transfer learning methods and handle the knowledge transfer process in real-world systems, computational intelligence has recently been applied in transfer learning. This paper systematically examines computational intelligence-based transfer learning techniques and clusters related technique developments into four main categories: (a) neural network-based transfer learning; (b) Bayes-based transfer learning; (c) fuzzy transfer learning, and (d) applications of computational intelligence-based transfer learning. By providing state-of-the-art knowledge, this survey will directly support researchers and practice-based professionals to understand the developments in computational intelligence-based transfer learning research and applications. Ó 2015 Elsevier B.V. All rights reserved.

Article history: Received 3 December 2014 Received in revised form 7 January 2015 Accepted 17 January 2015 Available online 22 January 2015 Keywords: Transfer learning Computational intelligence Neural network Bayes Fuzzy sets and systems Genetic algorithm

1. Introduction Although machine learning technologies have attracted a remarkable level of attention from researchers in different computational ﬁelds, most of these technologies work under the common assumption that the training data (source domain) and the test data (target domain) have identical feature spaces with underlying distribution. As a result, once the feature space or the feature distribution of the test data changes, the prediction models cannot be used and must be rebuilt and retrained from scratch using newlycollected training data, which is very expensive and sometimes not practically possible. Similarly, since learning-based models need adequate labeled data for training, it is nearly impossible to establish a learning-based model for a target domain which has very few labeled data available for supervised learning. If we can transfer and exploit the knowledge from an existing similar but not identical source domain with plenty of labeled data, however, we can pave the way for construction of the learning-based model for the target domain. In real world scenarios, there are many situations in which very few labeled data are available, and collecting

⇑ Corresponding author.
E-mail addresses: jie.lu@uts.edu.au (J. Lu), vahid.behbood@uts.edu.au (V. Behbood), peng.hao@student.uts.edu.au (P. Hao), hua.zuo@student.uts.edu.au (H. Zuo), shan.xue@student.uts.edu.au (S. Xue), guangquan.zhang@uts.edu.au (G. Zhang). http://dx.doi.org/10.1016/j.knosys.2015.01.010 0950-7051/Ó 2015 Elsevier B.V. All rights reserved.

new labeled training data and forming a particular model are practically impossible. Transfer learning has emerged in the computer science literature as a means of transferring knowledge from a source domain to a target domain. Unlike traditional machine learning and semi-supervised algorithms [1–4], transfer learning considers that the domains of the training data and the test data may be different [5]. Traditional machine learning algorithms make predictions on the future data using mathematical models that are trained on previously collected labeled or unlabeled training data which is the same as future data [6–8]. Transfer learning, in contrast, allows the domains, tasks, and distributions used in training and testing to be different. In the real world, we observe many examples of transfer learning. We may ﬁnd that learning to recognize apples might help us to recognize pears, or learning to play the electronic organ may facilitate learning the piano. The study of transfer learning has been inspired by the fact that human beings can utilize previously-acquired knowledge to solve new but similar problems much more quickly and effectively. The fundamental motivation for transfer learning in the ﬁeld of machine learning focuses on the need for lifelong machine learning methods that retain and reuse previously learned knowledge. Research on transfer learning has been undertaken since 1995 under a variety of names: learning to learn; life-long learning; knowledge transfer; meta learning; inductive transfer; knowledge consolidation; context sensitive learning and multi-task learning [9]. In 2005, the

J. Lu et al. / Knowledge-Based Systems 80 (2015) 14–23

15

Broad Agency Announcement of the Defense Advanced Research Projects Agency’s Information Processing Technology Ofﬁce gave a new mission to transfer learning: the ability of a system to recognize and apply knowledge and skills learned in previous tasks to novel tasks. In this deﬁnition, transfer learning aims to extract the knowledge from one or more source tasks and then apply the knowledge to a target task. Traditional machine learning techniques only try to learn each task from scratch, while transfer learning techniques try to transfer the knowledge from other tasks and/or domains to a target task when the latter has few high-quality training data. Several survey papers on transfer learning have been published in the last few years. For example, the paper by [9] presented an extensive overview of transfer learning and different categories. However, these papers focus on transfer learning techniques and approaches only; none of them discusses how the computational intelligence approach can be used in transfer learning. Since the computational intelligence approach has been applied in transfer learning more recently and has already demonstrated its advantage, this survey is timely. There are three main types of articles being reviewed in this survey: Type 1 – articles on transfer learning techniques (including related methods and approaches) and Type 2 – articles on transfer learning using computational intelligence techniques. Type 3 – articles on related computational intelligence techniques. The search and selection of these articles were performed according to the following ﬁve steps: Step 1. Publication database identiﬁcation and determination: The eminent publication databases such as Science Direct, ACM Digital Library, IEEE Xplore and SpringerLink, were searched to provide a comprehensive bibliography of research papers on transfer learning and transfer learning using computational intelligence. Step 2. Type 1 article selection: These papers were selected according to the two criteria: (1) novelty; and (2) impact- published in high quality (high impact factor) journals, or in conference proceedings or book chapters but with high citations1. These types of article are mainly used in Section 2. Step 3. Preliminary screening of Type 2 articles: The search was ﬁrst performed based on related keywords of computational intelligence in transfer learning. Step 4. Result ﬁltering for Type 2 articles: The keywords of the preliminary references were extracted and clustered manually. Based on the keywords related to application domain, these papers were divided, using ‘‘topic clustering’’, into four groups: (a) Neural Network in transfer learning; (b) Bayes in transfer learning; (c) fuzzy and genetic algorithm in transfer learning and (d) application of transfer learning. This article selection process was based on the following criteria: (1) novelty – published within the last few years; (2) impact – see Step 2; (3) coverage – reported a new or particular application domain; and (4) typicality – only the most typical methodology and applications were retained. Step 5. Type 3 article selection: These papers were selected according to the requirement of Step 4, aiming to introduce related concepts of computational intelligence techniques. The main contributions of this paper are: (1) it comprehensively and perceptively summarizes research achievements on transfer learning from the point of view of applications of computational intelligence, and strategically clusters the transfer learning into
‘‘high citation’’ means that the citation of the paper is greater than the average citation rates listed in the ‘‘ISI Web of Knowledge – Essential Science Indicators’’, and the citation per year of the paper is larger than 1.
1

four computational intelligence application domains; (2) for each computational intelligence technique it carefully analyses typical transfer learning frameworks and effectively identiﬁes the speciﬁc requirements of computational intelligence techniques in transfer learning. This will directly support researchers and practitioners to promote the popularization and application of computational intelligence in transfer learning in different domains; and (3) it also covers several very new transfer learning techniques with computational intelligence, and reveals their successful applications. The remainder of this paper is structured as follows. In Section 2, the transfer learning techniques are reviewed and analyzed. Sections 3–5 respectively present the 4 main application domains of transfer learning. Section 6 discusses the applications of computational intelligence-based transfer learning methods. Section 7 presents our analysis and main ﬁndings. 2. Basic transfer learning techniques To understand and analyze the application developments of transfer learning by using computational intelligence, this section ﬁrst reviews the main transfer learning techniques. The notations and deﬁnitions that will be used throughout the section are introduced. According to the deﬁnitions, we then categorize the various settings of transfer learning methods that exist in the literature of machine learning. Deﬁnition 2.1 (Domain [9]). A domain, which is denoted by D ¼ fv; PðXÞg, consists of two components: (1) Feature space v; and (2) Marginal probability X ¼ fx1 ; . . . ; xn g 2 v. distribution PðXÞ, where

Deﬁnition 2.2 (Task [9]). A task, which is denoted by T ¼ fY; f ðÁÞg, consists of two components: (1) A label space Y ¼ fy1 ; . . . ; ym g; and (2) An objective predictive function f ðÁÞ which is not observed and is to be learned by pairs fxi ; yi g. The function f ðÁÞ can be used to predict the corresponding label, f ðxi Þ, of a new instance xi . From a probabilistic viewpoint, f ðxi Þ can be written as Pðyi jxi Þ. In the bank failure prediction example, which is a binary prediction task, yi can be the label of failed or survived. More speciﬁcally, the source domain can be denoted as Ds ¼ fðxs1 ; ys1 Þ; . . . ; ðxsn ; ysn Þg where xsi 2 vs is the source instance or bank in the bank failure prediction example and ysi 2 Y s is the corresponding class label which can be failed or survived for bank failure prediction. Similarly, the target domain can be denoted as Dt ¼ fðxt1 ; yt1 Þ; . . . ; ðxtn ; ytn Þg where xt 2 vt is the target instance and yti 2 Y t is the corresponding class label and in most scenarios tn ( sn . Deﬁnition 2.3 (Transfer and learning task T s , a transfer learning aims predictive function f t ðÁÞ where Ds –Dt or T s –T t . learning [9]). Given a source domain Ds target domain Dt and learning task T t , to improve the learning of the target in Dt using the knowledge in Ds and T s

In the above deﬁnition, the condition Ds –Dt implies that either

vs –vt or Ps ðXÞ–Pt ðXÞ. Similarly, the condition T s –T t implies that
either Y s –Y t or f s ðÁÞ–f t ðÁÞ. In addition, there are some explicit or implicit relationships between the feature spaces of two domains such that we imply that the source domain and target domain are related. It should be mentioned that when the target and

16

J. Lu et al. / Knowledge-Based Systems 80 (2015) 14–23

source domains are the same (Ds ¼ Dt ) and their learning tasks are also the same (T s ¼ T t ), the learning problem becomes a traditional machine learning problem. According to the uniform deﬁnition of transfer learning introduced by Deﬁnition 2.3, transfer learning techniques can be divided into three main categories [9]: (1) Inductive transfer learning, in which the learning task in the target domain is different from the target task in the source domain (T s –T t ); (2) Unsupervised transfer learning which is similar to inductive transfer learning but focuses on solving unsupervised learning tasks in the target domain such as clustering, dimensionality reduction and density estimation (T s –T t ); and (3) Transductive transfer learning, in which the learning tasks are the same in both domains, while the source and target domains are different (T s ¼ T t ; Ds –Dt Þ. In the literature, transductive transfer learning, domain adaptation, covariate shift, sample selection bias, transfer learning, multi-task learning, robust learning, and concept drift are all terms which have been used to handle the related scenarios. More speciﬁcally, when the method aims to optimize the performance on multiple tasks or domains simultaneously, it is considered to be multi-task learning. If it optimizes performance on one domain, given training data that is from a different but related domain, it is considered to be transductive transfer learning or domain adaptation. Transfer learning and transductive transfer learning have often been used interchangeably with domain adaptation. Concept drift refers to a scenario in which data arrives sequentially with changing distribution, and the goal is to predict the next batch given the previously-arrived data [10]. The goal of robust learning is to build a classiﬁer that is less sensitive to certain types of change, such as feature change or deletion in the test data. In addition, unsupervised domain adaptation can be considered as a form of semi-supervised learning, but it assumes that the labeled training data and the unlabeled test data are drawn from different distributions. The existing techniques and methods, which have thus far been used to handle the domain adaptation problem, can be divided into four main classes [11]: (1) Instance weighting for covariate shift methods which weight samples in the source domain to match the target domain. The covariate shift scenario might arise in cases where the training data has been biased toward one region of the input space or is selected in a non-I.I.D. manner. It is closely related to the idea of sample-selection bias which has long been studied in statistics [12] and in recent years it has been explored for machine-learning. Huang et al. [13] proposed a novel procedure called Kernel Mean Matching (KMM) to estimate weights on each instance in the source domain, based on the goal of making the weighted distribution of the source domain look similar to the distribution of the target domain. Sugiyama et al. [14] and Tsuboi et al. [15] proposed a similar idea called the Kullback–Leibler Importance Estimation Procedure (KLIEP). Here too the goal is to estimate weights to maximize similarity between the target and weight-corrected source distributions. (2) Self-labeling methods which include unlabeled target domain samples in the training process and initialize their labels and then iteratively reﬁne the labels. Self-training has a close relationship with the Expectation Maximization (EM) algorithm, which has hard and soft versions. The hard version adds samples with single certain labels while the soft version assigns label conﬁdences when ﬁtting the model. Tan et al. [16] modiﬁed the relative contributions of the source and target domains in EM. They increased the weight on the target data at each iteration, while Dai et al. [17] speciﬁed the tradeoff between the source and target data terms by estimating KL divergence between the source and target

distributions, placing more weight on the target data as KL divergence increases. Self-training methods have been applied to domain adaptation on Natural Language Processing (NLP) tasks including parsing [18–21]; part-of-speech tagging [22]; conversation summarization [23]; entity recognition [22,24,25]; sentiment classiﬁcation [26]; spam detection [22]; cross-language document classiﬁcation [27,28]; and speech act classiﬁcation [29]. (3) Feature representation methods which try to ﬁnd a new feature representation of the data, either to make the target and source distributions look similar, or to ﬁnd an abstracted representation for domain-speciﬁc features. The feature representation approaches can be categorized into two classes [11]: (A) Distribution similarity approaches aim explicitly to make the source and target domain sample distributions similar, either by penalizing or removing features whose statistics vary between domains [24,30–32] or by learning a feature space projection in which a distribution divergence statistic is minimized [33–35]; (B) Latent feature approaches aim to construct new features by analyzing large amounts of unlabeled source and target domain data [25,36–42]. (4) Cluster-based learning methods rely on the assumption that samples connected by high-density paths are likely to have the same label if there is a high density path between them [43]. These methods aim to construct a graph in which the labeled and unlabeled samples are the nodes, with the edge weights among samples based on their similarity. Dai et al. [17] proposed a co-clustering based algorithm to propagate the label information across domains for document classiﬁcation. Xue et al. [44] proposed a cross-domain text classiﬁcation algorithm known as TPLSA to integrate labeled and unlabeled data from different but related domains. 3. Transfer learning using neural network Neural Network aims to solve complex non-linear problems using a learning-based method inspired by human brain structure and processes. In classical machine learning problems, many studies have demonstrated the superior performance of neural network compared to statistical methods. This fact has encouraged many researchers to use neural network for transfer learning, particularly in complicated problems. To address the problem in transfer learning, a number of neural network-based transfer learning algorithms have been developed in recent years. This section reviews three of the principal Neural Network techniques: Deep Neural Network, Multiple Tasks Neural Network, and Radial Basis Function Neural Network, and presents their applications in transfer learning. 3.1. Transfer learning using deep neural network Deep neural network is considered to be an intelligent feature extraction module that offers great ﬂexibility in extracting highlevel features in transfer learning. The prominent characteristic of deep neural network is its multiple hidden layers, which can capture the intricate non-linear representations of data. The core idea of a deep neural network is that unsupervised learning is used to pre-train each layer, and the output is then the input of the next layer. Ultimately, all layers are ﬁne-tuned in a supervised learning way. Hubel and Wiesel [45] proposed multi-stage Hubel–Wiesel architectures that consist of alternating layers of convolutions and max pooling to extract data features. A new model blending the above structure and multiple tasks is proposed for transfer learning [46]. In this model, a target task and related tasks are trained together with shared input and hidden layers, and separately output neurons. The model is then extended to the case in which each task has multiple output neurons [47]. Likewise,

J. Lu et al. / Knowledge-Based Systems 80 (2015) 14–23

17

based on the multi-stage Hubel–Wiesel architectures, whether shared hidden layers trained by the source task can be reused on a different target task is detected. For the target task model, only the last classiﬁcation layer needs to be retrained, but any layer of the new model could be ﬁne-tuned if desired. In this case, the parameters of hidden layers in the source task model act as initialization parameters of the new target task model, and this strategy is especially promising for a model in which good initialization is very important [48]. As mentioned above, generally all the layers except the last layer are treated as feature extractors in a deep neural network. In contrast to this network structure, a new feature extractor structure is proposed by Collobert and Weston [49]. Only the ﬁrst two layers are used to extract features at different levels, such as word level and sentence level in Natural Language Processing. Subsequent layers are classical neural network layers used for prediction. The Stacked Denoising Autoencoder (SDA) is another structure that is presented in deep neural network [50]. Based on the SDA model, different feature transference strategies are introduced to target tasks with varying degrees of complexity. The number of layers transferred to the new model depends on the high-level or low-level feature representations that are needed. This means if low-level features are needed, only the ﬁrst layer parameters are transferred to the target task [50]. In addition, an interpolating path is presented to transfer knowledge from the source task to the target task in a deep neural network. The original high dimensional features of the source and target domains are projected to lower dimensional subspaces that lie on the Grassman manifold, which presents a way to interpolate smoothly between the source and target domains; thus, a series of feature sets is generated on the interpolating path and intermediate feature extractors are formed based on deep neural network [51]. Deep neural networks can also combine with other technology to promote the performance of transfer learning. Swietojanski et al. [52] applied restricted Boltzmann machine to pre-train deep neural network, and the outputs of the network are used as features for a hidden Markov model. 3.2. Transfer learning using multiple task neural network To improve the learning for the target task, multiple task learning (MTL) is proposed. Information contained in other related tasks is used to promote the performance of target task [53]. In multiple task neural network learning, all tasks are trained in parallel using the shared input and hidden neurons and separate output neurons depending on different tasks [54]. The biggest difference between the MTL here and the MTL in deep neural network is the number of hidden layers. Generally, the number of hidden layers in MTL is much smaller than in deep neural networks. In MTL, secondary tasks as auxiliary information help the primary task to improve performance. However, due to different relatedness between secondary tasks and the primary task, the contributions of secondary tasks should be distinguished. Therefore, a modiﬁed version of multitask learning called g MTL is introduced. Based on a measure of relatedness between secondary tasks and the primary task, g MTL applies a separate learning rate for each task output neuron [55]. Silver and Mercer [56] presented a task rehearsal method (TRM) to transfer knowledge of secondary tasks to the primary task at a functional level. Instead of the interrelation between representations of various tasks, the relationship between functions of tasks is the core content in their new model. After demonstrating the good performance of g MTL and TRM on synthetic tasks, they were practically applied to a series of medical diagnostic tasks [57]. In the MTL model, the output layer consists of a separate neuron corresponding to each task, which may lead to redundant outputs and overlapping information. In addition, for the continuous tasks, contextual cues must be provided to guide the system to associate

an example with a particular task. In light of these problems, Silver and Poirier [58] proposed context-sensitive multiple task learning (csMTL) with two major differences. To eliminate the redundant outputs and reduce the free parameters, only one neuron is included in the output layer. Another difference is reﬂected in the input layer, which can be divided into two parts. Apart from the set of input variables for the tasks, the input layer also contains a set of context inputs that associates each training example with a particular task. To verify the effectiveness of csMTL, a set of experiments was designed to detect csMTL and MTL neural networks in their ability to transfer knowledge [59]. The above model makes the assumption that each task only has one output neuron. Further, csMTL is extended to learn tasks that have multiple output neurons [60]. 3.3. Transfer learning using radial basis function neural networks Yamauchi [61] considered covariate shift, one category of transfer learning, and incremental learning. Under the assumption that incremental learning environments are a subset of covariate shift, a novel incremental learning method is presented on the basis of radial basis function neural network. Further, a number of model-selection criteria are set up to optimize the network; for example, the information criterion [62] is applied to determine the number of hidden neurons [63]. In some literatures, neural network acts as a part of the whole algorithm. Liu et al. [64] applied neural network to initialize the weights of labeled data in the source domain. Each instance in the source domain is input into the neural network trained by limited target labeled data to gain the contribution degree based on the error value. In addition, the neural network is used as pre-processing technique to extract features from high dimensional space to low dimensional space [65]. Sometimes, neural network is combined with other intelligent techniques to form an integrated model to improve the performance of transfer learning [66]. 4. Transfer learning using Bayes Bayesian techniques refer to methods that are related to statistical inference and are developed based on Bayesian theorem. A Bayesian classiﬁer is a probabilistic methodology for solving classiﬁcation problems. Since probability is a useful tool for modeling the uncertainty in the real world and is adequate for quantifying the certainty degree of an uncertain truth, Bayesian classiﬁer is popular in the machine learning community. When it comes to the transfer learning setting, the distribution of the training data and test data is not identical, so a Bayesian classiﬁer trained on training data may not be predictive for the test data. To address this challenging problem, many Bayesian-based transfer learning algorithms have been developed in recent years. This section reviews three of the main Bayesian techniques: naïve Bayes classiﬁer, Bayesian network and the hierarchical Bayesian model, and illustrates their application within the framework of transfer learning. 4.1. Transfer learning using Naïve Bayes The naïve Bayes classiﬁers [67] are among the most popular classiﬁers in real world application. They pose a simple but strong assumption that there is independence between each pair of features (attributes) given the class variables. Though this assumption is not suitable in most real scenarios, naïve Bayes classiﬁers have nevertheless been proved to work quite well in some complicated applications, especially automatic medical diagnosis [68], spam ﬁltering [69] and text categorization [70], in which they may even

18

J. Lu et al. / Knowledge-Based Systems 80 (2015) 14–23

outperform more advanced algorithms, such as support vector machine, or random forests. Normally, the probabilistic model for a classiﬁer is

pðCjF 1 ; . . . ; F n Þ ¼

pðCÞpðF 1 ; . . . ; F n jCÞ pðF 1 ; . . . ; F n Þ

ð1Þ

where pðCjF 1 ; . . . ; F n Þ indicates a posteriori probability of class variable C, conditional on feature variables F1 through Fn. Since pðF 1 ; . . . ; F n Þ has no relation with the class variable and the value of Fi (i = 1, . . . , n) is observable, the above equation can be expressed as

The implementation can be summarized in three steps: it ﬁrst collects maximum and minimum value vectors of the target feature from test data, then each feature of a training sample is compared with the corresponding part of those two vectors to calculate the number of similar attributes and the weight of that training instance is computed through a gravitational analogy. After obtaining all the weights for the training data, a prediction model can be built with those weighted training data to classify the test dataset. 4.2. Transfer learning using Bayesian network Assuming total independence between features is not applicable for many real world problems, because the occurrence of an event may arise as the result of a number of relevant factors. In other words, there are correlations between features in a decision region and the Bayesian network is a suitable representation to this fact. A Bayesian network is a graphical model that encodes probabilistic relationships among variables of interest. It consists of two components: (1) a directed acyclic graph (DAG), which contains nodes and arcs. In particular, the nodes can be observed quantities, latent variables, or unknown parameters, while the directed arcs reﬂect conditional dependencies among variables, and (2) conditional probability tables (CPTs), which record local probability distributions associated with each node. Bayesian networks have four distinct advantages when compared to other data mining methods, namely, the ability to handle incomplete datasets, to discover causal relationships hidden in the data, to incorporate both domain knowledge and data into one model, and to avoid data over-ﬁtting [73]. In a simple case, the graphical model of a Bayesian network can be constructed by the prior knowledge of an expert. However in some complex applications, the deﬁnition of ‘‘network’’ is difﬁcult for humans, so it is necessary to learn the network structure and parameters of the local distributions from data [74]. To learn a Bayesian network from data, one needs to consider two important phases: structure learning and parameter learning, respectively. The former relates to the learning of a graphical model from data, while the latter deals with the evaluation of condition probability distribution for each variable given the model. To our knowledge, most works focus on structure learning by leveraging previous data, and less effort is expended on parameter learning. When the training data in a task is scarce, learning a reliable Bayesian network is difﬁcult; therefore transfer learning can help improve the robustness of learned networks through exploiting data from related tasks or knowledge from domain experts. In [75], the authors extended the Bayesian network learning from a single domain (task) to multiple domains (tasks). In this case, instead of learning a structure in isolation, the relationships between tasks should be taken into account. Similar to the multitask learning scenario, multiple Bayesian network structures are jointly built from multiple related datasets. To make learning efﬁcient, the parameters of Bayesian networks from related tasks are assumed to be independent. The prior is deﬁned in such a way that it penalizes structures that are different from one another. A score and search approach is then performed on the space of multiple Bayesian networks to ﬁnd the best structures, in particular, by deﬁning a suitable search space and devising a branch and bound procedure that enables efﬁcient moves in this search space. In contrast to learning optimal models simultaneously for different tasks, [76] proposed learning models from auxiliary tasks to improve related tasks. In this paper, without giving sufﬁcient data for independence test, a PC–TL algorithm is developed with consideration of both the conﬁdence of the independence tests and the similarity of the auxiliary tasks to the target task in a combined function. An example that uses transfer learning to strengthen the quality of learned Bayesian networks through the use of an inductive bias

pðCjF 1 ; . . . ; F n Þ / pðCÞpðF 1 ; . . . ; F n jCÞ

ð2Þ

Under the independence assumption adopted by naïve Bayes classiﬁer, which means

pðCjF 1 ; . . . ; F n Þ / pðCÞ

n Y i¼1

pðF i jCÞ

ð3Þ

From Eq. (3) we ﬁnd that a prediction made by a classiﬁer depends on the prior probability of the class variable and the product of the likelihood of each feature variable given a speciﬁc class variable. To estimate each feature’s distribution, it is necessary to make parameter estimation, assuming a predeﬁned distribution (i.e., multinomial distribution or multivariate Bernoulli distribution) or generating a non-parametric model for a feature that comes from training data. However, if the test data (new-domain data) follow a different distribution from the training data (old-domain data), we cannot obtain an accurate feature distribution estimation for the new-domain data based on the parameter learned from the old-domain data, which leads to bad prediction performance in the result. Estimating the feature distribution for new-domain unlabeled data limits the application of the naïve Bayes classiﬁer in the transfer learning setting. To adapt the naïve Bayes classiﬁer from the training data to the test data, [17] proposed a novel naïve Bayes transfer learning (NBTL) classiﬁcation algorithm for text categorization. NBTL ﬁrst trains a naïve Bayes classiﬁer on the training data and applies the learned classiﬁer on the test data to obtain a pseudo label for the test data during learning, thereby providing an initial model estimation for the test data under target distribution. The Expectation–Maximization (EM) algorithm is then applied in iteration to ﬁnd a local optimal model only for ﬁtting the target distribution, meaning that the naïve Bayes classiﬁer trained on the training data is adapted to the test data. To measure the difference between the different distributions, KL divergence is used to estimate a tradeoff parameter in the NBTL model, and the experiment results show that the performance of NBTL increases when the distribution between the training data and the test data is signiﬁcantly different. The main disadvantage of NBTL lies in the fact that the inﬂuence of new-domain speciﬁc features is ignored. Instead of treating both old-domain and new-domain data equally, an adaptive naïve Bayes is proposed in [16]. It uses a weighted EM algorithm to dynamically increase the importance of new-domain data and decrease the weight of old data, while at the same time emphasizing the usage of both generalizable features drawn from the old-domain data and all the features from the new-domain data for tackling the cross-domain sentiment classiﬁcation problem. Roy and Kaelbling [71] developed an alternative method of transferring the naïve Bayes classiﬁer. They ﬁrst partition the dataset into a number of clusters, such that the data for each cluster for all tasks has the same distribution. Then they train one classiﬁer for each partition; all classiﬁers are then combined using a Dirichlet process. In addition to text classiﬁcation, [72] developed a transfer naïve Bayes (TNB) algorithm to predict cross-company software defects.

J. Lu et al. / Knowledge-Based Systems 80 (2015) 14–23

19

may also be found in [77]. The main limitation of such multi-task network structure learning algorithms lies in the assumption that all pairs of tasks are equally related, which violates the truth that different pairs of tasks can differ in their degree of relatedness. As a result, Oyen and Lane [78] relaxed this assumption by adding a task relatedness metric, which explicitly controls the amount of information sharing between tasks, into a learning objective to incorporate domain knowledge about task-relatedness. Experimental results show that leveraging domain knowledge produces models that are both robust and in accordance with a domain expert’s objective. Recently, Oyen and Lane [79] pointed out that it is more appropriate to estimate a posterior distribution over multiple learned Bayesian networks rather than a single posteriori. In their paper, the authors proposed the incorporation of structure bias into order-conditional network discovery algorithms to extend network discovery in individual Bayesian network learning [80,81] for transfer network learning. Given a Bayesian network structure, the work of parameter learning is to estimate the conditional probability tables (CPTs) for each node given the combination of its parent’s nodes. If we have data from all tasks, then we can directly estimate the CPTs from data. However in some cases we only have models from related tasks and need to estimate the CPT for the target task. In [76], two novel aggregation methods were deﬁned. The ﬁrst calculates a weighted average of the probabilities from the data of the auxiliary tasks based on the conﬁdence of the probability estimated from the auxiliary tasks and the similarity with the target estimates. This average is then combined with the target probability estimate, weighted by a factor that depends on its similarity to the target probability. The second method works similarly, but the average of probabilities is obtained from those closer to the target rather than from all the data of the auxiliary tasks. In addition, the average is combined to the target estimate with a conﬁdence factor, which is based on the amount of data. 4.3. Transfer learning using hierarchical Bayesian model Hierarchical Bayesian models are considered to be a particular type of Bayesian network and are used when the data are structured in groups. This hierarchical model can represent and reason about knowledge at multiple levels of abstraction, therefore a hierarchical Bayesian model provides a uniﬁed framework to explain both how abstract knowledge is used for induction and how abstract knowledge can be acquired. In considering the problem of multi-task learning, Wilson et al. [82] used a hierarchical Bayesian inﬁnite mixture model to model the distribution over multiple Markov Decision Processes (MDPs) such that the characteristics of new environments can be quickly inferred based on the learned distribution as an informed prior. This idea is extended to solve the problem of sequential decision-making tasks [83]. Yang et al. [84] combined all the tasks in a single RBF network and deﬁned a novel Bayesian multi-task learning model for non-linear regression. Meanwhile, Raykar et al. [85] presented a novel Bayesian multiple instance learning (MIL) algorithm, which performs feature selection and classiﬁer construction simultaneously. The results show that the proposed method is more accurate and effective when a smaller set of useful features is selected. In reference to the domain adaptation problem, a novel hierarchical Bayesian domain adaptation model was developed based on the use of a hierarchical Bayes prior [86]. In the proposed model, several parameters are set to each feature in each domain, and top level parameters are proposed on the upper level such that the Gaussian prior over the parameter values in each domain is now centered around these top level parameters instead of around zero, while the zero-mean Gaussian prior is placed over the top

level parameters. At the same time, Wood and Teh [87] designed a doubly hierarchical Pitman–Yor process language model, in which the bottom layer utilizes multiple hierarchical Pitman–Yor process language models to represent a number of domains while the top layer is responsible for sharing the statistical strength. A more special case is considered in [88], where only a single example from a new category is provided; thus, it is more difﬁcult to estimate the variance and similarity metric for categorizing an object in this case. It is possible with this model to encode priors for new classes into super-categories. Following the inference of the sub-category to which the novel category belongs, the model can estimate not only the mean of the new category but also an appropriate similarity metric based on parameters inherited from the super-category.

5. Transfer learning using fuzzy system and genetic algorithm Imprecision, approximation, vagueness and ambiguity of information are driven by the variability encountered when trying to learn an activity with little information. There is a clear co-dependency on the level of certainty in any learning activity and the amount of information that is available, and problems with little information, can have a high degree of uncertainty. This is why couple of researches appears very recently to apply fuzzy techniques into transfer learning. The use of fuzzy logic allows for the incorporation of approximation and a greater expressiveness of the uncertainty within the knowledge transfer. Zadeh [89] introduced the concept of fuzzy sets which he later expanded on by introducing further aspects of Fuzzy Logic, including fuzzy rules in [90]. The two primary elements within fuzzy logic, the linguistic variable and the fuzzy if-then rule, are able to mimic the human ability to capture imprecision and uncertainty within knowledge transfer. Fuzzy logic forms a major component of the published Fuzzy Transfer Learning techniques. Behbood et al. [91,92] developed a fuzzy-based transductive transfer learning for long term bank failure prediction in which the distribution of data in the source domain differs from that in the target domain. They applied three classical predictors, Neural Network, Support Vector Machine and Fuzzy Neural Network, to predict the initial labels for samples in the target domain, then attempted to reﬁne the labels using fuzzy similarity measures. The authors subsequently improved the performance of the fuzzy reﬁnement domain adaptation method [93] by developing a novel fuzzy measure to simultaneously take account of the similarity and dissimilarity in the reﬁnement process. The proposed method has been applied to text categorization and bank failure prediction. The experimental results demonstrated the superior performance of the proposed method compared to popular classical transductive transfer learning methods. Using fuzzy techniques in similarity measurement and label production, the authors revealed the advantage of fuzzy logic in knowledge transfer where the target domain lacks critical information and involves uncertainty and vagueness. Shell and Coupland domains [94,95] proposed a framework of fuzzy transfer learning to form a prediction model in intelligent environments. To address the issues of modeling environments in the presence of uncertainty and noise, they introduced a fuzzy logic-based transfer learning that enables the absorption of the inherent uncertainty and dynamic nature of transfer knowledge in intelligent environments. They created a transferable fuzzy inference system using labeled data in the source domain, then adapted and applied the resultant rule base to predict the labels for samples in the target domain. The source rules were adjusted and adapted to the target domain using the Euclidean distance measure. The proposed method was examined in two simulated intelligent environments. The experimental results demonstrated the superior performance

20

J. Lu et al. / Knowledge-Based Systems 80 (2015) 14–23

of fuzzy transfer learning compared to classical prediction models; however the method has not been compared with transfer learning methods. Deng et al. [96] proposed the generalized hidden-mapping ridge regression (GHRR) method to train various types of classical intelligence models, including neural networks, fuzzy logic systems and kernel methods. The knowledge-leverage based transfer learning mechanism is integrated with GHRR to realize the inductive transfer learning method called transfer GHRR (TGHRR). Since the information from the induced knowledge is much clearer and more concise than the information from the data in the source domain, it is more convenient to control and balance the similarity and difference of data distributions between the source and target domains. The proposed GHRR and TGHRR algorithms have been evaluated experimentally by performing regression and classiﬁcation on synthetic and real world datasets. The results demonstrated that the performance of TGHRR is competitive with or even superior to existing state-of-the-art inductive transfer learning algorithms. Genetic algorithm is an evolutionary method that simulates the process of natural selection to solve mainly optimization and search problems. This method uses techniques inspired by natural evolution such as inheritance, mutation, selection and crossover. Koçer and Arslan [97] introduced the use of genetic algorithm and transfer learning by extending a previously constructed algorithm. Their approach was to extend the transfer learning method of producing a translation function. This process allows for differing value functions that have been learnt to be mapped from source to target tasks. The authors incorporated the use of a set of policies originally constructed by a genetic algorithm to form the initial population for training the target task. They showed that the transfer of inter-task mappings can reduce the time required to learn a second, more complex task. 6. Applications of transfer learning Transfer learning approaches with the support of computational intelligence methods such as neural network, Bayesian network, and fuzzy logic have been applied in real-world applications. These applications largely fall into the following ﬁve categories: (1) Nature language processing; (2) Computer vision; (3) Biology; (4) Finance; and (5) Business management. 6.1. Nature language processing Nature language processing (NLP), which can be regarded as the study of human languages, is proposed to make natural language processing interpretable by computers. In general, there are numerous sub-learning tasks in NLP ﬁelds, such as text-based learning problems (e.g., text classiﬁcation or non-topical text analysis), language knowledge understanding, etc. For text related analysis, i.e., exploring the useful information from a given document, the learning problem of how to label the text documents across different distributions is addressed [17]. In this setting, the labeled training samples share different distributions from the unlabeled test data. Accordingly, a novel transferlearning algorithm based on an Expectation–Maximization based Naive Bayes model is proposed for further learning, which has demonstrated the best performance on three different types of data sets. Moreover, considering that most existing transfer learning methods assume that features and labels are numeric, and lack the ability to handle the uncertainty property, Behbood et al. [98] proposed a Fuzzy Domain Adaptation (FDA) approach and carried out an investigation of its applicability to text classiﬁcation. In addition, for sentiment classiﬁcation, which is a key challenge in non-topical text analysis, transfer learning technique is also applicable, such as adapting naïve Bayes to domain adaptation

for sentiment analysis by fully utilizing the information from both the old-domain and unlabeled new-domain data sets [16]. Furthermore, the transfer learning approach can be used to deal with language knowledge understanding problems. For speech recognition, for example, Swietojanski et al. [52] exploited untranscribed acoustic data to the target languages in a deep neural network on unsupervised cross-lingual knowledge transfer. Similarly, Huang et al. [47] dealt with the cross-language knowledge transfer learning tasks by a shared-hidden-layer multi-lingual deep neural network. 6.2. Computer vision and image processing The computer vision applied to transfer learning using computational intelligence includes methods for acquiring, processing, analysing, and understanding images, especially in high-dimensional data from the real world, for producing numerical or symbolic information. In this section, we summarize computer visual applications for camera images processing, from digits to letters processing, and video processing. In early camera image applications based on computational intelligent transfer learning, all approaches used a database of camera images of different objects, each of which had a distinct color or size and was used for vision learning, such as ALVINN-like road-following vision recognition [54]. One challenge in image object recognition is that the distributions of the training images and test images are different. Thus, Chopra et al. [51] argued that in the representation learning camp for images, existing deep learning approaches could not encode the distributional shift between the source and target domains. To this end, the authors proposed a novel transfer deep learning method for object recognition which allows the application of deep learning for domain adaptation. Camera images were also used to solve robotics problems. A visual object tracking routine, which recognizes and tracks the marker in real time, challenged robot researchers [99,100] that robot-mounted camera [54] was employed for task mappings, e.g. RoboCup soccer Keepaway [101]. Recently, image learning has mainly been used for human facial recognition, e.g., gender and ethnicity recognition based on facial appearance [46], emotional facial appearance recognition derived from synthetic images of neutral faces to that of corresponding images of angry, happy and sad faces [60], age estimations from face images [65], and gaze gesture recognition by eye tracking devices and eye gaze technologies [94]. Knowledge transfer between different handwritten character recognition tasks [48] is another kind of application of transfer learning in computer vision. Kandaswamy et al. [50] trained a neural network to classify Latin digits (speciﬁc source problem) and reused it to classify a lowercase letters (different but related target problem) without having to train it from scratch. In the empirical analysis, the authors used the proposed neural network to transfer knowledge from Arabic digits to Latin digits as well. Authors [50] also considered a problem of classifying images of English lowercase a-to-z by reusing ﬁne-tuned features of English handwritten digits 0-to-9. By applying salient feature detection and tracking in videos to simulate ﬁxations and smooth pursuit in human vision, Zou et al. [102] successfully implemented an unsupervised learning algorithm in a self-taught learning setting. With concrete recognition, features learned from natural videos do not only apply to still images, but also give competitive results on a number of object recognition benchmarks. 6.3. Biology Transfer learning has been applied to biology ﬁelds, including medical problems, biological modeling designs, ecology issues,

J. Lu et al. / Knowledge-Based Systems 80 (2015) 14–23

21

and so on. In applications related to medical issues, Caruana [54] suggested using multi-task learning in artiﬁcial neural networks, and proposed an inductive transfer learning approach for pneumonia risk prediction. A life-long inductive learning approach [56] retained task knowledge in a representational form and transferred knowledge in another form of virtual examples on three heart disease domains, through a neural network-based multi-task learning algorithm. They also put forward another type of sequential inductive transfer model for a medical diagnostics task, i.e., coronary artery disease diagnosis [57]. Recently, Oyen and Lane [79] argued that existing transfer learning methods for Bayesian networks focus on a single posteriori estimation, and that in doing so, other models may be ignored. To this end, they proposed a transfer multi-Bayesian Networks model for whole-brain neuroimaging. From the aspect of biological modeling designs, e.g., robot bionics, Celiberto et al. [66] combined three artiﬁcial intelligence techniques, case-based reasoning, heuristically accelerated reinforcement learning and neural networks, in a transfer learning problem. They then proposed a novel model called L3 to speed up the reinforcement learning framework for a set of empirical evaluations between two domains (the Acrobot and the Robocup 3D). Another important biology domain, ecology, has attracted the attention of researchers into transfer learning. For instance, Niculescu-Mizil and Caruana [75] proposed a multi-task Bayesian network structure learning (i.e., inductive transfer) to re-evaluate the performance of ALARM (a logical alarm reduction mechanism) and to handle a real bird ecology problem in North America. 6.4. Finance Another application area of transfer learning is ﬁnance, such as in the area of car insurance risk estimations and ﬁnancial early warning systems. Niculescu-Mizil and Caruana [75] presented an inductive transfer learning approach, which jointly learns multiple Bayesian network structures instead of adaptive probabilistic networks from multiple related data sets. The authors examined the proposed method using car insurance risk estimation networks. It is worth noticing that the works on intelligent ﬁnancial warning systems and long term prediction in banking ecosystems [91–93] are the ﬁrst systematic studies to apply transfer learning approaches using fuzzy logic techniques of computational intelligence to real-world ﬁnancial applications to exploit the knowledge of the banking system, e.g., transferring the information from one country to establish a prediction model in another country. 6.5. Business management Transfer learning using computational intelligence has been applied in business management. For instance, Roy and Kaelbling [71] proposed an efﬁcient Bayesian task-level transfer learning to tackle the user’s behavior in the meeting domain. Jin and Sun [103] indicated that traditional neural network methods for trafﬁc ﬂow forecasting are based on a single task which cannot utilize information from other tasks. To address this challenge, multi-task
Table 1 Summary of transfer learning techniques in each application domain. Domains Techniques Artiﬁcial neural networks Natural language processing Computer vision & image processing Biology Finance Business management Total 2 11 4 0 1 17

based neural network is proposed to transfer knowledge to deal with trafﬁc ﬂow forecasting. Luis et al. [76] proposed the use of a novel transfer Bayesian network learning framework, including structure and parameter learning, to handle a product manufacture process issue. Recently, Ma et al. [72] studied the cross-company software defect prediction scenario in which the source and target data sets come from different companies, and proposed a novel transfer naive Bayes as the solution. A dynamic model for intelligent environments has been proposed to make use of the data from different feature spaces and domains [94,104], with a novel fuzzy transfer learning process. 7. Comprehensive analysis and ﬁndings In this paper, we have reviewed several current trends of computational intelligence-based transfer learning. According to the review, computational intelligence techniques used in transfer learning are classiﬁed to three main groups of: Neural Network, Bayes and Fuzzy Logic and Genetic Algorithm. The number of reviewed transfer learning papers for each computational intelligence technique in each application domain is summarized and presented in Table 1. From the summary of transfer learning, it is concluded that transfer learning with the use computational intelligence, as an emerging research topic, starts playing an important role in almost all kinds of application. Of the computational intelligence methods, neural network has been extensively used for transfer learning, mainly in computer vision and image processing domain. The main reason why neural network has been widely used in transfer learning is that it does not have i.i.d. assumption on data while almost all stochastic techniques have. It can also be identiﬁed that Fuzzy-based transfer learning techniques have played an increasingly important role in recent applications particularly ﬁnance. Since many real world applications have noisy and uncertainty in data, researchers take fuzzy systems into account for transfer learning more and more. In the future, several important research challenges in the ﬁeld of computational intelligence-based transfer learning need to be addressed. First, the computational complexity is a crucial issue in computational intelligence-based transfer learning. Almost, all reviewed studies have focused on accuracy as a measurement for model performance. However, comparing with the statistical transfer learning methods, computational intelligence techniques usually gain more computational complexity which should be handled. In addition, how to avoid negative transfer is an open problem in not only the classical transfer learning but also in computational intelligence-based transfer learning. The transferability among source and target domains needs to be studied profoundly and a comprehensive and accurate transferability measures to be implemented that can guarantee the negative learning will not happens. Moreover, all reviewed studies have assumed that the feature spaces between the source and target domains are the same. However, in many applications, which we wish to transfer knowledge among domains, this assumption cannot be held. This type of transfer learning which is referred as the

Bayes 2 0 2 1 3 7

Fuzzy logic 1 1 0 3 2 6

No. of listed references 5 12 6 4 6 30

22

J. Lu et al. / Knowledge-Based Systems 80 (2015) 14–23 [19] B. Roark, M. Bacchiani, Supervised and unsupervised PCFG adaptation to novel domains, in: Conference of the North American Chapter of the Association for Computational Linguistics on Human Language, Edmonton, Canada, 2003. [20] K. Sagae, J. Tsujii, Dependency parsing and domain adaptation with LR models and parser ensembles, in: 11th Conference on Computational Natural Language Learning, Prague, Czech Republic, 2007. [21] K. Sagae, Self-training without reranking for parser domain adaptation and its impact on semantic role labeling, in: Workshop on Domain Adaptation for Natural Language Processing, Uppsala, Sweden, 2010. [22] J. Jiang, C.X. Zhai, Instance weighting for domain adaptation in NLP, in: 45th Annual Meeting of the Association of Computational Linguistics, Prague, Czech Republic, 2007. [23] O. Sandu, G. Carenini, G. Murray, R. Ng, Domain adaptation to summarize human conversations, in: Workshop on Domain Adaptation for Natural Language Processing, Uppsala, Sweden, 2010. [24] J. Jiang, C.X. Zhai, A two-stage approach to domain adaptation for statistical classiﬁers, in: Sixteenth ACM Conference on Information and Knowledge Management, Lisbon, Portugal, 2007. [25] M. Ciaramita, O. Chapelle, Adaptive parameters for entity recognition with perceptron HMMs, in: Workshop on Domain Adaptation for Natural Language Processing, Uppsala, Sweden, 2010. [26] S. Tan, Y. Wang, G. Wu, X. Cheng, Using unlabeled data to handle domaintransfer problem of semantic detection, in: ACM Symposium on Applied Computing, Fortaleza, Brazil, 2008. [27] L. Rigutini, M. Maggini, B. Liu, An EM based training algorithm for crosslanguage text categorization, in: IEEE/WIC/ACM International Conference on Web Intelligence, Compiegne, France, 2005. [28] L. Shi, R. Mihalcea, M. Tian, Cross language text classiﬁcation by model translation and semi-supervised learning, in: Conference on Empirical Methods in Natural Language Processing, Cambridge, USA, 2010. [29] M. Jeong, C.Y. Lin, G.G. Lee, Semi-supervised speech act recognition in emails and forums, in: Conference on Empirical Methods in Natural Language Processing, Singapore, Singapore, 2009. [30] A. Arnold, R. Nallapati, W.W. Cohen, A comparative study of methods for transductive transfer learning, in: Seventh IEEE International Conference on Data Mining Workshops, Omaha, USA, 2007. [31] A. Aue, M. Gamon, Customizing sentiment classiﬁers to new domains: A case study, in: International Conference on Recent Advances in Natural Language Processing, Borovets, Bulgaria, 2005. [32] S. Satpal, S. Sarawagi, Domain adaptation of conditional probability models via feature subsetting, in: 11th European Conference on Principles and Practice of Knowledge Discovery in Databases, Warsaw, Poland, 2007. [33] B. Chen, W. Lam, I. Tsang, T.L. Wong, Extracting discriminative concepts for domain adaptation in text mining, in: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Paris, France, 2009. [34] S.J. Pan, J.T. Kwok, Q. Yang, Transfer learning via dimensionality reduction, in: 23rd National Conference on Artiﬁcial Intelligence, Chicago, USA, 2008. [35] S.J. Pan, I.W. Tsang, J.T. Kwork, Q. Yang, Domain adaptation via transfer component analysis, IEEE Trans. Neural Networks 22 (2) (2011) 199–210. [36] J. Blitzer, R. McOnald, F. Pereira, Domain adaptation with structural correspondence learning, in: Conference on Empirical Methods in Natural Language Processing, Sydney, Australia, 2006. [37] J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, J. Wortman, Learning bounds for domain adaptation, in: Twenty-First Annual Conference on Neural Information Processing Systems, Cambridge, USA, 2007. [38] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, J.W. Vaughan, A theory of learning from different domains, Mach. Learn. 79 (1) (2010) 151– 175. [39] F. Huang, A. Yates, Exploring representation-learning approaches to domain adaptation, in: Workshop on Domain Adaptation for Natural Language Processing, Uppsala, Sweden, 2010. [40] F. Huang, A. Yates, Open-domain semantic role labeling by modeling word spans, in: 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, 2010. [41] F. Huang, A. Yates, Distributional representations for handling sparsity in supervised sequence-labeling, in: Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Conference on Natural Language Processing, Singapore, Singapore, 2009. [42] S.J. Pan, X. Ni, J. Sun, Q. Yang, Z. Chen, Cross-domain sentiment classiﬁcation via spectral feature alignment, in: 19th International Conference on World Wide Web, Raleigh, USA, 2010. [43] J. Gao, W. Fan, J. Jiang, J. Han, Knowledge transfer via multiple model local structure mapping, in: 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, USA, 2008. [44] G.-R. Xue, W. Dai, Q. Yang, Y. Yu, Topic-bridged PLSA for cross-domain text classiﬁcation, in: 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Singapore, Singapore, 2008. [45] D.H. Hubel, T.N. Wiesel, Receptive ﬁelds, binocular interaction and functional architecture in the cat’s visual cortex, J. Physiol. 160 (1) (1962) 106–154. [46] A. Ahmed, K. Yu, W. Xu, Y. Gong, E. Xing, Training Hierarchical Feed-Forward Visual Recognition Models Using Transfer Learning from Pseudo-Tasks, in Computer Vision–ECCV 2008, Springer, 2008. pp. 69–82.

heterogeneous transfer learning has not been addressed in computational intelligence-based transfer learning literature. Finally, so far the computational intelligence techniques are applied for small scale transfer learning problems. Nonetheless, in the era of big data, there are many interesting applications such as social network analysis and web-based recommender systems that can exploit transfer learning and computational intelligence techniques. The capability of computational intelligence to handle non-i.i.d. noisy data can pave the way to use these techniques in big scale real world applications. Two important features of this paper clearly distinguish it from other survey papers in the transfer learning area: Firstly, It targets the development of transfer learning methods that use computational intelligence. Secondly, it systematically examines the applications of transfer learning that are integrated with computational intelligence. We believe that this paper can provide researchers and practical professionals with state-of-the-art knowledge on transfer learning with computational intelligence and give guidelines about how to develop and apply transfer learning in different domains to support users in various decision activities. Acknowledgment The work presented in this paper was supported by the Australian Research Council (ARC) under discovery grant DP140101366. References
[1] X. Zhu, Semi-Supervised Learning Literature Survey, University of Wisconsin, Madison, USA, 2005. [2] K. Nigam, A.K. Mccallum, S. Thrun, T. Mitchell, Text classiﬁcation from labeled and unlabeled documents using EM, Mach. Learn. 39 (2–3) (2000) 103–134. [3] A. Blum, T. Mitchell, Combining labeled and unlabeled data with co-training, in: Eleventh Annual Conference on Computational Learning Theory, Madison, USA, 1998. [4] T. Joachims, Transductive inference for text classiﬁcation using support vector machines, in: Sixteenth International Conference on Machine Learning, Bled, USA, 1999. [5] G.P.C. Fung, J.X. Yu, H.J. Lu, P.S. Yu, Text classiﬁcation without negative examples revisit, IEEE Trans. Knowl. Data Eng. 18 (1) (2006) 6–20. [6] X. Yin, J. Han, J. Yang, P.S. Yu, Efﬁcient classiﬁcation across multiple database relations: a CrossMine approach, IEEE Trans. Knowl. Data Eng. 18 (6) (2006) 770–783. [7] L.I. Kuncheva, J.J. Rodriguez, Classiﬁer ensembles with a random linear oracle, IEEE Trans. Knowl. Data Eng. 19 (4) (2007) 500–508. [8] E. Baralis, S. Chiusano, P. Garza, A lazy approach to associative classiﬁcation, IEEE Trans. Knowl. Data Eng. 20 (2) (2008) 156–171. [9] S.J. Pan, Q. Yang, A survey on transfer learning, IEEE Trans. Knowl. Data Eng. 22 (10) (2010) 1345–1359. [10] R. Klinkenberg, T. Joachims, Detecting concept drift with support vector machines, in: Seventeenth International Conference on Machine Learning, Stanford, USA, 2000. [11] A. Margolis, A Literature Review of Domain Adaptation with Unlabeled Data, Rapport Technique, University of Washington, 2011. [12] J.J. Heckman, Sample selection bias as a speciﬁcation error, Econometrica 47 (1) (1979) 153–162. [13] J. Huang, A.J. Smola, A.. Gretton, K.M. Borgwardt, B. Scholkopf, Correcting sample selection bias by unlabeled data, Adv. Neural Inf. Process. Syst. 19 (2007) 601–608. [14] M. Sugiyama, S. Nakajima, H. Kashima, P. Bunau, M. Kawanabe, Direct importance estimation with model selection and its application to covariate shift adaptation, Adv. Neural Inf. Process. Syst. 20 (2008) 1433–1440. [15] Y. Tsuboi, H. Kashima, S. Hido, S. Bickel, M. Sugiyama, Direct density ratio estimation for large-scale covariate shift adaptation, Inf. Media Technol. 4 (2) (2009) 529–546. [16] S. Tan, X. Cheng, Y. Wang, H. Xu, Adapting naive Bayes to domain adaptation for sentiment analysis, in: 31st European Conference on Advances in Information Retrieval, Toulouse, France, 2009. [17] W. Dai, G. Xue, Q. Yang, Y. Yu, Transferring naive Bayes classiﬁers for text classiﬁcation, in: 22nd National Conference on Artiﬁcial Intelligence, Vancouver, Canada, 2007. [18] D. McClosky, E. Charniak, M. Johnson, Reranking and self-training for parser adaptation, in: 21st International Conference on Computational Linguistics, Sydney, Australia, 2006.

J. Lu et al. / Knowledge-Based Systems 80 (2015) 14–23 [47] J.-T. Huang, J. Li, D. Yu, L. Deng, Y. Gong, Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers, in: IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Vancouver, Canada, 2013. [48] D.C. Ciresan, U. Meier, J. Schmidhuber, Transfer learning for Latin and Chinese characters with deep neural networks, in: International Joint Conference on Neural Networks (IJCNN), Brisbane, Australia, 2012. [49] R. Collobert, J. Weston, A uniﬁed architecture for natural language processing: deep neural networks with multitask learning, in: 25th International Conference on Machine Learning, Helsinki, Finland, 2008. [50] C. Kandaswamy, L.M. Silva, L.A. Alexandre, J.M. Santos, J.M. de Sa, Improving deep neural network performance by reusing features trained with transductive transference, in: Proceedings of the 24th International Conference on Artiﬁcial Neural Networks, Hamburg, Germany, 2014. [51] S. Chopra, S. Balakrishnan, R. Gopalan, DLID: deep learning for domain adaptation by interpolating between domains, in: ICML Workshop on Challenges in Representation Learning, Atlanta, USA, 2013. [52] P. Swietojanski, A. Ghoshal, S. Renals, Unsupervised cross-lingual knowledge transfer in DNN-based LVCSR, in: IEEE Workshop on Spoken Language Technology (SLT), Miami, USA, 2012. [53] R. Caruana, Multitask learning: a knowledge-based source of inductive bias, in: Tenth International Conference of Machine Learning, MA, USA, 1993. [54] R. Caruana, Multitask learning, Mach. Learn. 28 (1997) 41–75. [55] D. Silver, R. Mercer, Selective functional transfer: Inductive bias from related tasks, in: International Conference on Artiﬁcial Intelligence and Soft Computing (ASC), Cancun, Mexico, 2001. [56] D.L. Silver, R.E. Mercer, The task rehearsal method of life-long learning: overcoming impoverished data, in: Advances in Artiﬁcial Intelligence, Springer, 2002, pp. 90–101. [57] D.L. Silver, R.E. Mercer, Sequential inductive transfer for coronary artery disease diagnosis, in: International Joint Conference on Neural Networks (IJCNN), Orlando, USA, 2007. [58] D.L. Silver, R. Poirier, Context-sensitive MTL networks for machine lifelong learning, in: 20th Florida Artiﬁcial Intelligence Research Society (FLAIRS) Conference, Key West, USA, 2007. [59] D.L. Silver, R. Poirier, D. Currie, Inductive transfer with context-sensitive neural networks, Mach. Learn. 73 (3) (2008) 313–336. [60] D.L. Silver, L. Tu, Image transformation: inductive transfer between multiple tasks having multiple outputs, in: Advances in Artiﬁcial Intelligence, Springer, 2008, pp. 296–307. [61] K. Yamauchi, Covariate shift and incremental learning, in: Advances in NeuroInformation Processing, Springer, 2009, pp. 1154–1162. [62] H. Shimodaira, Improving predictive inference under covariate shift by weighting the log-likelihood function, J. Stat. Plann. Inference 90 (2) (2000) 227–244. [63] K. Yamauchi, Optimal incremental learning under covariate shift, Memetic Comput. 1 (4) (2009) 271–279. [64] W. Liu, H. Zhang, J. Li, Inductive transfer through neural network error and dataset regrouping, in: IEEE International Conference on Intelligent Computing and Intelligent Systems (ICIS), Shanghai, China, 2009. [65] K. Ueki, M. Sugiyama, Y. Ihara, Perceived age estimation under lighting condition change by covariate shift adaptation, in: 20th International Conference on Pattern Recognition (ICPR), Istanbul, Turkey, 2010. [66] L.A. Celiberto Jr., L.A., J.P. Matsuura, R.L. de Mantaras, R.A.C. Bianchi, Using cases as heuristics in reinforcement learning: a transfer learning application, in: IJCAI Proceedings-International Joint Conference on Artiﬁcial Intelligence, Barcelona, Spain, 2011. [67] D.D. Lewis, Representation and Learning in Information Retrieval, University of Massachusetts, 1992. [68] I. Kononenko, Inductive and Bayesian learning in medical diagnosis, Appl. Artif. Intell. Int. J. 7 (4) (1993) 317–337. [69] I. Androutsopoulos, J. Koutsias, K.V. Chandrinos, C.D. Spyropoulos, An experimental comparison of naive Bayesian and keyword-based anti-spam ﬁltering with personal e-mail messages, in: 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Pisa, Italy, 2000. [70] F. Sebastiani, Machine learning in automated text categorization, ACM Comput. Surveys (CSUR) 34 (1) (2002) 1–47. [71] D.M. Roy, L.P. Kaelbling, Efﬁcient Bayesian task-level transfer learning, in: International Joint Conference on Artiﬁcial Intelligence, Hyderabad, India, 2007. [72] Y. Ma, G. Luo, X. Zeng, A. Chen, Transfer learning for cross-company software defect prediction, Inf. Softw. Technol. 54 (3) (2012) 248–256. [73] D. Heckerman, A Tutorial on Learning with Bayesian Networks, Springer, 1998. [74] W.L. Buntine, A guide to the literature on learning probabilistic networks from data, IEEE Trans. Knowl. Data Eng. 8 (2) (1996) 195–210.

23

[75] A. Niculescu-Mizil, R. Caruana, Inductive transfer for Bayesian network structure learning, in: Eleventh International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), San Juan, Puerto Rico, 2007. [76] R. Luis, L.E. Sucar, E.F. Morales, Inductive transfer for learning Bayesian networks, Mach. Learn. 79 (1–2) (2010) 227–255. [77] M. Richardson, P. Domingos, Learning with knowledge from multiple experts, in: International Conference on Machine learning (ICML), Washington, USA, 2003. [78] D. Oyen, T. Lane, Leveraging domain knowledge in multitask Bayesian network structure learning, in: 26th Association for the Advancement of Artiﬁcial Intelligence (AAAI) Conference, Toronto, Canada, 2012. [79] D. Oyen, T. Lane, Bayesian discovery of multiple Bayesian networks via transfer learning, in: 13th IEEE International Conference on Data Mining (ICDM), Dalla, USA, 2013. [80] N. Friedman, D. Koller, Being Bayesian about network structure: a Bayesian approach to structure discovery in Bayesian networks, Mach. Learn. 50 (1–2) (2003) 95–125. [81] M. Koivisto, K. Sood, Exact Bayesian structure discovery in Bayesian networks, J. Mach. Learn. Res. 5 (2004) 549–573. [82] A. Wilson, A. Fern, S. Ray, P. Tadepalli, Multi-task reinforcement learning: a hierarchical Bayesian approach, in: 24th International Conference on Machine Learning, Corvallis, USA, 2007. [83] A. Wilson, A. Fern, P. Tadepalli, Transfer learning in sequential decision problems: a hierarchical Bayesian approach, in: International Conference of Machine Learning, Edinburgh, Scotland, 2012. [84] P. Yang, Q. Tan, Y. Ding, Bayesian task-level transfer learning for non-linear regression, in: International Conference on Computer Science and Software Engineering, Wuhan, China, 2008. [85] V.C. Raykar, B. Krishnapuram, J. Bi, M. Dundar, R.B. Rao, Bayesian multiple instance learning: Automatic feature selection and inductive transfer, in: 25th International Conference on Machine Learning, Helsinki, Finland, 2008. [86] J.R. Finkel, C.D. Manning, Hierarchical Bayesian domain adaptation, in: Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, USA, 2009. [87] F. Wood, Y.W. Teh, A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation, in: International Conference on Artiﬁcial Intelligence and Statistics, Las Vegas, USA, 2009. [88] R. Salakhutdinov, J. Tenenbaum, A. Torralba, One-shot learning with a hierarchical nonparametric Bayesian model, MIT-CSAIL-TR-2010-052, Editor, MIT, 2010. [89] L.A. Zadeh, Fuzzy sets, Inf. Control 8 (3) (1965) 338–353. [90] R.E. Bellman, L.A. Zadeh, Decision-making in a fuzzy environment, Manage. Sci. 17 (4) (1970) 141–164. [91] V. Behbood, J. Lu, G. Zhang, Long term bank failure prediction using fuzzy reﬁnement-based transductive transfer learning, in: IEEE International Conference on Fuzzy Systems (FUZZ), Taipei, Taiwan, 2011. [92] V. Behbood, J. Lu, G. Zhang, Fuzzy bridged reﬁnement domain adaptation: long-term bank failure prediction, Int. J. Comput. Intell. Appl. 12 (01) (2013). [93] V. Behbood, J. Lu, G. Zhang, Fuzzy reﬁnement domain adaptation for long term prediction in banking ecosystem, IEEE Trans. Industr. Inf. 10 (2) (2014) 1637–1646. [94] J. Shell, S. Coupland, Towards fuzzy transfer learning for intelligent environments, in: Ambient Intelligence, vol. 7683, 2012, pp. 145–160. [95] J. Shell, S. Coupland, Fuzzy transfer learning: methodology and application, Inf. Sci. 293 (2015) 59–79. [96] Z. Deng, K. Choi, Y. Jiang, Generalized hidden-mapping ridge regression, knowledge-leveraged inductive transfer learning for neural networks, fuzzy systems and kernel method, IEEE Trans. Cybern. 44 (12) (2014) 2585–2599. [97] B. Koçer, A. Arslan, Genetic transfer learning, Expert Syst. Appl. 37 (10) (2010) 6997–7002. [98] V. Behbood, J. Lu, G. Zhang, Text categorization by fuzzy domain adaptation, in: IEEE International Conference on Fuzzy Systems, Hyderabad, India, 2013. [99] S. Thrun, Is learning the n-th thing any easier than learning the ﬁrst?, Adv Neural Inf. Process. Syst. (1996) 640–646. [100] S. Thrun, A lifelong learning perspective for mobile robot control, in: IEEE/ RSJ/GI International Conference on Intelligent Robots and Systems, Munich, Germany, 1994. [101] M.E. Taylor, P. Stone, Y. Liu, Transfer learning via inter-task mappings for temporal difference learning, J. Mach. Learn. Res. 8 (1) (2007) 2125–2167. [102] W. Zou, S. Zhu, A.Y. Ng, K. Yu, Deep learning of invariant features via simulated ﬁxations in video, in: Advances in Neural Information Processing Systems, Lake Tahoe, USA, 2012. [103] F. Jin, S. Sun, Neural network multitask learning for trafﬁc ﬂow forecasting, in: IEEE International Joint Conference on Neural Networks (IJCNN), Hong Kong, China, 2008. [104] J. Shell, Fuzzy transfer learning, Ph.D. Thesis, De Montfort University, 2013.

