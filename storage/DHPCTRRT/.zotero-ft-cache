Springer Series in Statistics

Trevor Hastie Robert Tibshirani Jerome Friedman

The Elements of Statistical Learning
Data Mining, Inference, and Prediction
Second Edition

This is page v Printer: Opaque this

To our parents: Valerie and Patrick Hastie Vera and Sami Tibshirani Florence and Harry Friedman

and to our families: Samantha, Timothy, and Lynda Charlie, Ryan, Julie, and Cheryl Melanie, Dora, Monika, and Ildiko

vi

This is page vii Printer: Opaque this

Preface to the Second Edition

In God we trust, all others bring data.

âWilliam Edwards Deming (1900-1993)1

We have been gratiï¬ed by the popularity of the ï¬rst edition of The Elements of Statistical Learning. This, along with the fast pace of research in the statistical learning ï¬eld, motivated us to update our book with a second edition. We have added four new chapters and updated some of the existing chapters. Because many readers are familiar with the layout of the ï¬rst edition, we have tried to change it as little as possible. Here is a summary of the main changes:
1 On the Web, this quote has been widely attributed to both Deming and Robert W. Hayden; however Professor Hayden told us that he can claim no credit for this quote, and ironically we could ï¬nd no âdataâ conï¬rming that Deming actually said this.

viii

Preface to the Second Edition Whatâs new

Chapter 1. Introduction 2. Overview of Supervised Learning 3. Linear Methods for Regression 4. Linear Methods for Classiï¬cation 5. Basis Expansions and Regularization 6. Kernel Smoothing Methods 7. Model Assessment and Selection 8. Model Inference and Averaging 9. Additive Models, Trees, and Related Methods 10. Boosting and Additive Trees 11. Neural Networks 12. Support Vector Machines and Flexible Discriminants 13. Prototype Methods and Nearest-Neighbors 14. Unsupervised Learning

LAR algorithm and generalizations of the lasso Lasso path for logistic regression Additional illustrations of RKHS

Strengths and pitfalls of crossvalidation

New example from ecology; some material split oï¬ to Chapter 16. Bayesian neural nets and the NIPS 2003 challenge Path algorithm for SVM classiï¬er

15. 16. 17. 18.

Random Forests Ensemble Learning Undirected Graphical Models High-Dimensional Problems

Spectral clustering, kernel PCA, sparse PCA, non-negative matrix factorization archetypal analysis, nonlinear dimension reduction, Google page rank algorithm, a direct approach to ICA New New New New

Some further notes: â¢ Our ï¬rst edition was unfriendly to colorblind readers; in particular, we tended to favor red/green contrasts which are particularly troublesome. We have changed the color palette in this edition to a large extent, replacing the above with an orange/blue contrast. â¢ We have changed the name of Chapter 6 from âKernel Methodsâ to âKernel Smoothing Methodsâ, to avoid confusion with the machinelearning kernel method that is discussed in the context of support vector machines (Chapter 11) and more generally in Chapters 5 and 14. â¢ In the ï¬rst edition, the discussion of error-rate estimation in Chapter 7 was sloppy, as we did not clearly diï¬erentiate the notions of conditional error rates (conditional on the training set) and unconditional rates. We have ï¬xed this in the new edition.

Preface to the Second Edition

ix

â¢ Chapters 15 and 16 follow naturally from Chapter 10, and the chapters are probably best read in that order. â¢ In Chapter 17, we have not attempted a comprehensive treatment of graphical models, and discuss only undirected models and some new methods for their estimation. Due to a lack of space, we have speciï¬cally omitted coverage of directed graphical models. â¢ Chapter 18 explores the âp â« N â problem, which is learning in highdimensional feature spaces. These problems arise in many areas, including genomic and proteomic studies, and document classiï¬cation. We thank the many readers who have found the (too numerous) errors in the ï¬rst edition. We apologize for those and have done our best to avoid errors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry Wasserman for comments on some of the new chapters, and many Stanford graduate and post-doctoral students who oï¬ered comments, in particular Mohammed AlQuraishi, John Boik, Holger Hoeï¬ing, Arian Maleki, Donal McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and Hui Zou. We thank John Kimmel for his patience in guiding us through this new edition. RT dedicates this edition to the memory of Anna McPhee. Trevor Hastie Robert Tibshirani Jerome Friedman Stanford, California August 2008

x

Preface to the Second Edition

This is page xi Printer: Opaque this

Preface to the First Edition

We are drowning in information and starving for knowledge. âRutherford D. Roger

The ï¬eld of Statistics is constantly challenged by the problems that science and industry brings to its door. In the early days, these problems often came from agricultural and industrial experiments and were relatively small in scope. With the advent of computers and the information age, statistical problems have exploded both in size and complexity. Challenges in the areas of data storage, organization and searching have led to the new ï¬eld of âdata miningâ; statistical and computational problems in biology and medicine have created âbioinformatics.â Vast amounts of data are being generated in many ï¬elds, and the statisticianâs job is to make sense of it all: to extract important patterns and trends, and understand âwhat the data says.â We call this learning from data. The challenges in learning from data have led to a revolution in the statistical sciences. Since computation plays such a key role, it is not surprising that much of this new development has been done by researchers in other ï¬elds such as computer science and engineering. The learning problems that we consider can be roughly categorized as either supervised or unsupervised. In supervised learning, the goal is to predict the value of an outcome measure based on a number of input measures; in unsupervised learning, there is no outcome measure, and the goal is to describe the associations and patterns among a set of input measures.

xii

Preface to the First Edition

This book is our attempt to bring together many of the important new ideas in learning, and explain them in a statistical framework. While some mathematical details are needed, we emphasize the methods and their conceptual underpinnings rather than their theoretical properties. As a result, we hope that this book will appeal not just to statisticians but also to researchers and practitioners in a wide variety of ï¬elds. Just as we have learned a great deal from researchers outside of the ï¬eld of statistics, our statistical viewpoint may help others to better understand diï¬erent aspects of learning: There is no true interpretation of anything; interpretation is a vehicle in the service of human comprehension. The value of interpretation is in enabling others to fruitfully think about an idea. âAndreas Buja We would like to acknowledge the contribution of many people to the conception and completion of this book. David Andrews, Leo Breiman, Andreas Buja, John Chambers, Bradley Efron, Geoï¬rey Hinton, Werner Stuetzle, and John Tukey have greatly inï¬uenced our careers. Balasubramanian Narasimhan gave us advice and help on many computational problems, and maintained an excellent computing environment. Shin-Ho Bang helped in the production of a number of the ï¬gures. Lee Wilkinson gave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Maya Gupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bogdan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu Zhu, two reviewers and many students read parts of the manuscript and oï¬ered helpful suggestions. John Kimmel was supportive, patient and helpful at every phase; MaryAnn Brickner and Frank Ganz headed a superb production team at Springer. Trevor Hastie would like to thank the statistics department at the University of Cape Town for their hospitality during the ï¬nal stages of this book. We gratefully acknowledge NSF and NIH for their support of this work. Finally, we would like to thank our families and our parents for their love and support. Trevor Hastie Robert Tibshirani Jerome Friedman Stanford, California May 2001 The quiet statisticians have changed our world; not by discovering new facts or technical developments, but by changing the ways that we reason, experiment and form our opinions .... âIan Hacking

This is page xiii Printer: Opaque this

Contents

Preface to the Second Edition Preface to the First Edition 1 Introduction 2 Overview of Supervised Learning 2.1 Introduction . . . . . . . . . . . . . . . . . . . . . 2.2 Variable Types and Terminology . . . . . . . . . . 2.3 Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors . . . . . . . 2.3.1 Linear Models and Least Squares . . . . 2.3.2 Nearest-Neighbor Methods . . . . . . . . 2.3.3 From Least Squares to Nearest Neighbors 2.4 Statistical Decision Theory . . . . . . . . . . . . . 2.5 Local Methods in High Dimensions . . . . . . . . . 2.6 Statistical Models, Supervised Learning and Function Approximation . . . . . . . . . . . . 2.6.1 A Statistical Model for the Joint Distribution Pr(X, Y ) . . . 2.6.2 Supervised Learning . . . . . . . . . . . . 2.6.3 Function Approximation . . . . . . . . . 2.7 Structured Regression Models . . . . . . . . . . . 2.7.1 Diï¬culty of the Problem . . . . . . . . .

vii xi 1 9 9 9 11 11 14 16 18 22 28 28 29 29 32 32

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . .

xiv

Contents

2.8

Classes of Restricted Estimators . . . . . . . . . . . 2.8.1 Roughness Penalty and Bayesian Methods 2.8.2 Kernel Methods and Local Regression . . . 2.8.3 Basis Functions and Dictionary Methods . 2.9 Model Selection and the BiasâVariance Tradeoï¬ . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . .

. . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

33 34 34 35 37 39 39 43 43 44 49 51 52 56 57 57 58 60 61 61 61 68 69 73 79 79 80 82 84 86 86 89 89 90 91 92 93 94 94

3 Linear Methods for Regression 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Linear Regression Models and Least Squares . . . . . . 3.2.1 Example: Prostate Cancer . . . . . . . . . . . 3.2.2 The GaussâMarkov Theorem . . . . . . . . . . 3.2.3 Multiple Regression from Simple Univariate Regression . . . . . . . 3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . 3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Best-Subset Selection . . . . . . . . . . . . . . 3.3.2 Forward- and Backward-Stepwise Selection . . 3.3.3 Forward-Stagewise Regression . . . . . . . . . 3.3.4 Prostate Cancer Data Example (Continued) . 3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . 3.4.1 Ridge Regression . . . . . . . . . . . . . . . . 3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . 3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso . . . . . . . . . . . . . . . . . . 3.4.4 Least Angle Regression . . . . . . . . . . . . . 3.5 Methods Using Derived Input Directions . . . . . . . . 3.5.1 Principal Components Regression . . . . . . . 3.5.2 Partial Least Squares . . . . . . . . . . . . . . 3.6 Discussion: A Comparison of the Selection and Shrinkage Methods . . . . . . . . . . . . . . . . . . 3.7 Multiple Outcome Shrinkage and Selection . . . . . . . 3.8 More on the Lasso and Related Path Algorithms . . . . 3.8.1 Incremental Forward Stagewise Regression . . 3.8.2 Piecewise-Linear Path Algorithms . . . . . . . 3.8.3 The Dantzig Selector . . . . . . . . . . . . . . 3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . 3.8.5 Further Properties of the Lasso . . . . . . . . . 3.8.6 Pathwise Coordinate Optimization . . . . . . . 3.9 Computational Considerations . . . . . . . . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Contents

xv

4 Linear Methods for Classiï¬cation 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . 4.2 Linear Regression of an Indicator Matrix . . . . . . . 4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . 4.3.1 Regularized Discriminant Analysis . . . . . . 4.3.2 Computations for LDA . . . . . . . . . . . . 4.3.3 Reduced-Rank Linear Discriminant Analysis 4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . 4.4.1 Fitting Logistic Regression Models . . . . . . 4.4.2 Example: South African Heart Disease . . . 4.4.3 Quadratic Approximations and Inference . . 4.4.4 L1 Regularized Logistic Regression . . . . . . 4.4.5 Logistic Regression or LDA? . . . . . . . . . 4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . 4.5.1 Rosenblattâs Perceptron Learning Algorithm 4.5.2 Optimal Separating Hyperplanes . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . .

101 101 103 106 112 113 113 119 120 122 124 125 127 129 130 132 135 135

5 Basis Expansions and Regularization 139 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139 5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141 5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144 5.2.2 Example: South African Heart Disease (Continued)146 5.2.3 Example: Phoneme Recognition . . . . . . . . . 148 5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150 5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151 5.4.1 Degrees of Freedom and Smoother Matrices . . . 153 5.5 Automatic Selection of the Smoothing Parameters . . . . 156 5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158 5.5.2 The BiasâVariance Tradeoï¬ . . . . . . . . . . . . 158 5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161 5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162 5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167 5.8.1 Spaces of Functions Generated by Kernels . . . 168 5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170 5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174 5.9.1 Wavelet Bases and the Wavelet Transform . . . 176 5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181 Appendix: Computational Considerations for Splines . . . . . . 186 Appendix: B-splines . . . . . . . . . . . . . . . . . . . . . 186 Appendix: Computations for Smoothing Splines . . . . . 189

xvi

Contents

6 Kernel Smoothing Methods 6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 6.1.1 Local Linear Regression . . . . . . . . . . . . . . 6.1.2 Local Polynomial Regression . . . . . . . . . . . 6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 6.3 Local Regression in IRp . . . . . . . . . . . . . . . . . . . 6.4 Structured Local Regression Models in IRp . . . . . . . . 6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 6.4.2 Structured Regression Functions . . . . . . . . . 6.5 Local Likelihood and Other Models . . . . . . . . . . . . 6.6 Kernel Density Estimation and Classiï¬cation . . . . . . . 6.6.1 Kernel Density Estimation . . . . . . . . . . . . 6.6.2 Kernel Density Classiï¬cation . . . . . . . . . . . 6.6.3 The Naive Bayes Classiï¬er . . . . . . . . . . . . 6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 6.8 Mixture Models for Density Estimation and Classiï¬cation 6.9 Computational Considerations . . . . . . . . . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Model Assessment and Selection 7.1 Introduction . . . . . . . . . . . . . . . . . . 7.2 Bias, Variance and Model Complexity . . . . 7.3 The BiasâVariance Decomposition . . . . . . 7.3.1 Example: BiasâVariance Tradeoï¬ . 7.4 Optimism of the Training Error Rate . . . . 7.5 Estimates of In-Sample Prediction Error . . . 7.6 The Eï¬ective Number of Parameters . . . . . 7.7 The Bayesian Approach and BIC . . . . . . . 7.8 Minimum Description Length . . . . . . . . . 7.9 VapnikâChervonenkis Dimension . . . . . . . 7.9.1 Example (Continued) . . . . . . . . 7.10 Cross-Validation . . . . . . . . . . . . . . . . 7.10.1 K-Fold Cross-Validation . . . . . . 7.10.2 The Wrong and Right Way to Do Cross-validation . . . . . . . . 7.10.3 Does Cross-Validation Really Work? 7.11 Bootstrap Methods . . . . . . . . . . . . . . 7.11.1 Example (Continued) . . . . . . . . 7.12 Conditional or Expected Test Error? . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

191 192 194 197 198 200 201 203 203 205 208 208 210 210 212 214 216 216 216 219 219 219 223 226 228 230 232 233 235 237 239 241 241 245 247 249 252 254 257 257

8 Model Inference and Averaging 261 8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261

Contents

xvii

8.2

The Bootstrap and Maximum Likelihood Methods . . . . 8.2.1 A Smoothing Example . . . . . . . . . . . . . . 8.2.2 Maximum Likelihood Inference . . . . . . . . . . 8.2.3 Bootstrap versus Maximum Likelihood . . . . . 8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 8.4 Relationship Between the Bootstrap and Bayesian Inference . . . . . . . . . . . . . . . . . . . 8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 8.5.1 Two-Component Mixture Model . . . . . . . . . 8.5.2 The EM Algorithm in General . . . . . . . . . . 8.5.3 EM as a MaximizationâMaximization Procedure 8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.7.1 Example: Trees with Simulated Data . . . . . . 8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Additive Models, Trees, and Related Methods 9.1 Generalized Additive Models . . . . . . . . . . . . 9.1.1 Fitting Additive Models . . . . . . . . . . 9.1.2 Example: Additive Logistic Regression . 9.1.3 Summary . . . . . . . . . . . . . . . . . . 9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . 9.2.1 Background . . . . . . . . . . . . . . . . 9.2.2 Regression Trees . . . . . . . . . . . . . . 9.2.3 Classiï¬cation Trees . . . . . . . . . . . . 9.2.4 Other Issues . . . . . . . . . . . . . . . . 9.2.5 Spam Example (Continued) . . . . . . . 9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . 9.3.1 Spam Example (Continued) . . . . . . . 9.4 MARS: Multivariate Adaptive Regression Splines . 9.4.1 Spam Example (Continued) . . . . . . . 9.4.2 Example (Simulated Data) . . . . . . . . 9.4.3 Other Issues . . . . . . . . . . . . . . . . 9.5 Hierarchical Mixtures of Experts . . . . . . . . . . 9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . 9.7 Computational Considerations . . . . . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

261 261 265 267 267 271 272 272 276 277 279 282 283 288 290 292 293 295 295 297 299 304 305 305 307 308 310 313 317 320 321 326 327 328 329 332 334 334 335

10 Boosting and Additive Trees 337 10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337 10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340

xviii

Contents

10.2 10.3 10.4 10.5 10.6 10.7 10.8 10.9 10.10

Boosting Fits an Additive Model . . . . . . . . . . . Forward Stagewise Additive Modeling . . . . . . . . Exponential Loss and AdaBoost . . . . . . . . . . . Why Exponential Loss? . . . . . . . . . . . . . . . . Loss Functions and Robustness . . . . . . . . . . . . âOï¬-the-Shelfâ Procedures for Data Mining . . . . . Example: Spam Data . . . . . . . . . . . . . . . . . Boosting Trees . . . . . . . . . . . . . . . . . . . . . Numerical Optimization via Gradient Boosting . . . 10.10.1 Steepest Descent . . . . . . . . . . . . . . . 10.10.2 Gradient Boosting . . . . . . . . . . . . . . 10.10.3 Implementations of Gradient Boosting . . . 10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . 10.12 Regularization . . . . . . . . . . . . . . . . . . . . . 10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . 10.12.2 Subsampling . . . . . . . . . . . . . . . . . 10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . 10.13.1 Relative Importance of Predictor Variables 10.13.2 Partial Dependence Plots . . . . . . . . . . 10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . 10.14.1 California Housing . . . . . . . . . . . . . . 10.14.2 New Zealand Fish . . . . . . . . . . . . . . 10.14.3 Demographics Data . . . . . . . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 Neural Networks 11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . 11.2 Projection Pursuit Regression . . . . . . . . . . . . 11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . 11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . 11.5 Some Issues in Training Neural Networks . . . . . . 11.5.1 Starting Values . . . . . . . . . . . . . . . . 11.5.2 Overï¬tting . . . . . . . . . . . . . . . . . . 11.5.3 Scaling of the Inputs . . . . . . . . . . . . 11.5.4 Number of Hidden Units and Layers . . . . 11.5.5 Multiple Minima . . . . . . . . . . . . . . . 11.6 Example: Simulated Data . . . . . . . . . . . . . . . 11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . 11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . 11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 11.9.1 Bayes, Boosting and Bagging . . . . . . . . 11.9.2 Performance Comparisons . . . . . . . . . 11.10 Computational Considerations . . . . . . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

341 342 343 345 346 350 352 353 358 358 359 360 361 364 364 365 367 367 369 371 371 375 379 380 384 389 389 389 392 395 397 397 398 398 400 400 401 404 408 409 410 412 414 415

Contents

xix

Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415 12 Support Vector Machines and Flexible Discriminants 12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . 12.2 The Support Vector Classiï¬er . . . . . . . . . . . . . . . 12.2.1 Computing the Support Vector Classiï¬er . . . 12.2.2 Mixture Example (Continued) . . . . . . . . . 12.3 Support Vector Machines and Kernels . . . . . . . . . . 12.3.1 Computing the SVM for Classiï¬cation . . . . . 12.3.2 The SVM as a Penalization Method . . . . . . 12.3.3 Function Estimation and Reproducing Kernels 12.3.4 SVMs and the Curse of Dimensionality . . . . 12.3.5 A Path Algorithm for the SVM Classiï¬er . . . 12.3.6 Support Vector Machines for Regression . . . . 12.3.7 Regression and Kernels . . . . . . . . . . . . . 12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . 12.4 Generalizing Linear Discriminant Analysis . . . . . . . 12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . 12.5.1 Computing the FDA Estimates . . . . . . . . . 12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . 12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . 12.7.1 Example: Waveform Data . . . . . . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 Prototype Methods and Nearest-Neighbors 13.1 Introduction . . . . . . . . . . . . . . . . . . . . 13.2 Prototype Methods . . . . . . . . . . . . . . . . 13.2.1 K-means Clustering . . . . . . . . . . . 13.2.2 Learning Vector Quantization . . . . . 13.2.3 Gaussian Mixtures . . . . . . . . . . . . 13.3 k-Nearest-Neighbor Classiï¬ers . . . . . . . . . . 13.3.1 Example: A Comparative Study . . . . 13.3.2 Example: k-Nearest-Neighbors and Image Scene Classiï¬cation . . . . . 13.3.3 Invariant Metrics and Tangent Distance 13.4 Adaptive Nearest-Neighbor Methods . . . . . . . 13.4.1 Example . . . . . . . . . . . . . . . . . 13.4.2 Global Dimension Reduction for Nearest-Neighbors . . . . . . . . . . 13.5 Computational Considerations . . . . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . .

417 417 417 420 421 423 423 426 428 431 432 434 436 438 438 440 444 446 449 451 455 455 459 459 459 460 462 463 463 468 470 471 475 478 479 480 481 481

. . . . . . . . . . . . . . .

. . . . . . . . . . . . . . .

. . . . . . . . . . . . . . .

. . . . . . . . . . . . . . .

. . . . . . . . . . . . . . .

xx

Contents

14 Unsupervised Learning 14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . 14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . 14.2.1 Market Basket Analysis . . . . . . . . . . . . . 14.2.2 The Apriori Algorithm . . . . . . . . . . . . . 14.2.3 Example: Market Basket Analysis . . . . . . . 14.2.4 Unsupervised as Supervised Learning . . . . . 14.2.5 Generalized Association Rules . . . . . . . . . 14.2.6 Choice of Supervised Learning Method . . . . 14.2.7 Example: Market Basket Analysis (Continued) 14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . 14.3.1 Proximity Matrices . . . . . . . . . . . . . . . 14.3.2 Dissimilarities Based on Attributes . . . . . . 14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . 14.3.4 Clustering Algorithms . . . . . . . . . . . . . . 14.3.5 Combinatorial Algorithms . . . . . . . . . . . 14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . 14.3.7 Gaussian Mixtures as Soft K-means Clustering 14.3.8 Example: Human Tumor Microarray Data . . 14.3.9 Vector Quantization . . . . . . . . . . . . . . . 14.3.10 K-medoids . . . . . . . . . . . . . . . . . . . . 14.3.11 Practical Issues . . . . . . . . . . . . . . . . . 14.3.12 Hierarchical Clustering . . . . . . . . . . . . . 14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . 14.5 Principal Components, Curves and Surfaces . . . . . . . 14.5.1 Principal Components . . . . . . . . . . . . . . 14.5.2 Principal Curves and Surfaces . . . . . . . . . 14.5.3 Spectral Clustering . . . . . . . . . . . . . . . 14.5.4 Kernel Principal Components . . . . . . . . . . 14.5.5 Sparse Principal Components . . . . . . . . . . 14.6 Non-negative Matrix Factorization . . . . . . . . . . . . 14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . 14.7 Independent Component Analysis and Exploratory Projection Pursuit . . . . . . . . . . . 14.7.1 Latent Variables and Factor Analysis . . . . . 14.7.2 Independent Component Analysis . . . . . . . 14.7.3 Exploratory Projection Pursuit . . . . . . . . . 14.7.4 A Direct Approach to ICA . . . . . . . . . . . 14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . 14.9 Nonlinear Dimension Reduction and Local Multidimensional Scaling . . . . . . . . . . . 14.10 The Google PageRank Algorithm . . . . . . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

485 485 487 488 489 492 495 497 499 499 501 503 503 505 507 507 509 510 512 514 515 518 520 528 534 534 541 544 547 550 553 554 557 558 560 565 565 570 572 576 578 579

Contents

xxi

15 Random Forests 15.1 Introduction . . . . . . . . . . . . . . . . 15.2 Deï¬nition of Random Forests . . . . . . . 15.3 Details of Random Forests . . . . . . . . 15.3.1 Out of Bag Samples . . . . . . . 15.3.2 Variable Importance . . . . . . . 15.3.3 Proximity Plots . . . . . . . . . 15.3.4 Random Forests and Overï¬tting 15.4 Analysis of Random Forests . . . . . . . . 15.4.1 Variance and the De-Correlation 15.4.2 Bias . . . . . . . . . . . . . . . . 15.4.3 Adaptive Nearest Neighbors . . Bibliographic Notes . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Eï¬ect . . . . . . . . . . . . . . . .

. . . . . . . . . . . . .

. . . . . . . . . . . . .

. . . . . . . . . . . . .

. . . . . . . . . . . . .

. . . . . . . . . . . . .

587 587 587 592 592 593 595 596 597 597 600 601 602 603 605 605 607 607 610 613 616 617 622 623 624

16 Ensemble Learning 16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 16.2.1 Penalized Regression . . . . . . . . . . . . . . . 16.2.2 The âBet on Sparsityâ Principle . . . . . . . . . 16.2.3 Regularization Paths, Over-ï¬tting and Margins . 16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Undirected Graphical Models 17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . 17.2 Markov Graphs and Their Properties . . . . . . . . . . 17.3 Undirected Graphical Models for Continuous Variables 17.3.1 Estimation of the Parameters when the Graph Structure is Known . . . . . . 17.3.2 Estimation of the Graph Structure . . . . . . . 17.4 Undirected Graphical Models for Discrete Variables . . 17.4.1 Estimation of the Parameters when the Graph Structure is Known . . . . . . 17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . 17.4.3 Estimation of the Graph Structure . . . . . . . 17.4.4 Restricted Boltzmann Machines . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

625 . 625 . 627 . 630 . 631 . 635 . 638 . . . . . 639 641 642 643 645

18 High-Dimensional Problems: p â« N 649 18.1 When p is Much Bigger than N . . . . . . . . . . . . . . 649

xxii

Contents

Diagonal Linear Discriminant Analysis and Nearest Shrunken Centroids . . . . . . . . . . . . . . 18.3 Linear Classiï¬ers with Quadratic Regularization . . . . . 18.3.1 Regularized Discriminant Analysis . . . . . . . . 18.3.2 Logistic Regression with Quadratic Regularization . . . . . . . . . . 18.3.3 The Support Vector Classiï¬er . . . . . . . . . . 18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 18.3.5 Computational Shortcuts When p â« N . . . . . 18.4 Linear Classiï¬ers with L1 Regularization . . . . . . . . . 18.4.1 Application of Lasso to Protein Mass Spectroscopy . . . . . . . . . . 18.4.2 The Fused Lasso for Functional Data . . . . . . 18.5 Classiï¬cation When Features are Unavailable . . . . . . . 18.5.1 Example: String Kernels and Protein Classiï¬cation . . . . . . . . . . . . . 18.5.2 Classiï¬cation and Other Models Using Inner-Product Kernels and Pairwise Distances . 18.5.3 Example: Abstracts Classiï¬cation . . . . . . . . 18.6 High-Dimensional Regression: Supervised Principal Components . . . . . . . . . . . . . 18.6.1 Connection to Latent-Variable Modeling . . . . 18.6.2 Relationship with Partial Least Squares . . . . . 18.6.3 Pre-Conditioning for Feature Selection . . . . . 18.7 Feature Assessment and the Multiple-Testing Problem . . 18.7.1 The False Discovery Rate . . . . . . . . . . . . . 18.7.2 Asymmetric Cutpoints and the SAM Procedure 18.7.3 A Bayesian Interpretation of the FDR . . . . . . 18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . References Author Index Index

18.2

651 654 656 657 657 658 659 661 664 666 668 668 670 672 674 678 680 681 683 687 690 692 693 694 699 729 731

This is page 1 Printer: Opaque this

1
Introduction

Statistical learning plays a key role in many areas of science, ï¬nance and industry. Here are some examples of learning problems: â¢ Predict whether a patient, hospitalized due to a heart attack, will have a second heart attack. The prediction is to be based on demographic, diet and clinical measurements for that patient. â¢ Predict the price of a stock in 6 months from now, on the basis of company performance measures and economic data. â¢ Identify the numbers in a handwritten ZIP code, from a digitized image. â¢ Estimate the amount of glucose in the blood of a diabetic person, from the infrared absorption spectrum of that personâs blood. â¢ Identify the risk factors for prostate cancer, based on clinical and demographic variables. The science of learning plays a key role in the ï¬elds of statistics, data mining and artiï¬cial intelligence, intersecting with areas of engineering and other disciplines. This book is about learning from data. In a typical scenario, we have an outcome measurement, usually quantitative (such as a stock price) or categorical (such as heart attack/no heart attack), that we wish to predict based on a set of features (such as diet and clinical measurements). We have a training set of data, in which we observe the outcome and feature

2

1. Introduction

TABLE 1.1. Average percentage of words or characters in an email message equal to the indicated word or character. We have chosen the words and characters showing the largest diï¬erence between spam and email. george you your hp free hpl ! our re edu remove

spam email

0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29

0.28 0.01

measurements for a set of objects (such as people). Using this data we build a prediction model, or learner, which will enable us to predict the outcome for new unseen objects. A good learner is one that accurately predicts such an outcome. The examples above describe what is called the supervised learning problem. It is called âsupervisedâ because of the presence of the outcome variable to guide the learning process. In the unsupervised learning problem, we observe only the features and have no measurements of the outcome. Our task is rather to describe how the data are organized or clustered. We devote most of this book to supervised learning; the unsupervised problem is less developed in the literature, and is the focus of Chapter 14. Here are some examples of real learning problems that are discussed in this book.

Example 1: Email Spam
The data for this example consists of information from 4601 email messages, in a study to try to predict whether the email was junk email, or âspam.â The objective was to design an automatic spam detector that could ï¬lter out spam before clogging the usersâ mailboxes. For all 4601 email messages, the true outcome (email type) email or spam is available, along with the relative frequencies of 57 of the most commonly occurring words and punctuation marks in the email message. This is a supervised learning problem, with the outcome the class variable email/spam. It is also called a classiï¬cation problem. Table 1.1 lists the words and characters showing the largest average diï¬erence between spam and email. Our learning method has to decide which features to use and how: for example, we might use a rule such as if (%george < 0.6) & (%you > 1.5) Another form of a rule might be: if (0.2 Â· %you â 0.3 Â· %george) > 0 then spam else email. then spam else email.

1. Introduction
â1 1 2 3 4 o o o o oo o o o o o o o oooooo o ooo o o oooooooo oo o ooo o o ooo oooo oo ooo oo o o o oo o o o o oo o oo ooo o o ooo o o o oo o o o o o o o oo ooo o o o oo o o oooo o o oo o oo o o o oo o oo o o o o oo o oo ooo o oo o o oo o ooo oo oo o ooo o o o o o o oo o o oo o o ooo o o o o o o o o 40 o 50 o 60 70 o 80 o o o o o oo o o o o o o o oo o o o o o o o oo o oo o ooo oo o o oo o o o o o o oo o o oo oooo o o o o o o oo o o o o o o o o o o o o o o o o oo o o oo o o o o o o o oo oo o o o o o o o o o o o o oo o o o o o oo o o o o o o o oo o oo o o o o o o o o o o o o o o o o o oo o o oo o o o ooo o o oo oo o o o o ooo oo o o o ooo ooo o oo o oo oo o o o o o oo o o o o oo oo o o o o o o o o o o o o o o o o o o oo oo o o o oo o oo o o o ooo o o o o o o oo oooooo o oo o oo o o o ooo o o o o o o o o o o o o o o o o o 0.0 0.4 0.8 o o o o o o o o o o o o o o oo o o o o ooo o oo o o oo o o o o o o o oo o oo o o ooo o o o oo o o o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oo oo ooo oo o oo o o oooo o oo o o o oo o o o o oo o o oo o oo o o 6.0 7.0 o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o 8.0 9.0 o o o o o o o o o o o o o o oo o oo oo o o o o oooo o o o ooooo o o o o o oo oo o oo o o ooo o oo oo o o o o o o o o o o o oo o o o oo o o o o oo oo o o ooo o o o o o o oo o o o ooooooo o o oo oo o o o o oo oo o oo o o o o o o o o o o o o o o o o ooooo o oo oo o o o oo o o o o o o o o ooo o oo oo o o oo oo o ooooooo o o oo o o oo oo o oo o o o o o o oo o ooooo o o o o o o ooo o o o o o oo o o oo o o o o o o o o o o oo oo oo o oo oo o o o o o o o o o oo oo o oo oo o o o o o o ooo o oo ooo o o o ooo o oo ooo o o o oo o o o o o oo oo oo o

3

lpsa
4 o o o ooo o o ooo o oo oo o oo oo oo o o o oo o o ooo o o o ooo oo ooo o oo o o o o oo oo o o ooo o o o o oo o o o o oo o oo o o o o o o o oo oo o o oo oo oo o o o oooo o o o ooo oo o oo o o o oo oo ooo o oo o o o oo oo o oooo oo o o ooo o o ooo o o o oo ooo o oo o oo o 80 o oo oo o o o o oooo o o oo o o oo oooo oo o o oo oo o oooo o oo oo o o o o o ooo o o o ooo ooooo o o oo o oo o o o o oo o o o o o o o o o o o o o o oo o ooo o oo o o oo o o ooooo o o o o o ooo o o o oo o o o o o oo o o o o o o o oo oo ooo oo oo ooooooooooooooo o oo oooo o ooo oo o o o ooooo oo o 0.8

o o o o o o oo o oo o o ooo oo o o oo o o o oo oo oo o o ooo o o oooo oo o o ooo o o oo oo o o oo o oooo o o o o oo o o oo o o o o o oo

o oo o oo o oo o ooooo o o o o o ooo o o o o o oo o ooo oooo o o oooo o o o ooo oooo o oo o ooo o o oo o o o oo o o o o o o o oo o o o oo o o oo oo oooo o ooo o oooooo o oo o oo o o o ooo ooo oo o oo o ooooo o oo o o o oo o o o oo o oo o o o o o o oo o o o o o o o o o oo o oo o ooo oo o o o ooooo oo ooo o ooooooo o oo o ooooo o o oo o o o o o ooooo o o o o o o oo o o o o o o o oo o o oo o o o

o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o

o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o

o

o o o o o

lcavol
o o o o o oo oo oo o o o o o oo o o o oo o ooo oo oo oo o oo o o o o o o o ooo ooo ooo o ooooo o oo oo o o oo o ooo o o oo o oooo oo o o o o o oo o o o oo o o o o oo o o o o o o oo o oo oo o oooo o oo oo oo o o oooo o o o ooooooo o o o o o ooo o o oo o o oo o o oo oo o o o o o oo o o o o o o o o oo o o o oo o o o oo o o o oo o o o o o o o o oo o o o o o o o o o o o oo o o o o o o o oo o oo oooooooo oooo o o o o ooooo ooo ooo ooooo oo oo o oo o

1

o o o o o o o o o o o o o

3

o

o o o

â1

2

lweight
o o oo o o o o oo o oo o o oo o oooo ooooo oo o o o oo o o oooooo oo o oooooo oo o o o o ooo oo oooo oo oo o ooo o oo o o oo o o o o oo

o o o o o o o o o

70

o o

o o o o o o o o o o o o o o o o o o o o o o o

age
o oo o o oo o o oo oo ooooooo o o o oo o oo oo o o o o o oo o o o o ooo o o oo o o oo o ooo ooooooo oo oooooo oo o o ooooo o o ooo o o

40

o o oo o o o oo o o o o o o o o o oo o o oooo o o oo o o o o o o o o o oo oo o oo o o o o o oo o oo o o o o o oo o o o o o o o o o o o oo o o o o o o o o o o o o oo o o oo o oo o o o o o o o o o o o o o o o o o o oo oo o oo ooo o o o o o oo o o oo o ooooo o o o oooo o

o o o o o

o o o o o o o o o o o o o o o o o o o o o o o o o o o o o

50

60

ooo o oo o oo o o ooo oo o oo o o oo o oo o o oo o oo o o o ooo o o o o o o o o o o o oooooooo oo o o o ooo oo o oooo ooooooo ooo oo o

o o o o o o

o

o o o o oo o oo

ooo ooo o o ooo o o

0.0

0.4

svi
oo ooooooooo ooo o o oo oooo o o oooo o o o o o o o oo o o oo o o oo oo o o o oo o o o o o o o oo o oo o o o ooo o o oo o o ooo oo o o o oo ooooooooo o o o o oooo oo o o o oo o o oooo ooooooo oooo o o o oooooo ooo o o oo ooo o ooo o o ooo o o o oo o o o ooo oo o oo o o o o oooo o o ooo o o o o ooo oo o o o oo o o oooo ooo o ooo o o o o ooo o o o o o o ooooooo oooo o o o o ooo oooo o o o oo oooo ooo ooooo o o o ooo oo o o o o o o oo o o oo o oo oo o o oo o oo o o o o o o o o o o o oo o o ooo o o o o o oo o oo oo oooo ooo oooooo o o o ooo oo o o o o oo o o o o o ooooooooooo o o o o oooooooooo o o oooooo o o o ooo o o oo o oo o o o o ooo o o o oo oooo o o o o oo o o oo o o o oo o o oo o o o o o oo oo o o o o o o oo ooo o o oo o o oo o o ooooooo oooo o o o oo o o o o ooooooooooo o o o ooooooo oo o o o oo oo o o o o oooooo oo oo oooooo ooo o o o o o o o ooo oo o o oo o o o o oo o oo o oo o o oo oo o o o o o ooo oo o o o o oo o o o oooooo o o ooooooo oo oo o ooo o oo o 2.5 3.5 4.5 o oo o o ooooooooooo o oo ooooooooo o ooo o o o oo o o oo oo oo o o oo o oo o o o o o o o o o oo o oo o oo o oo oooo o oo oo o ooooooo o o oo oo oo o ooooooo o o o o o o o o o o o o oooooo oooo o oooooo o oo oo oo o o ooooo o o o oo oo oo o o o o o oo oo o o oo o o oo o o oo oo o oo oo o o o o oo o o ooo o o oo ooo o o o o o ooooooooo oo oo ooo oooooo oo o o o o o o oo ooo oooooooo o o ooo ooo ooo o oo o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oo o o oo oooo o oo oooo o o o o oo ooo o o o oo o o o o o o o o o o oooooooo o o ooo o o 100 o oo oo o o oo o o o oo oo o o o o o o o o o o o o o o o o o o o o o o o o o o o o oo o ooooo o o o o o o o o o oo o ooo o o oo o o oo o o o o oo o o o o o o o o oooo oo o ooo o o o o oo oooooo o o o o o ooo 3 0 20 0 20 60 100 60 â1 0 1 2 o o o o o o o o o oo ooo o o oo o o o o o o o o o o o oo o o o o o o oo o o o o o o o o oo oo o oo o o o ooo o o o o o ooo o ooo oo o ooo oo ooo oo oo o o o ooo o oo o oo o o o o

o

lcp

o o o o

o

9.0

ooo o o o o oo ooooooo o oooo ooo o o o o ooooo o o

8.0

6.0

7.0

gleason

oo ooooooooo oo o o ooo oo o o o oo o oo o o o oo oo o o o o oo oo o oo o o o o o o o o oo o o o oo o o ooo o o o o oo o o oo o oo ooooooooo oo o o oo oooo o ooo oo 0 1 2 3 4 5

o o o o o o o oo o o o o o o o o oo o o oo o o o o o o oo oo o o o o o o o o o oo o o o o o o o oooo oo oo â1 0 1 2

oo o o o o o o oo o o o oo o o o o o o o o o oo o ooo o o o o oo o o o o o o o o o oo o o o o oo o â1 0 1 2 3

o

o o o o o o o o o o o o o

pgg45
o

FIGURE 1.1. Scatterplot matrix of the prostate cancer data. The ï¬rst row shows the response against each of the predictors in turn. Two of the predictors, svi and gleason, are categorical.

For this problem not all errors are equal; we want to avoid ï¬ltering out good email, while letting spam get through is not desirable but less serious in its consequences. We discuss a number of diï¬erent methods for tackling this learning problem in the book.

Example 2: Prostate Cancer
The data for this example, displayed in Figure 1.11 , come from a study by Stamey et al. (1989) that examined the correlation between the level of
1 There was an error in these data in the ï¬rst edition of this book. Subject 32 had a value of 6.1 for lweight, which translates to a 449 gm prostate! The correct value is 44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.

â1

0

lbph

o

o o o o oo oo oo o o o o o o o

1

2

2.5

3.5

o

o o o o o

4.5

o

o o o oo oo o ooo ooo o oo oo o o o oo oo o o o o o oo oo o o o o o o o o oo o

0 1 2 3 4 5

4

1. Introduction

FIGURE 1.2. Examples of handwritten digits from U.S. postal envelopes.

prostate speciï¬c antigen (PSA) and a number of clinical measures, in 97 men who were about to receive a radical prostatectomy. The goal is to predict the log of PSA (lpsa) from a number of measurements including log cancer volume (lcavol), log prostate weight lweight, age, log of benign prostatic hyperplasia amount lbph, seminal vesicle invasion svi, log of capsular penetration lcp, Gleason score gleason, and percent of Gleason scores 4 or 5 pgg45. Figure 1.1 is a scatterplot matrix of the variables. Some correlations with lpsa are evident, but a good predictive model is diï¬cult to construct by eye. This is a supervised learning problem, known as a regression problem, because the outcome measurement is quantitative.

Example 3: Handwritten Digit Recognition
The data from this example come from the handwritten ZIP codes on envelopes from U.S. postal mail. Each image is a segment from a ï¬ve digit ZIP code, isolating a single digit. The images are 16Ã16 eight-bit grayscale maps, with each pixel ranging in intensity from 0 to 255. Some sample images are shown in Figure 1.2. The images have been normalized to have approximately the same size and orientation. The task is to predict, from the 16 Ã 16 matrix of pixel intensities, the identity of each image (0, 1, . . . , 9) quickly and accurately. If it is accurate enough, the resulting algorithm would be used as part of an automatic sorting procedure for envelopes. This is a classiï¬cation problem for which the error rate needs to be kept very low to avoid misdirection of

1. Introduction

5

mail. In order to achieve this low error rate, some objects can be assigned to a âdonât knowâ category, and sorted instead by hand.

Example 4: DNA Expression Microarrays
DNA stands for deoxyribonucleic acid, and is the basic material that makes up human chromosomes. DNA microarrays measure the expression of a gene in a cell by measuring the amount of mRNA (messenger ribonucleic acid) present for that gene. Microarrays are considered a breakthrough technology in biology, facilitating the quantitative study of thousands of genes simultaneously from a single sample of cells. Here is how a DNA microarray works. The nucleotide sequences for a few thousand genes are printed on a glass slide. A target sample and a reference sample are labeled with red and green dyes, and each are hybridized with the DNA on the slide. Through ï¬uoroscopy, the log (red/green) intensities of RNA hybridizing at each site is measured. The result is a few thousand numbers, typically ranging from say â6 to 6, measuring the expression level of each gene in the target relative to the reference sample. Positive values indicate higher expression in the target versus the reference, and vice versa for negative values. A gene expression dataset collects together the expression values from a series of DNA microarray experiments, with each column representing an experiment. There are therefore several thousand rows representing individual genes, and tens of columns representing samples: in the particular example of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns), although for clarity only a random sample of 100 rows are shown. The ï¬gure displays the data set as a heat map, ranging from green (negative) to red (positive). The samples are 64 cancer tumors from diï¬erent patients. The challenge here is to understand how the genes and samples are organized. Typical questions include the following: (a) which samples are most similar to each other, in terms of their expression proï¬les across genes? (b) which genes are most similar to each other, in terms of their expression proï¬les across samples? (c) do certain genes show very high (or low) expression for certain cancer samples? We could view this task as a regression problem, with two categorical predictor variablesâgenes and samplesâwith the response variable being the level of expression. However, it is probably more useful to view it as unsupervised learning problem. For example, for question (a) above, we think of the samples as points in 6830âdimensional space, which we want to cluster together in some way.

6

1. Introduction

SIDW299104 SIDW380102 SID73161 GNAL H.sapiensmRNA SID325394 RASGTPASE SID207172 ESTs SIDW377402 HumanmRNA SIDW469884 ESTs SID471915 MYBPROTO ESTsChr.1 SID377451 DNAPOLYMER SID375812 SIDW31489 SID167117 SIDW470459 SIDW487261 Homosapiens SIDW376586 Chr MITOCHONDRIAL60 SID47116 ESTsChr.6 SIDW296310 SID488017 SID305167 ESTsChr.3 SID127504 SID289414 PTPRC SIDW298203 SIDW310141 SIDW376928 ESTsCh31 SID114241 SID377419 SID297117 SIDW201620 SIDW279664 SIDW510534 HLACLASSI SIDW203464 SID239012 SIDW205716 SIDW376776 HYPOTHETICAL WASWiskott SIDW321854 ESTsChr.15 SIDW376394 SID280066 ESTsChr.5 SIDW488221 SID46536 SIDW257915 ESTsChr.2 SIDW322806 SID200394 ESTsChr.15 SID284853 SID485148 SID297905 ESTs SIDW486740 SMALLNUC ESTs SIDW366311 SIDW357197 SID52979 ESTs SID43609 SIDW416621 ERLUMEN TUPLE1TUP1 SIDW428642 SID381079 SIDW298052 SIDW417270 SIDW362471 ESTsChr.15 SIDW321925 SID380265 SIDW308182 SID381508 SID377133 SIDW365099 ESTsChr.10 SIDW325120 SID360097 SID375990 SIDW128368 SID301902 SID31984 SID42354
BREAST RENAL MELANOMA MELANOMA MCF7D-repro COLON COLON K562B-repro COLON NSCLC LEUKEMIA RENAL MELANOMA BREAST CNS CNS RENAL MCF7A-repro NSCLC K562A-repro COLON CNS NSCLC NSCLC LEUKEMIA CNS OVARIAN BREAST LEUKEMIA MELANOMA MELANOMA OVARIAN OVARIAN NSCLC RENAL BREAST MELANOMA OVARIAN OVARIAN NSCLC RENAL BREAST MELANOMA LEUKEMIA COLON BREAST LEUKEMIA COLON CNS MELANOMA NSCLC PROSTATE NSCLC RENAL RENAL NSCLC RENAL LEUKEMIA OVARIAN PROSTATE COLON BREAST RENAL UNKNOWN

FIGURE 1.3. DNA microarray data: expression matrix of 6830 genes (rows) and 64 samples (columns), for the human tumor data. Only a random sample of 100 rows are shown. The display is a heat map, ranging from bright green (negative, under expressed) to bright red (positive, over expressed). Missing values are gray. The rows and columns are displayed in a randomly chosen order.

1. Introduction

7

Who Should Read this Book
This book is designed for researchers and students in a broad variety of ï¬elds: statistics, artiï¬cial intelligence, engineering, ï¬nance and others. We expect that the reader will have had at least one elementary course in statistics, covering basic topics including linear regression. We have not attempted to write a comprehensive catalog of learning methods, but rather to describe some of the most important techniques. Equally notable, we describe the underlying concepts and considerations by which a researcher can judge a learning method. We have tried to write this book in an intuitive fashion, emphasizing concepts rather than mathematical details. As statisticians, our exposition will naturally reï¬ect our backgrounds and areas of expertise. However in the past eight years we have been attending conferences in neural networks, data mining and machine learning, and our thinking has been heavily inï¬uenced by these exciting ï¬elds. This inï¬uence is evident in our current research, and in this book.

How This Book is Organized
Our view is that one must understand simple methods before trying to grasp more complex ones. Hence, after giving an overview of the supervising learning problem in Chapter 2, we discuss linear methods for regression and classiï¬cation in Chapters 3 and 4. In Chapter 5 we describe splines, wavelets and regularization/penalization methods for a single predictor, while Chapter 6 covers kernel methods and local regression. Both of these sets of methods are important building blocks for high-dimensional learning techniques. Model assessment and selection is the topic of Chapter 7, covering the concepts of bias and variance, overï¬tting and methods such as cross-validation for choosing models. Chapter 8 discusses model inference and averaging, including an overview of maximum likelihood, Bayesian inference and the bootstrap, the EM algorithm, Gibbs sampling and bagging, A related procedure called boosting is the focus of Chapter 10. In Chapters 9â13 we describe a series of structured methods for supervised learning, with Chapters 9 and 11 covering regression and Chapters 12 and 13 focusing on classiï¬cation. Chapter 14 describes methods for unsupervised learning. Two recently proposed techniques, random forests and ensemble learning, are discussed in Chapters 15 and 16. We describe undirected graphical models in Chapter 17 and ï¬nally we study highdimensional problems in Chapter 18. At the end of each chapter we discuss computational considerations important for data mining applications, including how the computations scale with the number of observations and predictors. Each chapter ends with Bibliographic Notes giving background references for the material.

8

1. Introduction

We recommend that Chapters 1â4 be ï¬rst read in sequence. Chapter 7 should also be considered mandatory, as it covers central concepts that pertain to all learning methods. With this in mind, the rest of the book can be read sequentially, or sampled, depending on the readerâs interest. indicates a technically diï¬cult section, one that can The symbol be skipped without interrupting the ï¬ow of the discussion.

Book Website
The website for this book is located at
http://www-stat.stanford.edu/ElemStatLearn

It contains a number of resources, including many of the datasets used in this book.

Note for Instructors
We have successively used the ï¬rst edition of this book as the basis for a two-quarter course, and with the additional materials in this second edition, it could even be used for a three-quarter sequence. Exercises are provided at the end of each chapter. It is important for students to have access to good software tools for these topics. We used the R and S-PLUS programming languages in our courses.

This is page 9 Printer: Opaque this

2
Overview of Supervised Learning

2.1 Introduction
The ï¬rst three examples described in Chapter 1 have several components in common. For each there is a set of variables that might be denoted as inputs, which are measured or preset. These have some inï¬uence on one or more outputs. For each example the goal is to use the inputs to predict the values of the outputs. This exercise is called supervised learning. We have used the more modern language of machine learning. In the statistical literature the inputs are often called the predictors, a term we will use interchangeably with inputs, and more classically the independent variables. In the pattern recognition literature the term features is preferred, which we use as well. The outputs are called the responses, or classically the dependent variables.

2.2 Variable Types and Terminology
The outputs vary in nature among the examples. In the glucose prediction example, the output is a quantitative measurement, where some measurements are bigger than others, and measurements close in value are close in nature. In the famous Iris discrimination example due to R. A. Fisher, the output is qualitative (species of Iris) and assumes values in a ï¬nite set G = {Virginica, Setosa and Versicolor}. In the handwritten digit example the output is one of 10 diï¬erent digit classes: G = {0, 1, . . . , 9}. In both of

10

2. Overview of Supervised Learning

these there is no explicit ordering in the classes, and in fact often descriptive labels rather than numbers are used to denote the classes. Qualitative variables are also referred to as categorical or discrete variables as well as factors. For both types of outputs it makes sense to think of using the inputs to predict the output. Given some speciï¬c atmospheric measurements today and yesterday, we want to predict the ozone level tomorrow. Given the grayscale values for the pixels of the digitized image of the handwritten digit, we want to predict its class label. This distinction in output type has led to a naming convention for the prediction tasks: regression when we predict quantitative outputs, and classiï¬cation when we predict qualitative outputs. We will see that these two tasks have a lot in common, and in particular both can be viewed as a task in function approximation. Inputs also vary in measurement type; we can have some of each of qualitative and quantitative input variables. These have also led to distinctions in the types of methods that are used for prediction: some methods are deï¬ned most naturally for quantitative inputs, some most naturally for qualitative and some for both. A third variable type is ordered categorical, such as small, medium and large, where there is an ordering between the values, but no metric notion is appropriate (the diï¬erence between medium and small need not be the same as that between large and medium). These are discussed further in Chapter 4. Qualitative variables are typically represented numerically by codes. The easiest case is when there are only two classes or categories, such as âsuccessâ or âfailure,â âsurvivedâ or âdied.â These are often represented by a single binary digit or bit as 0 or 1, or else by â1 and 1. For reasons that will become apparent, such numeric codes are sometimes referred to as targets. When there are more than two categories, several alternatives are available. The most useful and commonly used coding is via dummy variables. Here a K-level qualitative variable is represented by a vector of K binary variables or bits, only one of which is âonâ at a time. Although more compact coding schemes are possible, dummy variables are symmetric in the levels of the factor. We will typically denote an input variable by the symbol X. If X is a vector, its components can be accessed by subscripts Xj . Quantitative outputs will be denoted by Y , and qualitative outputs by G (for group). We use uppercase letters such as X, Y or G when referring to the generic aspects of a variable. Observed values are written in lowercase; hence the ith observed value of X is written as xi (where xi is again a scalar or vector). Matrices are represented by bold uppercase letters; for example, a set of N input p-vectors xi , i = 1, . . . , N would be represented by the N Ãp matrix X. In general, vectors will not be bold, except when they have N components; this convention distinguishes a p-vector of inputs xi for the

2.3 Least Squares and Nearest Neighbors

11

ith observation from the N -vector xj consisting of all the observations on variable Xj . Since all vectors are assumed to be column vectors, the ith row of X is xT , the vector transpose of xi . i For the moment we can loosely state the learning task as follows: given the value of an input vector X, make a good prediction of the output Y, Ë denoted by Y (pronounced ây-hatâ). If Y takes values in IR then so should Ë ; likewise for categorical outputs, G should take values in the same set G Ë Y associated with G. For a two-class G, one approach is to denote the binary coded target Ë as Y , and then treat it as a quantitative output. The predictions Y will Ë the class label according to typically lie in [0, 1], and we can assign to G whether y > 0.5. This approach generalizes to K-level qualitative outputs Ë as well. We need data to construct prediction rules, often a lot of it. We thus suppose we have available a set of measurements (xi , yi ) or (xi , gi ), i = 1, . . . , N , known as the training data, with which to construct our prediction rule.

2.3 Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors
In this section we develop two simple but powerful prediction methods: the linear model ï¬t by least squares and the k-nearest-neighbor prediction rule. The linear model makes huge assumptions about structure and yields stable but possibly inaccurate predictions. The method of k-nearest neighbors makes very mild structural assumptions: its predictions are often accurate but can be unstable.

2.3.1 Linear Models and Least Squares
The linear model has been a mainstay of statistics for the past 30 years and remains one of our most important tools. Given a vector of inputs X T = (X1 , X2 , . . . , Xp ), we predict the output Y via the model
p

Ë Ë Y = Î²0 +
j=1

Ë Xj Î²j .

(2.1)

Ë The term Î²0 is the intercept, also known as the bias in machine learning. Ë Often it is convenient to include the constant variable 1 in X, include Î²0 in Ë and then write the linear model in vector form the vector of coeï¬cients Î², as an inner product Ë Ë Y = X T Î², (2.2)

12

2. Overview of Supervised Learning

where X T denotes vector or matrix transpose (X being a column vector). Ë Ë Here we are modeling a single output, so Y is a scalar; in general Y can be a Kâvector, in which case Î² would be a p Ã K matrix of coeï¬cients. In the Ë (p + 1)-dimensional inputâoutput space, (X, Y ) represents a hyperplane. If the constant is included in X, then the hyperplane includes the origin and is a subspace; if not, it is an aï¬ne set cutting the Y -axis at the point Ë Ë (0, Î²0 ). From now on we assume that the intercept is included in Î². Viewed as a function over the p-dimensional input space, f (X) = X T Î² is linear, and the gradient f â² (X) = Î² is a vector in input space that points in the steepest uphill direction. How do we ï¬t the linear model to a set of training data? There are many diï¬erent methods, but by far the most popular is the method of least squares. In this approach, we pick the coeï¬cients Î² to minimize the residual sum of squares
N

RSS(Î²) =
i=1

(yi â xT Î²)2 . i

(2.3)

RSS(Î²) is a quadratic function of the parameters, and hence its minimum always exists, but may not be unique. The solution is easiest to characterize in matrix notation. We can write RSS(Î²) = (y â XÎ²)T (y â XÎ²), (2.4)

If XT X is nonsingular, then the unique solution is given by Ë Î² = (XT X)â1 XT y,

where X is an N Ã p matrix with each row an input vector, and y is an N -vector of the outputs in the training set. Diï¬erentiating w.r.t. Î² we get the normal equations XT (y â XÎ²) = 0. (2.5) (2.6)

Ë and the ï¬tted value at the ith input xi is yi = y (xi ) = xT Î². At an arbiË Ë i T Ë trary input x0 the prediction is y (x0 ) = x0 Î². The entire ï¬tted surface is Ë Ë characterized by the p parameters Î². Intuitively, it seems that we do not need a very large data set to ï¬t such a model. Letâs look at an example of the linear model in a classiï¬cation context. Figure 2.1 shows a scatterplot of training data on a pair of inputs X1 and X2 . The data are simulated, and for the present the simulation model is not important. The output class variable G has the values BLUE or ORANGE, and is represented as such in the scatterplot. There are 100 points in each of the two classes. The linear regression model was ï¬t to these data, with Ë the response Y coded as 0 for BLUE and 1 for ORANGE. The ï¬tted values Y Ë according to the rule are converted to a ï¬tted class variable G Ë G=
ORANGE BLUE

Ë if Y > 0.5, Ë if Y â¤ 0.5.

(2.7)

2.3 Least Squares and Nearest Neighbors
Linear Regression of 0/1 Response
..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...... .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ....................................................................

13

oo o o o oo o o o o oo o o o o o o o o oo o oo o o oo o o o o oo o o o oo o o o o o o o o o o o o o o o o o o oo oo o o o o oo oo oo oo o oo o o oo oo o o o o o o ooo o oo o o o o o o o oo o oo o o o o oo o o o o o o o oo oo oo o o o o o o o o o o o oo o oo o oo o o oo o o oo o oo o o o o o o o o o o o o o o o o oo o o o oo o o o oo o o o o o

o o oo o

o

FIGURE 2.1. A classiï¬cation example in two dimensions. The classes are coded as a binary variable (BLUE = 0, ORANGE = 1), and then ï¬t by linear regression. Ë The line is the decision boundary deï¬ned by xT Î² = 0.5. The orange shaded region denotes that part of input space classiï¬ed as ORANGE, while the blue region is classiï¬ed as BLUE.

Ë The set of points in IR2 classiï¬ed as ORANGE corresponds to {x : xT Î² > 0.5}, indicated in Figure 2.1, and the two predicted classes are separated by the Ë decision boundary {x : xT Î² = 0.5}, which is linear in this case. We see that for these data there are several misclassiï¬cations on both sides of the decision boundary. Perhaps our linear model is too rigidâ or are such errors unavoidable? Remember that these are errors on the training data itself, and we have not said where the constructed data came from. Consider the two possible scenarios: Scenario 1: The training data in each class were generated from bivariate Gaussian distributions with uncorrelated components and diï¬erent means. Scenario 2: The training data in each class came from a mixture of 10 lowvariance Gaussian distributions, with individual means themselves distributed as Gaussian. A mixture of Gaussians is best described in terms of the generative model. One ï¬rst generates a discrete variable that determines which of

14

2. Overview of Supervised Learning

the component Gaussians to use, and then generates an observation from the chosen density. In the case of one Gaussian per class, we will see in Chapter 4 that a linear decision boundary is the best one can do, and that our estimate is almost optimal. The region of overlap is inevitable, and future data to be predicted will be plagued by this overlap as well. In the case of mixtures of tightly clustered Gaussians the story is different. A linear decision boundary is unlikely to be optimal, and in fact is not. The optimal decision boundary is nonlinear and disjoint, and as such will be much more diï¬cult to obtain. We now look at another classiï¬cation and regression procedure that is in some sense at the opposite end of the spectrum to the linear model, and far better suited to the second scenario.

2.3.2 Nearest-Neighbor Methods
Nearest-neighbor methods use those observations in the training set T closË est in input space to x to form Y . Speciï¬cally, the k-nearest neighbor ï¬t Ë for Y is deï¬ned as follows: 1 Ë Y (x) = k yi ,
xi âNk (x)

(2.8)

where Nk (x) is the neighborhood of x deï¬ned by the k closest points xi in the training sample. Closeness implies a metric, which for the moment we assume is Euclidean distance. So, in words, we ï¬nd the k observations with xi closest to x in input space, and average their responses. In Figure 2.2 we use the same training data as in Figure 2.1, and use 15-nearest-neighbor averaging of the binary coded response as the method Ë of ï¬tting. Thus Y is the proportion of ORANGEâs in the neighborhood, and Ë Ë so assigning class ORANGE to G if Y > 0.5 amounts to a majority vote in the neighborhood. The colored regions indicate all those points in input space classiï¬ed as BLUE or ORANGE by such a rule, in this case found by evaluating the procedure on a ï¬ne grid in input space. We see that the decision boundaries that separate the BLUE from the ORANGE regions are far more irregular, and respond to local clusters where one class dominates. Ë Figure 2.3 shows the results for 1-nearest-neighbor classiï¬cation: Y is assigned the value yâ of the closest point xâ to x in the training data. In this case the regions of classiï¬cation can be computed relatively easily, and correspond to a Voronoi tessellation of the training data. Each point xi has an associated tile bounding the region for which it is the closest input Ë point. For all points x in the tile, G(x) = gi . The decision boundary is even more irregular than before. The method of k-nearest-neighbor averaging is deï¬ned in exactly the same way for regression of a quantitative output Y , although k = 1 would be an unlikely choice.

2.3 Least Squares and Nearest Neighbors
15-Nearest Neighbor Classifier
.. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . ..................................................................... . ..................................................................... ..................................................................... ..................................................................... .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . ..................................................................... . ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. ... .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. ... .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. ... .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. ... .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. .. . .. .. .. . .. . . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. .. . .. .. .. . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. .. . .. .. .. . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .. .. ... .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . . ..................................................................... . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. ..................................................................... ..................................................................... ... ... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... . ..................................................................... ................... . . ................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... .................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... .....................................................................

15

oo o o o oo o o o o oo o o o o o o o o oo o oo o o oo o o o o oo o o o oo o o o o o o o o o o o o o o o o o o oo oo o o o o oo oo oo oo o oo o o oo oo o o o o o o ooo o oo o o o o o o o oo o oo o o o o oo o o o o oo o o o o o o o o o o oo o o o o o o oo o oo o oo o o o oo o oo o o o oo o o o o o o o o o o o o o o oo o o o oo o o o oo o o o o o

o o oo o

o

FIGURE 2.2. The same classiï¬cation example in two dimensions as in Figure 2.1. The classes are coded as a binary variable (BLUE = 0, ORANGE = 1) and then ï¬t by 15-nearest-neighbor averaging as in (2.8). The predicted class is hence chosen by majority vote amongst the 15-nearest neighbors.

In Figure 2.2 we see that far fewer training observations are misclassiï¬ed than in Figure 2.1. This should not give us too much comfort, though, since in Figure 2.3 none of the training data are misclassiï¬ed. A little thought suggests that for k-nearest-neighbor ï¬ts, the error on the training data should be approximately an increasing function of k, and will always be 0 for k = 1. An independent test set would give us a more satisfactory means for comparing the diï¬erent methods. It appears that k-nearest-neighbor ï¬ts have a single parameter, the number of neighbors k, compared to the p parameters in least-squares ï¬ts. Although this is the case, we will see that the eï¬ective number of parameters of k-nearest neighbors is N/k and is generally bigger than p, and decreases with increasing k. To get an idea of why, note that if the neighborhoods were nonoverlapping, there would be N/k neighborhoods and we would ï¬t one parameter (a mean) in each neighborhood. It is also clear that we cannot use sum-of-squared errors on the training set as a criterion for picking k, since we would always pick k = 1! It would seem that k-nearest-neighbor methods would be more appropriate for the mixture Scenario 2 described above, while for Gaussian data the decision boundaries of k-nearest neighbors would be unnecessarily noisy.

16

2. Overview of Supervised Learning
1-Nearest Neighbor Classifier
..................................................................... . ..................................................................... ..................................................................... ..................................................................... ............................................................... ...... . .......................... ..................................... ...... .......................... ...... .............................. ....... ............................... ............... ......................... ....... .............................. ....... . ......................... ...... ............................... ....... ........................ ...... ....................................... .............................................................. ....... .................... ......................................... ........ .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. ... .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . .. .. . .. .. . .. .. . .. .... . .. .. . .. .. . .. .. .. ... .. . .. .. . .... .. . .. .. . .. .. .. . .. . . .. . .. .. . .. .. . .. .... . .. .. . .. .. . . . .. .. . .. .. . .. .. . .... .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .... . . . .. ... .. . . . .. .. . .. .. . .. .. . .... .. . .. .. . . . .. .. . .. . .................... ........... ........... .................. ......... .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . . . .. .. . .... . .. .. . .... .. . .. .. . . . .. .. . .. . . .. . .. .. . .. .. . .. .. .. . .. .. . .... . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . . . .. .. . ... .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . . . .. .. . ... .. . .. .. . .. .. . .... .. . .. .. . .. .. . .. .. .. . .. .. . .... . .. .. .. . .. .. . . . .. .. . ... . ................... .......... .............................. .......... .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. ... .. . .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. ... .. . .. .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . . . .. . .. .. . .. .. .. . .. .. ... . . ... .. .. . .. .. . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . . . .. . .... . .. .. .. . .. .. ... .. ... .. .. . .. .. . .. .. .. . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . . .. . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . .. . . . . . . . . . . . . . .. . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . .. . . . . . . . . . . . . .. . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . .. . .. . . . . . . . . . . . . . .. . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . .. . .. . . . . . . . . . . . . . . .. . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . .. . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . .. . . . . . .. .. . . . . . . . .. . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . .. . . . . . . . . . .. . . . . . . . .. . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . .. . . . . . . . . . . .. . . . . . . . .. . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . .. . . . . . . . . . . . .. . . . . . . . .. . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . .. . . . . .. . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . .. . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . .. . .. .. . .. .. . .. .. .. . .. .. . .... . .. .. .. . .. .. . .. .. ... .. .. . .. .. . .... .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. ... .. . .. .. .. . .. .. . .. .. ... .. .. . .. . . . .... .. . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . .. . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .. . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . .. . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . .. . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . .................................... .................... ............. . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . .. . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . . ... . .. .. . .. .. .. . .. .. . .. .. ... .. . ........................................................... .......... .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . . . .. ... .. . .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. ... . .............................................................. ....... . ............................................................... ...... ..................................................................... ... ..................................................................... . ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ....................................................................

oo o o o oo o o o o oo o o o o o o o o oo o oo o o oo o o o o oo o o o oo o o o o o o o o o o o o o o o o o o oo oo o o o o oo oo oo oo o oo o o oo oo o o o o o o ooo o oo o o o o o o o oo o oo o o o o oo o o o o oo o o o o o o o o o o oo o o o o o o oo o oo o oo o o o oo o oo o o o oo o o o o o o o o o o o o o o oo o o o oo o o o oo o o o o o

o o oo o

o

FIGURE 2.3. The same classiï¬cation example in two dimensions as in Figure 2.1. The classes are coded as a binary variable (BLUE = 0, ORANGE = 1), and then predicted by 1-nearest-neighbor classiï¬cation.

2.3.3 From Least Squares to Nearest Neighbors
The linear decision boundary from least squares is very smooth, and apparently stable to ï¬t. It does appear to rely heavily on the assumption that a linear decision boundary is appropriate. In language we will develop shortly, it has low variance and potentially high bias. On the other hand, the k-nearest-neighbor procedures do not appear to rely on any stringent assumptions about the underlying data, and can adapt to any situation. However, any particular subregion of the decision boundary depends on a handful of input points and their particular positions, and is thus wiggly and unstableâhigh variance and low bias. Each method has its own situations for which it works best; in particular linear regression is more appropriate for Scenario 1 above, while nearest neighbors are more suitable for Scenario 2. The time has come to expose the oracle! The data in fact were simulated from a model somewhere between the two, but closer to Scenario 2. First we generated 10 means mk from a bivariate Gaussian distribution N ((1, 0)T , I) and labeled this class BLUE. Similarly, 10 more were drawn from N ((0, 1)T , I) and labeled class ORANGE. Then for each class we generated 100 observations as follows: for each observation, we picked an mk at random with probability 1/10, and

2.3 Least Squares and Nearest Neighbors
k â Number of Nearest Neighbors 151 101 69 45 31 21 11 7 5 3 1

17

0.30

Linear

Test Error

0.10

0.15

0.20

0.25

Train Test Bayes 2 3 5 8 12 18 29 67 200

Degrees of Freedom â N/k

FIGURE 2.4. Misclassiï¬cation curves for the simulation example used in Figures 2.1, 2.2 and 2.3. A single training sample of size 200 was used, and a test sample of size 10, 000. The orange curves are test and the blue are training error for k-nearest-neighbor classiï¬cation. The results for linear regression are the bigger orange and blue squares at three degrees of freedom. The purple line is the optimal Bayes error rate.

then generated a N (mk , I/5), thus leading to a mixture of Gaussian clusters for each class. Figure 2.4 shows the results of classifying 10,000 new observations generated from the model. We compare the results for least squares and those for k-nearest neighbors for a range of values of k. A large subset of the most popular techniques in use today are variants of these two simple procedures. In fact 1-nearest-neighbor, the simplest of all, captures a large percentage of the market for low-dimensional problems. The following list describes some ways in which these simple procedures have been enhanced: â¢ Kernel methods use weights that decrease smoothly to zero with distance from the target point, rather than the eï¬ective 0/1 weights used by k-nearest neighbors. â¢ In high-dimensional spaces the distance kernels are modiï¬ed to emphasize some variable more than others.

18

2. Overview of Supervised Learning

â¢ Local regression ï¬ts linear models by locally weighted least squares, rather than ï¬tting constants locally. â¢ Linear models ï¬t to a basis expansion of the original inputs allow arbitrarily complex models. â¢ Projection pursuit and neural network models consist of sums of nonlinearly transformed linear models.

2.4 Statistical Decision Theory
In this section we develop a small amount of theory that provides a framework for developing models such as those discussed informally so far. We ï¬rst consider the case of a quantitative output, and place ourselves in the world of random variables and probability spaces. Let X â IRp denote a real valued random input vector, and Y â IR a real valued random output variable, with joint distribution Pr(X, Y ). We seek a function f (X) for predicting Y given values of the input X. This theory requires a loss function L(Y, f (X)) for penalizing errors in prediction, and by far the most common and convenient is squared error loss: L(Y, f (X)) = (Y â f (X))2 . This leads us to a criterion for choosing f , EPE(f ) = = E(Y â f (X))2 [y â f (x)] Pr(dx, dy),
2

(2.9) (2.10)

the expected (squared) prediction error . By conditioning1 on X, we can write EPE as EPE(f ) = EX EY |X [Y â f (X)]2 |X (2.11) and we see that it suï¬ces to minimize EPE pointwise: f (x) = argminc EY |X [Y â c]2 |X = x . The solution is f (x) = E(Y |X = x), (2.13) the conditional expectation, also known as the regression function. Thus the best prediction of Y at any point X = x is the conditional mean, when best is measured by average squared error. The nearest-neighbor methods attempt to directly implement this recipe using the training data. At each point x, we might ask for the average of all
1 Conditioning here amounts to factoring the joint density Pr(X, Y ) = Pr(Y |X)Pr(X) where Pr(Y |X) = Pr(Y, X)/Pr(X), and splitting up the bivariate integral accordingly.

(2.12)

2.4 Statistical Decision Theory

19

those yi s with input xi = x. Since there is typically at most one observation at any point x, we settle for Ë f (x) = Ave(yi |xi â Nk (x)), (2.14)

where âAveâ denotes average, and Nk (x) is the neighborhood containing the k points in T closest to x. Two approximations are happening here: â¢ expectation is approximated by averaging over sample data; â¢ conditioning at a point is relaxed to conditioning on some region âcloseâ to the target point. For large training sample size N , the points in the neighborhood are likely to be close to x, and as k gets large the average will get more stable. In fact, under mild regularity conditions on the joint probability distribution Pr(X, Y ), one can show that as N, k â â such that k/N â 0, Ë f (x) â E(Y |X = x). In light of this, why look further, since it seems we have a universal approximator? We often do not have very large samples. If the linear or some more structured model is appropriate, then we can usually get a more stable estimate than k-nearest neighbors, although such knowledge has to be learned from the data as well. There are other problems though, sometimes disastrous. In Section 2.5 we see that as the dimension p gets large, so does the metric size of the k-nearest neighborhood. So settling for nearest neighborhood as a surrogate for conditioning will fail us miserably. The convergence above still holds, but the rate of convergence decreases as the dimension increases. How does linear regression ï¬t into this framework? The simplest explanation is that one assumes that the regression function f (x) is approximately linear in its arguments: f (x) â xT Î². (2.15) This is a model-based approachâwe specify a model for the regression function. Plugging this linear model for f (x) into EPE (2.9) and diï¬erentiating we can solve for Î² theoretically: Î² = [E(XX T )]â1 E(XY ). (2.16)

Note we have not conditioned on X; rather we have used our knowledge of the functional relationship to pool over values of X. The least squares solution (2.6) amounts to replacing the expectation in (2.16) by averages over the training data. So both k-nearest neighbors and least squares end up approximating conditional expectations by averages. But they diï¬er dramatically in terms of model assumptions: â¢ Least squares assumes f (x) is well approximated by a globally linear function.

20

2. Overview of Supervised Learning

â¢ k-nearest neighbors assumes f (x) is well approximated by a locally constant function. Although the latter seems more palatable, we have already seen that we may pay a price for this ï¬exibility. Many of the more modern techniques described in this book are model based, although far more ï¬exible than the rigid linear model. For example, additive models assume that
p

f (X) =
j=1

fj (Xj ).

(2.17)

This retains the additivity of the linear model, but each coordinate function fj is arbitrary. It turns out that the optimal estimate for the additive model uses techniques such as k-nearest neighbors to approximate univariate conditional expectations simultaneously for each of the coordinate functions. Thus the problems of estimating a conditional expectation in high dimensions are swept away in this case by imposing some (often unrealistic) model assumptions, in this case additivity. Are we happy with the criterion (2.11)? What happens if we replace the L2 loss function with the L1 : E|Y â f (X)|? The solution in this case is the conditional median, Ë f (x) = median(Y |X = x), (2.18)

which is a diï¬erent measure of location, and its estimates are more robust than those for the conditional mean. L1 criteria have discontinuities in their derivatives, which have hindered their widespread use. Other more resistant loss functions will be mentioned in later chapters, but squared error is analytically convenient and the most popular. What do we do when the output is a categorical variable G? The same paradigm works here, except we need a diï¬erent loss function for penalizing Ë prediction errors. An estimate G will assume values in G, the set of possible classes. Our loss function can be represented by a K Ã K matrix L, where K = card(G). L will be zero on the diagonal and nonnegative elsewhere, where L(k, â) is the price paid for classifying an observation belonging to class Gk as Gâ . Most often we use the zeroâone loss function, where all misclassiï¬cations are charged a single unit. The expected prediction error is Ë EPE = E[L(G, G(X))], (2.19) where again the expectation is taken with respect to the joint distribution Pr(G, X). Again we condition, and can write EPE as
K

EPE = EX
k=1

Ë L[Gk , G(X)]Pr(Gk |X)

(2.20)

2.4 Statistical Decision Theory
Bayes Optimal Classifier
..................................................................... ..................................................................... . ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ........................................................ ............. . ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. ........................................................ ............. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. . . . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. .. .. . .. . . . .. .. .. . .. . .. . .. .. . .. .. . .. .. .. . .. .. . .. ... .. . . .. . .. .. . .. ... .. .. .. . .. . . . .. .. .. . .. . .... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. .. . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . ... .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . .. ... .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. . .. .. .. . .. .. . .. .. .. . .. . .... .. ........ . .. .................................................................... . .................................................................... . .................................................................... . ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................... ..................................................................

21

oo o o o oo o o o o oo o o o o o o o o oo o oo o o oo o o o o oo o o o oo o o o o o o o o o o o o o o o o o o oo oo o o o o oo oo oo oo o oo o o oo oo o o o o o o ooo o oo o o o o o o o oo o oo o o o o oo o o o o oo o o o o o o o o o o oo o o o o o o oo o oo o oo o o o oo o oo o o o oo o o o o o o o o o o o o o o oo o o o oo o o o oo o o o o o

o o oo o

o

FIGURE 2.5. The optimal Bayes decision boundary for the simulation example of Figures 2.1, 2.2 and 2.3. Since the generating density is known for each class, this boundary can be calculated exactly (Exercise 2.2).

and again it suï¬ces to minimize EPE pointwise:
K

Ë G(x) = argmingâG
k=1

L(Gk , g)Pr(Gk |X = x).

(2.21)

With the 0â1 loss function this simpliï¬es to Ë G(x) = argmingâG [1 â Pr(g|X = x)] or simply Ë G(X) = Gk if Pr(Gk |X = x) = max Pr(g|X = x).
gâG

(2.22)

(2.23)

This reasonable solution is known as the Bayes classiï¬er, and says that we classify to the most probable class, using the conditional (discrete) distribution Pr(G|X). Figure 2.5 shows the Bayes-optimal decision boundary for our simulation example. The error rate of the Bayes classiï¬er is called the Bayes rate.

22

2. Overview of Supervised Learning

Again we see that the k-nearest neighbor classiï¬er directly approximates this solutionâa majority vote in a nearest neighborhood amounts to exactly this, except that conditional probability at a point is relaxed to conditional probability within a neighborhood of a point, and probabilities are estimated by training-sample proportions. Suppose for a two-class problem we had taken the dummy-variable approach and coded G via a binary Y , followed by squared error loss estimaË tion. Then f (X) = E(Y |X) = Pr(G = G1 |X) if G1 corresponded to Y = 1. Likewise for a K-class problem, E(Yk |X) = Pr(G = Gk |X). This shows that our dummy-variable regression procedure, followed by classiï¬cation to the largest ï¬tted value, is another way of representing the Bayes classiï¬er. Although this theory is exact, in practice problems can occur, depending on the regression model used. For example, when linear regression is used, Ë f (X) need not be positive, and we might be suspicious about using it as an estimate of a probability. We will discuss a variety of approaches to modeling Pr(G|X) in Chapter 4.

2.5 Local Methods in High Dimensions
We have examined two learning techniques for prediction so far: the stable but biased linear model and the less stable but apparently less biased class of k-nearest-neighbor estimates. It would seem that with a reasonably large set of training data, we could always approximate the theoretically optimal conditional expectation by k-nearest-neighbor averaging, since we should be able to ï¬nd a fairly large neighborhood of observations close to any x and average them. This approach and our intuition breaks down in high dimensions, and the phenomenon is commonly referred to as the curse of dimensionality (Bellman, 1961). There are many manifestations of this problem, and we will examine a few here. Consider the nearest-neighbor procedure for inputs uniformly distributed in a p-dimensional unit hypercube, as in Figure 2.6. Suppose we send out a hypercubical neighborhood about a target point to capture a fraction r of the observations. Since this corresponds to a fraction r of the unit volume, the expected edge length will be ep (r) = r1/p . In ten dimensions e10 (0.01) = 0.63 and e10 (0.1) = 0.80, while the entire range for each input is only 1.0. So to capture 1% or 10% of the data to form a local average, we must cover 63% or 80% of the range of each input variable. Such neighborhoods are no longer âlocal.â Reducing r dramatically does not help much either, since the fewer observations we average, the higher is the variance of our ï¬t. Another consequence of the sparse sampling in high dimensions is that all sample points are close to an edge of the sample. Consider N data points uniformly distributed in a p-dimensional unit ball centered at the origin. Suppose we consider a nearest-neighbor estimate at the origin. The median

2.5 Local Methods in High Dimensions
Unit Cube
1.0

23

p=10 p=3 p=2 p=1

Distance

0

1 Neighborhood

0.0
0.0

0.2

0.4

0.6

1

0.8

0.2

0.4

0.6

Fraction of Volume

FIGURE 2.6. The curse of dimensionality is well illustrated by a subcubical neighborhood for uniform data in a unit cube. The ï¬gure on the right shows the side-length of the subcube needed to capture a fraction r of the volume of the data, for diï¬erent dimensions p. In ten dimensions we need to cover 80% of the range of each coordinate to capture 10% of the data.

distance from the origin to the closest data point is given by the expression d(p, N ) = 1 â 1 1/N 2
1/p

(2.24)

(Exercise 2.3). A more complicated expression exists for the mean distance to the closest point. For N = 500, p = 10 , d(p, N ) â 0.52, more than halfway to the boundary. Hence most data points are closer to the boundary of the sample space than to any other data point. The reason that this presents a problem is that prediction is much more diï¬cult near the edges of the training sample. One must extrapolate from neighboring sample points rather than interpolate between them. Another manifestation of the curse is that the sampling density is proportional to N 1/p , where p is the dimension of the input space and N is the sample size. Thus, if N1 = 100 represents a dense sample for a single input problem, then N10 = 10010 is the sample size required for the same sampling density with 10 inputs. Thus in high dimensions all feasible training samples sparsely populate the input space. Let us construct another uniform example. Suppose we have 1000 training examples xi generated uniformly on [â1, 1]p . Assume that the true relationship between X and Y is Y = f (X) = eâ8||X|| , without any measurement error. We use the 1-nearest-neighbor rule to predict y0 at the test-point x0 = 0. Denote the training set by T . We can
2

24

2. Overview of Supervised Learning

compute the expected prediction error at x0 for our procedure, averaging over all such samples of size 1000. Since the problem is deterministic, this is the mean squared error (MSE) for estimating f (0): MSE(x0 ) = ET [f (x0 ) â y0 ]2 Ë

= ET [Ë0 â ET (Ë0 )]2 + [ET (Ë0 ) â f (x0 )]2 y y y 2 = VarT (Ë0 ) + Bias (Ë0 ). y y

(2.25)

Figure 2.7 illustrates the setup. We have broken down the MSE into two components that will become familiar as we proceed: variance and squared bias. Such a decomposition is always possible and often useful, and is known as the biasâvariance decomposition. Unless the nearest neighbor is at 0, y0 will be smaller than f (0) in this example, and so the average estimate Ë will be biased downward. The variance is due to the sampling variance of the 1-nearest neighbor. In low dimensions and with N = 1000, the nearest neighbor is very close to 0, and so both the bias and variance are small. As the dimension increases, the nearest neighbor tends to stray further from the target point, and both bias and variance are incurred. By p = 10, for more than 99% of the samples the nearest neighbor is a distance greater than 0.5 from the origin. Thus as p increases, the estimate tends to be 0 more often than not, and hence the MSE levels oï¬ at 1.0, as does the bias, and the variance starts dropping (an artifact of this example). Although this is a highly contrived example, similar phenomena occur more generally. The complexity of functions of many variables can grow exponentially with the dimension, and if we wish to be able to estimate such functions with the same accuracy as function in low dimensions, then we need the size of our training set to grow exponentially as well. In this example, the function is a complex interaction of all p variables involved. The dependence of the bias term on distance depends on the truth, and it need not always dominate with 1-nearest neighbor. For example, if the function always involves only a few dimensions as in Figure 2.8, then the variance can dominate instead. Suppose, on the other hand, that we know that the relationship between Y and X is linear, Y = X T Î² + Îµ, (2.26)

where Îµ â¼ N (0, Ï 2 ) and we ï¬t the model by least squares to the trainË ing data. For an arbitrary test point x0 , we have y0 = xT Î², which can Ë 0 N be written as y0 = xT Î² + i=1 âi (x0 )Îµi , where âi (x0 ) is the ith element Ë 0 of X(XT X)â1 x0 . Since under this model the least squares estimates are

2.5 Local Methods in High Dimensions

25

1-NN in One Dimension
1.0 1.0

1-NN in One vs. Two Dimensions

â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢
-0.5 0.0 X1 0.5 1.0

0.8

â¢ f(X) 0.6

0.0

X2

0.5

â¢

â¢

0.4

â¢

0.2

-1.0

-0.5

0.0 X

0.5

1.0

-1.0 -1.0

0.0

Distance to 1-NN vs. Dimension
1.0

-0.5

MSE vs. Dimension

â¢
Average Distance to Nearest Neighbor

0.6

â¢ â¢ â¢ â¢ â¢ â¢ â¢
2 4 6 Dimension 8 10 Mse 0.4 0.6

0.8

â¢ â¢

â¢ â¢ â¢

MSE Variance Sq. Bias

â¢ â¢ â¢ â¢

â¢

â¢ â¢

0.2

0.4

0.8

0.2

â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢
2 4 6 Dimension 8 10

0.0

FIGURE 2.7. A simulation example, demonstrating the curse of dimensionality and its eï¬ect on MSE, bias and variance. The input features are uniformly distributed in [â1, 1]p for p = 1, . . . , 10 The top left panel shows the target func2 tion (no noise) in IR: f (X) = eâ8||X|| , and demonstrates the error that 1-nearest neighbor makes in estimating f (0). The training point is indicated by the blue tick mark. The top right panel illustrates why the radius of the 1-nearest neighborhood increases with dimension p. The lower left panel shows the average radius of the 1-nearest neighborhoods. The lower-right panel shows the MSE, squared bias and variance curves as a function of dimension p.

0.0

26

2. Overview of Supervised Learning
1-NN in One Dimension
0.25 4

MSE vs. Dimension

â¢ â¢ â¢

MSE Variance Sq. Bias

â¢ â¢ â¢ â¢ â¢ â¢

â¢ â¢

3

MSE

f(X)

2

1

0.10 0.0 0.05

0.15

0.20 â¢ -1.0 -0.5 0.0 X 0.5 1.0

â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢
2 4 6 Dimension 8 10

FIGURE 2.8. A simulation example with the same setup as in Figure 2.7. Here the function is constant in all but one dimension: F (X) = 1 (X1 + 1)3 . The 2 variance dominates.

unbiased, we ï¬nd that EPE(x0 ) = = Ë Ey0 |x0 ET (y0 â y0 )2

0

= Var(y0 |x0 ) + VarT (Ë0 ) + Bias2 (Ë0 ) y y = Ï 2 + ET xT (XT X)â1 x0 Ï 2 + 02 . 0

Var(y0 |x0 ) + ET [Ë0 â ET y0 ]2 + [ET y0 â xT Î²]2 y Ë Ë 0 (2.27)

Here we have incurred an additional variance Ï 2 in the prediction error, since our target is not deterministic. There is no bias, and the variance depends on x0 . If N is large and T were selected at random, and assuming E(X) = 0, then XT X â N Cov(X) and Ex0 EPE(x0 ) â¼ Ex0 xT Cov(X)â1 x0 Ï 2 /N + Ï 2 0 = trace[Cov(X)â1 Cov(x0 )]Ï 2 /N + Ï 2 = Ï 2 (p/N ) + Ï 2 . (2.28)

Here we see that the expected EPE increases linearly as a function of p, with slope Ï 2 /N . If N is large and/or Ï 2 is small, this growth in variance is negligible (0 in the deterministic case). By imposing some heavy restrictions on the class of models being ï¬tted, we have avoided the curse of dimensionality. Some of the technical details in (2.27) and (2.28) are derived in Exercise 2.5. Figure 2.9 compares 1-nearest neighbor vs. least squares in two situations, both of which have the form Y = f (X) + Îµ, X uniform as before, and Îµ â¼ N (0, 1). The sample size is N = 500. For the orange curve, f (x)

2.5 Local Methods in High Dimensions
Expected Prediction Error of 1NN vs. OLS
2.1

27

â¢
â¢ â¢

â¢

â¢
Linear Cubic

EPE Ratio

1.9

2.0

â¢

â¢

â¢

â¢

â¢

â¢

â¢

â¢ â¢ â¢
4

1.8

â¢

â¢
2

â¢

â¢

â¢
6

â¢

1.6

1.7

â¢

8

10

Dimension

FIGURE 2.9. The curves show the expected prediction error (at x0 = 0) for 1-nearest neighbor relative to least squares for the model Y = f (X) + Îµ. For the 1 orange curve, f (x) = x1 , while for the blue curve f (x) = 2 (x1 + 1)3 .

is linear in the ï¬rst coordinate, for the blue curve, cubic as in Figure 2.8. Shown is the relative EPE of 1-nearest neighbor to least squares, which appears to start at around 2 for the linear case. Least squares is unbiased in this case, and as discussed above the EPE is slightly above Ï 2 = 1. The EPE for 1-nearest neighbor is always above 2, since the variance of Ë f (x0 ) in this case is at least Ï 2 , and the ratio increases with dimension as the nearest neighbor strays from the target point. For the cubic case, least squares is biased, which moderates the ratio. Clearly we could manufacture examples where the bias of least squares would dominate the variance, and the 1-nearest neighbor would come out the winner. By relying on rigid assumptions, the linear model has no bias at all and negligible variance, while the error in 1-nearest neighbor is substantially larger. However, if the assumptions are wrong, all bets are oï¬ and the 1-nearest neighbor may dominate. We will see that there is a whole spectrum of models between the rigid linear models and the extremely ï¬exible 1-nearest-neighbor models, each with their own assumptions and biases, which have been proposed speciï¬cally to avoid the exponential growth in complexity of functions in high dimensions by drawing heavily on these assumptions. Before we delve more deeply, let us elaborate a bit on the concept of statistical models and see how they ï¬t into the prediction framework.

28

2. Overview of Supervised Learning

2.6 Statistical Models, Supervised Learning and Function Approximation
Ë Our goal is to ï¬nd a useful approximation f (x) to the function f (x) that underlies the predictive relationship between the inputs and outputs. In the theoretical setting of Section 2.4, we saw that squared error loss lead us to the regression function f (x) = E(Y |X = x) for a quantitative response. The class of nearest-neighbor methods can be viewed as direct estimates of this conditional expectation, but we have seen that they can fail in at least two ways: â¢ if the dimension of the input space is high, the nearest neighbors need not be close to the target point, and can result in large errors; â¢ if special structure is known to exist, this can be used to reduce both the bias and the variance of the estimates. We anticipate using other classes of models for f (x), in many cases specifically designed to overcome the dimensionality problems, and here we discuss a framework for incorporating them into the prediction problem.

2.6.1 A Statistical Model for the Joint Distribution Pr(X, Y )
Suppose in fact that our data arose from a statistical model Y = f (X) + Îµ, (2.29)

where the random error Îµ has E(Îµ) = 0 and is independent of X. Note that for this model, f (x) = E(Y |X = x), and in fact the conditional distribution Pr(Y |X) depends on X only through the conditional mean f (x). The additive error model is a useful approximation to the truth. For most systems the inputâoutput pairs (X, Y ) will not have a deterministic relationship Y = f (X). Generally there will be other unmeasured variables that also contribute to Y , including measurement error. The additive model assumes that we can capture all these departures from a deterministic relationship via the error Îµ. For some problems a deterministic relationship does hold. Many of the classiï¬cation problems studied in machine learning are of this form, where the response surface can be thought of as a colored map deï¬ned in IRp . The training data consist of colored examples from the map {xi , gi }, and the goal is to be able to color any point. Here the function is deterministic, and the randomness enters through the x location of the training points. For the moment we will not pursue such problems, but will see that they can be handled by techniques appropriate for the error-based models. The assumption in (2.29) that the errors are independent and identically distributed is not strictly necessary, but seems to be at the back of our mind

2.6 Statistical Models, Supervised Learning and Function Approximation

29

when we average squared errors uniformly in our EPE criterion. With such a model it becomes natural to use least squares as a data criterion for model estimation as in (2.1). Simple modiï¬cations can be made to avoid the independence assumption; for example, we can have Var(Y |X = x) = Ï(x), and now both the mean and variance depend on X. In general the conditional distribution Pr(Y |X) can depend on X in complicated ways, but the additive error model precludes these. So far we have concentrated on the quantitative response. Additive error models are typically not used for qualitative outputs G; in this case the target function p(X) is the conditional density Pr(G|X), and this is modeled directly. For example, for two-class data, it is often reasonable to assume that the data arise from independent binary trials, with the probability of one particular outcome being p(X), and the other 1 â p(X). Thus if Y is the 0â1 coded version of G, then E(Y |X = x) = p(x), but the variance depends on x as well: Var(Y |X = x) = p(x)[1 â p(x)].

2.6.2 Supervised Learning
Before we launch into more statistically oriented jargon, we present the function-ï¬tting paradigm from a machine learning point of view. Suppose for simplicity that the errors are additive and that the model Y = f (X) + Îµ is a reasonable assumption. Supervised learning attempts to learn f by example through a teacher. One observes the system under study, both the inputs and outputs, and assembles a training set of observations T = (xi , yi ), i = 1, . . . , N . The observed input values to the system xi are also fed into an artiï¬cial system, known as a learning algorithm (usually a comË puter program), which also produces outputs f (xi ) in response to the inputs. The learning algorithm has the property that it can modify its inË Ë put/output relationship f in response to diï¬erences yi â f (xi ) between the original and generated outputs. This process is known as learning by example. Upon completion of the learning process the hope is that the artiï¬cial and real outputs will be close enough to be useful for all sets of inputs likely to be encountered in practice.

2.6.3 Function Approximation
The learning paradigm of the previous section has been the motivation for research into the supervised learning problem in the ï¬elds of machine learning (with analogies to human reasoning) and neural networks (with biological analogies to the brain). The approach taken in applied mathematics and statistics has been from the perspective of function approximation and estimation. Here the data pairs {xi , yi } are viewed as points in a (p + 1)-dimensional Euclidean space. The function f (x) has domain equal to the p-dimensional input subspace, and is related to the data via a model

30

2. Overview of Supervised Learning

such as yi = f (xi ) + Îµi . For convenience in this chapter we will assume the domain is IRp , a p-dimensional Euclidean space, although in general the inputs can be of mixed type. The goal is to obtain a useful approximation to f (x) for all x in some region of IRp , given the representations in T . Although somewhat less glamorous than the learning paradigm, treating supervised learning as a problem in function approximation encourages the geometrical concepts of Euclidean spaces and mathematical concepts of probabilistic inference to be applied to the problem. This is the approach taken in this book. Many of the approximations we will encounter have associated a set of parameters Î¸ that can be modiï¬ed to suit the data at hand. For example, the linear model f (x) = xT Î² has Î¸ = Î². Another class of useful approximators can be expressed as linear basis expansions
K

fÎ¸ (x) =
k=1

hk (x)Î¸k ,

(2.30)

where the hk are a suitable set of functions or transformations of the input vector x. Traditional examples are polynomial and trigonometric expansions, where for example hk might be x2 , x1 x2 , cos(x1 ) and so on. We 2 1 also encounter nonlinear expansions, such as the sigmoid transformation common to neural network models, hk (x) = 1 . 1 + exp(âxT Î²k ) (2.31)

We can use least squares to estimate the parameters Î¸ in fÎ¸ as we did for the linear model, by minimizing the residual sum-of-squares
N

RSS(Î¸) =
i=1

(yi â fÎ¸ (xi ))2

(2.32)

as a function of Î¸. This seems a reasonable criterion for an additive error model. In terms of function approximation, we imagine our parameterized function as a surface in p + 1 space, and what we observe are noisy realizations from it. This is easy to visualize when p = 2 and the vertical coordinate is the output y, as in Figure 2.10. The noise is in the output coordinate, so we ï¬nd the set of parameters such that the ï¬tted surface gets as close to the observed points as possible, where close is measured by the sum of squared vertical errors in RSS(Î¸). For the linear model we get a simple closed form solution to the minimization problem. This is also true for the basis function methods, if the basis functions themselves do not have any hidden parameters. Otherwise the solution requires either iterative methods or numerical optimization. While least squares is generally very convenient, it is not the only criterion used and in some cases would not make much sense. A more general

2.6 Statistical Models, Supervised Learning and Function Approximation

31

â¢

â¢

â¢

â¢ â¢ â¢â¢ â¢ â¢â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢
FIGURE 2.10. Least squares ï¬tting of a function of two inputs. The parameters of fÎ¸ (x) are chosen so as to minimize the sum-of-squared vertical errors.

principle for estimation is maximum likelihood estimation. Suppose we have a random sample yi , i = 1, . . . , N from a density PrÎ¸ (y) indexed by some parameters Î¸. The log-probability of the observed sample is
N

L(Î¸) =
i=1

log PrÎ¸ (yi ).

(2.33)

The principle of maximum likelihood assumes that the most reasonable values for Î¸ are those for which the probability of the observed sample is largest. Least squares for the additive error model Y = fÎ¸ (X) + Îµ, with Îµ â¼ N (0, Ï 2 ), is equivalent to maximum likelihood using the conditional likelihood Pr(Y |X, Î¸) = N (fÎ¸ (X), Ï 2 ). (2.34) So although the additional assumption of normality seems more restrictive, the results are the same. The log-likelihood of the data is L(Î¸) = â 1 N log(2Ï) â N log Ï â 2 2 2Ï
N i=1

(yi â fÎ¸ (xi ))2 ,

(2.35)

and the only term involving Î¸ is the last, which is RSS(Î¸) up to a scalar negative multiplier. A more interesting example is the multinomial likelihood for the regression function Pr(G|X) for a qualitative output G. Suppose we have a model Pr(G = Gk |X = x) = pk,Î¸ (x), k = 1, . . . , K for the conditional probability of each class given X, indexed by the parameter vector Î¸. Then the

32

2. Overview of Supervised Learning

log-likelihood (also referred to as the cross-entropy) is
N

L(Î¸) =
i=1

log pgi ,Î¸ (xi ),

(2.36)

and when maximized it delivers values of Î¸ that best conform with the data in this likelihood sense.

2.7 Structured Regression Models
We have seen that although nearest-neighbor and other local methods focus directly on estimating the function at a point, they face problems in high dimensions. They may also be inappropriate even in low dimensions in cases where more structured approaches can make more eï¬cient use of the data. This section introduces classes of such structured approaches. Before we proceed, though, we discuss further the need for such classes.

2.7.1 Diï¬culty of the Problem
Consider the RSS criterion for an arbitrary function f ,
N

RSS(f ) =
i=1

(yi â f (xi ))2 .

(2.37)

Ë Minimizing (2.37) leads to inï¬nitely many solutions: any function f passing through the training points (xi , yi ) is a solution. Any particular solution chosen might be a poor predictor at test points diï¬erent from the training points. If there are multiple observation pairs xi , yiâ , â = 1, . . . , Ni at each value of xi , the risk is limited. In this case, the solutions pass through the average values of the yiâ at each xi ; see Exercise 2.6. The situation is similar to the one we have already visited in Section 2.4; indeed, (2.37) is the ï¬nite sample version of (2.11) on page 18. If the sample size N were suï¬ciently large such that repeats were guaranteed and densely arranged, it would seem that these solutions might all tend to the limiting conditional expectation. In order to obtain useful results for ï¬nite N , we must restrict the eligible solutions to (2.37) to a smaller set of functions. How to decide on the nature of the restrictions is based on considerations outside of the data. These restrictions are sometimes encoded via the parametric representation of fÎ¸ , or may be built into the learning method itself, either implicitly or explicitly. These restricted classes of solutions are the major topic of this book. One thing should be clear, though. Any restrictions imposed on f that lead to a unique solution to (2.37) do not really remove the ambiguity

2.8 Classes of Restricted Estimators

33

caused by the multiplicity of solutions. There are inï¬nitely many possible restrictions, each leading to a unique solution, so the ambiguity has simply been transferred to the choice of constraint. In general the constraints imposed by most learning methods can be described as complexity restrictions of one kind or another. This usually means some kind of regular behavior in small neighborhoods of the input space. That is, for all input points x suï¬ciently close to each other in Ë some metric, f exhibits some special structure such as nearly constant, linear or low-order polynomial behavior. The estimator is then obtained by averaging or polynomial ï¬tting in that neighborhood. The strength of the constraint is dictated by the neighborhood size. The larger the size of the neighborhood, the stronger the constraint, and the more sensitive the solution is to the particular choice of constraint. For example, local constant ï¬ts in inï¬nitesimally small neighborhoods is no constraint at all; local linear ï¬ts in very large neighborhoods is almost a globally linear model, and is very restrictive. The nature of the constraint depends on the metric used. Some methods, such as kernel and local regression and tree-based methods, directly specify the metric and size of the neighborhood. The nearest-neighbor methods discussed so far are based on the assumption that locally the function is constant; close to a target input x0 , the function does not change much, and Ë so close outputs can be averaged to produce f (x0 ). Other methods such as splines, neural networks and basis-function methods implicitly deï¬ne neighborhoods of local behavior. In Section 5.4.1 we discuss the concept of an equivalent kernel (see Figure 5.8 on page 157), which describes this local dependence for any method linear in the outputs. These equivalent kernels in many cases look just like the explicitly deï¬ned weighting kernels discussed aboveâpeaked at the target point and falling away smoothly away from it. One fact should be clear by now. Any method that attempts to produce locally varying functions in small isotropic neighborhoods will run into problems in high dimensionsâagain the curse of dimensionality. And conversely, all methods that overcome the dimensionality problems have an associatedâand often implicit or adaptiveâmetric for measuring neighborhoods, which basically does not allow the neighborhood to be simultaneously small in all directions.

2.8

Classes of Restricted Estimators

The variety of nonparametric regression techniques or learning methods fall into a number of diï¬erent classes depending on the nature of the restrictions imposed. These classes are not distinct, and indeed some methods fall in several classes. Here we give a brief summary, since detailed descriptions

34

2. Overview of Supervised Learning

are given in later chapters. Each of the classes has associated with it one or more parameters, sometimes appropriately called smoothing parameters, that control the eï¬ective size of the local neighborhood. Here we describe three broad classes.

2.8.1 Roughness Penalty and Bayesian Methods
Here the class of functions is controlled by explicitly penalizing RSS(f ) with a roughness penalty PRSS(f ; Î») = RSS(f ) + Î»J(f ). (2.38)

The user-selected functional J(f ) will be large for functions f that vary too rapidly over small regions of input space. For example, the popular cubic smoothing spline for one-dimensional inputs is the solution to the penalized least-squares criterion
N

PRSS(f ; Î») =
i=1

(yi â f (xi ))2 + Î»

[f â²â² (x)]2 dx.

(2.39)

The roughness penalty here controls large values of the second derivative of f , and the amount of penalty is dictated by Î» â¥ 0. For Î» = 0 no penalty is imposed, and any interpolating function will do, while for Î» = â only functions linear in x are permitted. Penalty functionals J can be constructed for functions in any dimension, and special versions can be created to impose special structure. For exp ample, additive penalties J(f ) = j=1 J(fj ) are used in conjunction with p additive functions f (X) = j=1 fj (Xj ) to create additive models with smooth coordinate functions. Similarly, projection pursuit regression modM T els have f (X) = m=1 gm (Î±m X) for adaptively chosen directions Î±m , and the functions gm can each have an associated roughness penalty. Penalty function, or regularization methods, express our prior belief that the type of functions we seek exhibit a certain type of smooth behavior, and indeed can usually be cast in a Bayesian framework. The penalty J corresponds to a log-prior, and PRSS(f ; Î») the log-posterior distribution, and minimizing PRSS(f ; Î») amounts to ï¬nding the posterior mode. We discuss roughness-penalty approaches in Chapter 5 and the Bayesian paradigm in Chapter 8.

2.8.2 Kernel Methods and Local Regression
These methods can be thought of as explicitly providing estimates of the regression function or conditional expectation by specifying the nature of the local neighborhood, and of the class of regular functions ï¬tted locally. The local neighborhood is speciï¬ed by a kernel function KÎ» (x0 , x) which assigns

2.8 Classes of Restricted Estimators

35

weights to points x in a region around x0 (see Figure 6.1 on page 192). For example, the Gaussian kernel has a weight function based on the Gaussian density function KÎ» (x0 , x) = ||x â x0 ||2 1 exp â Î» 2Î» (2.40)

and assigns weights to points that die exponentially with their squared Euclidean distance from x0 . The parameter Î» corresponds to the variance of the Gaussian density, and controls the width of the neighborhood. The simplest form of kernel estimate is the NadarayaâWatson weighted average Ë f (x0 ) =
N i=1 KÎ» (x0 , xi )yi . N i=1 KÎ» (x0 , xi )

(2.41)

In general we can deï¬ne a local regression estimate of f (x0 ) as fÎ¸ (x0 ), Ë Ë where Î¸ minimizes
N

RSS(fÎ¸ , x0 ) =
i=1

KÎ» (x0 , xi )(yi â fÎ¸ (xi ))2 ,

(2.42)

and fÎ¸ is some parameterized function, such as a low-order polynomial. Some examples are: â¢ fÎ¸ (x) = Î¸0 , the constant function; this results in the Nadarayaâ Watson estimate in (2.41) above. â¢ fÎ¸ (x) = Î¸0 + Î¸1 x gives the popular local linear regression model. Nearest-neighbor methods can be thought of as kernel methods having a more data-dependent metric. Indeed, the metric for k-nearest neighbors is Kk (x, x0 ) = I(||x â x0 || â¤ ||x(k) â x0 ||), where x(k) is the training observation ranked kth in distance from x0 , and I(S) is the indicator of the set S. These methods of course need to be modiï¬ed in high dimensions, to avoid the curse of dimensionality. Various adaptations are discussed in Chapter 6.

2.8.3 Basis Functions and Dictionary Methods
This class of methods includes the familiar linear and polynomial expansions, but more importantly a wide variety of more ï¬exible models. The model for f is a linear expansion of basis functions
M

fÎ¸ (x) =
m=1

Î¸m hm (x),

(2.43)

36

2. Overview of Supervised Learning

where each of the hm is a function of the input x, and the term linear here refers to the action of the parameters Î¸. This class covers a wide variety of methods. In some cases the sequence of basis functions is prescribed, such as a basis for polynomials in x of total degree M . For one-dimensional x, polynomial splines of degree K can be represented by an appropriate sequence of M spline basis functions, determined in turn by M â K knots. These produce functions that are piecewise polynomials of degree K between the knots, and joined up with continuity of degree K â 1 at the knots. As an example consider linear splines, or piecewise linear functions. One intuitively satisfying basis consists of the functions b1 (x) = 1, b2 (x) = x, and bm+2 (x) = (x â tm )+ , m = 1, . . . , M â 2, where tm is the mth knot, and z+ denotes positive part. Tensor products of spline bases can be used for inputs with dimensions larger than one (see Section 5.2, and the CART and MARS models in Chapter 9.) The parameter Î¸ can be the total degree of the polynomial or the number of knots in the case of splines. Radial basis functions are symmetric p-dimensional kernels located at particular centroids,
M

fÎ¸ (x) =
m=1

KÎ»m (Âµm , x)Î¸m ;
2

(2.44)

for example, the Gaussian kernel KÎ» (Âµ, x) = eâ||xâÂµ|| /2Î» is popular. Radial basis functions have centroids Âµm and scales Î»m that have to be determined. The spline basis functions have knots. In general we would like the data to dictate them as well. Including these as parameters changes the regression problem from a straightforward linear problem to a combinatorially hard nonlinear problem. In practice, shortcuts such as greedy algorithms or two stage processes are used. Section 6.7 describes some such approaches. A single-layer feed-forward neural network model with linear output weights can be thought of as an adaptive basis function method. The model has the form
M

fÎ¸ (x) =
m=1

T Î²m Ï(Î±m x + bm ),

(2.45)

where Ï(x) = 1/(1 + eâx ) is known as the activation function. Here, as in the projection pursuit model, the directions Î±m and the bias terms bm have to be determined, and their estimation is the meat of the computation. Details are give in Chapter 11. These adaptively chosen basis function methods are also known as dictionary methods, where one has available a possibly inï¬nite set or dictionary D of candidate basis functions from which to choose, and models are built up by employing some kind of search mechanism.

2.9 Model Selection and the BiasâVariance Tradeoï¬

37

2.9 Model Selection and the BiasâVariance Tradeoï¬
All the models described above and many others discussed in later chapters have a smoothing or complexity parameter that has to be determined: â¢ the multiplier of the penalty term; â¢ the width of the kernel; â¢ or the number of basis functions. In the case of the smoothing spline, the parameter Î» indexes models ranging from a straight line ï¬t to the interpolating model. Similarly a local degreem polynomial model ranges between a degree-m global polynomial when the window size is inï¬nitely large, to an interpolating ï¬t when the window size shrinks to zero. This means that we cannot use residual sum-of-squares on the training data to determine these parameters as well, since we would always pick those that gave interpolating ï¬ts and hence zero residuals. Such a model is unlikely to predict future data well at all. Ë The k-nearest-neighbor regression ï¬t fk (x0 ) usefully illustrates the competing forces that eï¬ect the predictive ability of such approximations. Suppose the data arise from a model Y = f (X) + Îµ, with E(Îµ) = 0 and Var(Îµ) = Ï 2 . For simplicity here we assume that the values of xi in the sample are ï¬xed in advance (nonrandom). The expected prediction error at x0 , also known as test or generalization error, can be decomposed: EPEk (x0 ) = Ë E[(Y â fk (x0 ))2 |X = x0 ] 2 Ë Ë = Ï + [Bias2 (fk (x0 )) + VarT (fk (x0 ))] = Ï 2 + f (x0 ) â 1 k
k

(2.46) (2.47)

f (x(â) )
â=1

2

+

Ï2 . k

The subscripts in parentheses (â) indicate the sequence of nearest neighbors to x0 . There are three terms in this expression. The ï¬rst term Ï 2 is the irreducible errorâthe variance of the new test targetâand is beyond our control, even if we know the true f (x0 ). The second and third terms are under our control, and make up the Ë mean squared error of fk (x0 ) in estimating f (x0 ), which is broken down into a bias component and a variance component. The bias term is the squared diï¬erence between the true mean f (x0 ) and the expected value of Ë the estimateâ[ET (fk (x0 )) â f (x0 )]2 âwhere the expectation averages the randomness in the training data. This term will most likely increase with k, if the true function is reasonably smooth. For small k the few closest neighbors will have values f (x(â) ) close to f (x0 ), so their average should

38

2. Overview of Supervised Learning
High Bias Low Variance Low Bias High Variance

Prediction Error

Test Sample

Training Sample

Low

High

Model Complexity FIGURE 2.11. Test and training error as a function of model complexity.

be close to f (x0 ). As k grows, the neighbors are further away, and then anything can happen. The variance term is simply the variance of an average here, and decreases as the inverse of k. So as k varies, there is a biasâvariance tradeoï¬. More generally, as the model complexity of our procedure is increased, the variance tends to increase and the squared bias tends to decreases. The opposite behavior occurs as the model complexity is decreased. For k-nearest neighbors, the model complexity is controlled by k. Typically we would like to choose our model complexity to trade bias oï¬ with variance in such a way as to minimize the test error. An obvious 1 Ë estimate of test error is the training error N i (yi â yi )2 . Unfortunately training error is not a good estimate of test error, as it does not properly account for model complexity. Figure 2.11 shows the typical behavior of the test and training error, as model complexity is varied. The training error tends to decrease whenever we increase the model complexity, that is, whenever we ï¬t the data harder. However with too much ï¬tting, the model adapts itself too closely to the training data, and will not generalize well (i.e., have large test error). In Ë that case the predictions f (x0 ) will have large variance, as reï¬ected in the last term of expression (2.46). In contrast, if the model is not complex enough, it will underï¬t and may have large bias, again resulting in poor generalization. In Chapter 7 we discuss methods for estimating the test error of a prediction method, and hence estimating the optimal amount of model complexity for a given prediction method and training set.

Exercises

39

Bibliographic Notes
Some good general books on the learning problem are Duda et al. (2000), Bishop (1995),(Bishop, 2006), Ripley (1996), Cherkassky and Mulier (2007) and Vapnik (1996). Parts of this chapter are based on Friedman (1994b).

Exercises
Ex. 2.1 Suppose each of K-classes has an associated target tk , which is a vector of all zeros, except a one in the kth position. Show that classifying to the largest element of y amounts to choosing the closest target, mink ||tk â Ë y ||, if the elements of y sum to one. Ë Ë Ex. 2.2 Show how to compute the Bayes decision boundary for the simulation example in Figure 2.5. Ex. 2.3 Derive equation (2.24). Ex. 2.4 The edge eï¬ect problem discussed on page 23 is not peculiar to uniform sampling from bounded domains. Consider inputs drawn from a spherical multinormal distribution X â¼ N (0, Ip ). The squared distance from any sample point to the origin has a Ï2 distribution with mean p. p Consider a prediction point x0 drawn from this distribution, and let a = x0 /||x0 || be an associated unit vector. Let zi = aT xi be the projection of each of the training points on this direction. Show that the zi are distributed N (0, 1) with expected squared distance from the origin 1, while the target point has expected squared distance p from the origin. Hence for p = 10, a randomly drawn test point is about 3.1 standard deviations from the origin, while all the training points are on average one standard deviation along direction a. So most prediction points see themselves as lying on the edge of the training set. Ex. 2.5 (a) Derive equation (2.27). The last line makes use of (3.8) through a conditioning argument. (b) Derive equation (2.28), making use of the cyclic property of the trace operator [trace(AB) = trace(BA)], and its linearity (which allows us to interchange the order of trace and expectation). Ex. 2.6 Consider a regression problem with inputs xi and outputs yi , and a parameterized model fÎ¸ (x) to be ï¬t by least squares. Show that if there are observations with tied or identical values of x, then the ï¬t can be obtained from a reduced weighted least squares problem.

40

2. Overview of Supervised Learning

Ex. 2.7 Suppose we have a sample of N pairs xi , yi drawn i.i.d. from the distribution characterized as follows: xi â¼ h(x), the design density yi = f (xi ) + Îµi , f is the regression function Îµi â¼ (0, Ï 2 ) (mean zero, variance Ï 2 ) We construct an estimator for f linear in the yi ,
N

Ë f (x0 ) =
i=1

âi (x0 ; X )yi ,

where the weights âi (x0 ; X ) do not depend on the yi , but do depend on the entire training sequence of xi , denoted here by X . (a) Show that linear regression and k-nearest-neighbor regression are members of this class of estimators. Describe explicitly the weights âi (x0 ; X ) in each of these cases. (b) Decompose the conditional mean-squared error Ë EY|X (f (x0 ) â f (x0 ))2 into a conditional squared bias and a conditional variance component. Like X , Y represents the entire training sequence of yi . (c) Decompose the (unconditional) mean-squared error Ë EY,X (f (x0 ) â f (x0 ))2 into a squared bias and a variance component. (d) Establish a relationship between the squared biases and variances in the above two cases. Ex. 2.8 Compare the classiï¬cation performance of linear regression and kâ nearest neighbor classiï¬cation on the zipcode data. In particular, consider only the 2âs and 3âs, and k = 1, 3, 5, 7 and 15. Show both the training and test error for each choice. The zipcode data are available from the book website www-stat.stanford.edu/ElemStatLearn. Ex. 2.9 Consider a linear regression model with p parameters, ï¬t by least squares to a set of training data (x1 , y1 ), . . . , (xN , yN ) drawn at random Ë from a population. Let Î² be the least squares estimate. Suppose we have some test data (Ë1 , y1 ), . . . , (ËM , yM ) drawn at random from the same popx Ë x Ë N 1 ulation as the training data. If Rtr (Î²) = N 1 (yi â Î² T xi )2 and Rte (Î²) = M 1 T y Ë 2 1 (Ëi â Î² xi ) , prove that M Ë Ë E[Rtr (Î²)] â¤ E[Rte (Î²)],

Exercises

41

where the expectations are over all that is random in each expression. [This exercise was brought to our attention by Ryan Tibshirani, from a homework assignment given by Andrew Ng.]

42

2. Overview of Supervised Learning

This is page 43 Printer: Opaque this

3
Linear Methods for Regression

3.1 Introduction
A linear regression model assumes that the regression function E(Y |X) is linear in the inputs X1 , . . . , Xp . Linear models were largely developed in the precomputer age of statistics, but even in todayâs computer era there are still good reasons to study and use them. They are simple and often provide an adequate and interpretable description of how the inputs aï¬ect the output. For prediction purposes they can sometimes outperform fancier nonlinear models, especially in situations with small numbers of training cases, low signal-to-noise ratio or sparse data. Finally, linear methods can be applied to transformations of the inputs and this considerably expands their scope. These generalizations are sometimes called basis-function methods, and are discussed in Chapter 5. In this chapter we describe linear methods for regression, while in the next chapter we discuss linear methods for classiï¬cation. On some topics we go into considerable detail, as it is our ï¬rm belief that an understanding of linear methods is essential for understanding nonlinear ones. In fact, many nonlinear techniques are direct generalizations of the linear methods discussed here.

44

3. Linear Methods for Regression

3.2 Linear Regression Models and Least Squares
As introduced in Chapter 2, we have an input vector X T = (X1 , X2 , . . . , Xp ), and want to predict a real-valued output Y . The linear regression model has the form
p

f (X) = Î²0 +
j=1

Xj Î²j .

(3.1)

The linear model either assumes that the regression function E(Y |X) is linear, or that the linear model is a reasonable approximation. Here the Î²j âs are unknown parameters or coeï¬cients, and the variables Xj can come from diï¬erent sources: â¢ quantitative inputs; â¢ transformations of quantitative inputs, such as log, square-root or square;
2 3 â¢ basis expansions, such as X2 = X1 , X3 = X1 , leading to a polynomial representation;

â¢ numeric or âdummyâ coding of the levels of qualitative inputs. For example, if G is a ï¬ve-level factor input, we might create Xj , j = 1, . . . , 5, such that Xj = I(G = j). Together this group of Xj represents the eï¬ect of G by a set of level-dependent constants, since in 5 j=1 Xj Î²j , one of the Xj s is one, and the others are zero. â¢ interactions between variables, for example, X3 = X1 Â· X2 . No matter the source of the Xj , the model is linear in the parameters. Typically we have a set of training data (x1 , y1 ) . . . (xN , yN ) from which to estimate the parameters Î². Each xi = (xi1 , xi2 , . . . , xip )T is a vector of feature measurements for the ith case. The most popular estimation method is least squares, in which we pick the coeï¬cients Î² = (Î²0 , Î²1 , . . . , Î²p )T to minimize the residual sum of squares
N

RSS(Î²)

=
i=1 N

(yi â f (xi ))2
p

=
i=1

yi â Î²0 â

xij Î²j
j=1

2

.

(3.2)

From a statistical point of view, this criterion is reasonable if the training observations (xi , yi ) represent independent random draws from their population. Even if the xi âs were not drawn randomly, the criterion is still valid if the yi âs are conditionally independent given the inputs xi . Figure 3.1 illustrates the geometry of least-squares ï¬tting in the IRp+1 -dimensional

3.2 Linear Regression Models and Least Squares Y

45

â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ X2 â¢ â¢ â¢ â¢ â¢
X1 FIGURE 3.1. Linear least squares ï¬tting with X â IR2 . We seek the linear function of X that minimizes the sum of squared residuals from Y .

â¢

â¢

space occupied by the pairs (X, Y ). Note that (3.2) makes no assumptions about the validity of model (3.1); it simply ï¬nds the best linear ï¬t to the data. Least squares ï¬tting is intuitively satisfying no matter how the data arise; the criterion measures the average lack of ï¬t. How do we minimize (3.2)? Denote by X the N Ã (p + 1) matrix with each row an input vector (with a 1 in the ï¬rst position), and similarly let y be the N -vector of outputs in the training set. Then we can write the residual sum-of-squares as RSS(Î²) = (y â XÎ²)T (y â XÎ²). (3.3)

This is a quadratic function in the p + 1 parameters. Diï¬erentiating with respect to Î² we obtain âRSS = â2XT (y â XÎ²) âÎ² â 2 RSS = 2XT X. âÎ²âÎ² T

(3.4)

Assuming (for the moment) that X has full column rank, and hence XT X is positive deï¬nite, we set the ï¬rst derivative to zero XT (y â XÎ²) = 0 to obtain the unique solution Ë Î² = (XT X)â1 XT y. (3.6) (3.5)

46

3. Linear Methods for Regression

y

x2

y Ë x1
FIGURE 3.2. The N -dimensional geometry of least squares regression with two predictors. The outcome vector y is orthogonally projected onto the hyperplane Ë spanned by the input vectors x1 and x2 . The projection y represents the vector of the least squares predictions

Ë Ë The predicted values at an input vector x0 are given by f (x0 ) = (1 : x0 )T Î²; the ï¬tted values at the training inputs are Ë Ë y = XÎ² = X(XT X)â1 XT y, (3.7)

Ë where yi = f (xi ). The matrix H = X(XT X)â1 XT appearing in equation Ë (3.7) is sometimes called the âhatâ matrix because it puts the hat on y. Figure 3.2 shows a diï¬erent geometrical representation of the least squares estimate, this time in IRN . We denote the column vectors of X by x0 , x1 , . . . , xp , with x0 â¡ 1. For much of what follows, this ï¬rst column is treated like any other. These vectors span a subspace of IRN , also referred to as the column Ë space of X. We minimize RSS(Î²) = y â XÎ² 2 by choosing Î² so that the Ë residual vector y â y is orthogonal to this subspace. This orthogonality is Ë expressed in (3.5), and the resulting estimate y is hence the orthogonal projection of y onto this subspace. The hat matrix H computes the orthogonal projection, and hence it is also known as a projection matrix. It might happen that the columns of X are not linearly independent, so that X is not of full rank. This would occur, for example, if two of the inputs were perfectly correlated, (e.g., x2 = 3x1 ). Then XT X is singular Ë and the least squares coeï¬cients Î² are not uniquely deï¬ned. However, Ë Ë = XÎ² are still the projection of y onto the column the ï¬tted values y space of X; there is just more than one way to express that projection in terms of the column vectors of X. The non-full-rank case occurs most often when one or more qualitative inputs are coded in a redundant fashion. There is usually a natural way to resolve the non-unique representation, by recoding and/or dropping redundant columns in X. Most regression software packages detect these redundancies and automatically implement

3.2 Linear Regression Models and Least Squares

47

some strategy for removing them. Rank deï¬ciencies can also occur in signal and image analysis, where the number of inputs p can exceed the number of training cases N . In this case, the features are typically reduced by ï¬ltering or else the ï¬tting is controlled by regularization (Section 5.2.3 and Chapter 18). Up to now we have made minimal assumptions about the true distribuË tion of the data. In order to pin down the sampling properties of Î², we now assume that the observations yi are uncorrelated and have constant variance Ï 2 , and that the xi are ï¬xed (non random). The varianceâcovariance matrix of the least squares parameter estimates is easily derived from (3.6) and is given by Ë Var(Î²) = (XT X)â1 Ï 2 . Typically one estimates the variance Ï 2 by 1 Ï = Ë N âpâ1
2 N i=1

(3.8)

(yi â yi )2 . Ë

The N â p â 1 rather than N in the denominator makes Ï 2 an unbiased Ë estimate of Ï 2 : E(Ë 2 ) = Ï 2 . Ï To draw inferences about the parameters and the model, additional assumptions are needed. We now assume that (3.1) is the correct model for the mean; that is, the conditional expectation of Y is linear in X1 , . . . , Xp . We also assume that the deviations of Y around its expectation are additive and Gaussian. Hence Y = E(Y |X1 , . . . , Xp ) + Îµ
p

= Î²0 +
j=1

Xj Î²j + Îµ,

(3.9)

where the error Îµ is a Gaussian random variable with expectation zero and variance Ï 2 , written Îµ â¼ N (0, Ï 2 ). Under (3.9), it is easy to show that Ë Î² â¼ N (Î², (XT X)â1 Ï 2 ). (3.10)

This is a multivariate normal distribution with mean vector and varianceâ covariance matrix as shown. Also (N â p â 1)Ë 2 â¼ Ï 2 Ï2 âpâ1 , Ï N (3.11)

Ë a chi-squared distribution with N â p â 1 degrees of freedom. In addition Î² and Ï 2 are statistically independent. We use these distributional properties Ë to form tests of hypothesis and conï¬dence intervals for the parameters Î²j .

48

3. Linear Methods for Regression

0.01 0.02 0.03 0.04 0.05 0.06

Tail Probabilities

t30 t100 normal

2.0

2.2

2.4 Z

2.6

2.8

3.0

FIGURE 3.3. The tail probabilities Pr(|Z| > z) for three distributions, t30 , t100 and standard normal. Shown are the appropriate quantiles for testing signiï¬cance at the p = 0.05 and 0.01 levels. The diï¬erence between t and the standard normal becomes negligible for N bigger than about 100.

To test the hypothesis that a particular coeï¬cient Î²j = 0, we form the standardized coeï¬cient or Z-score zj = Ë Î²j â , Ï vj Ë (3.12)

where vj is the jth diagonal element of (XT X)â1 . Under the null hypothesis that Î²j = 0, zj is distributed as tN âpâ1 (a t distribution with N â p â 1 degrees of freedom), and hence a large (absolute) value of zj will lead to rejection of this null hypothesis. If Ï is replaced by a known value Ï, then Ë zj would have a standard normal distribution. The diï¬erence between the tail quantiles of a t-distribution and a standard normal become negligible as the sample size increases, and so we typically use the normal quantiles (see Figure 3.3). Often we need to test for the signiï¬cance of groups of coeï¬cients simultaneously. For example, to test if a categorical variable with k levels can be excluded from a model, we need to test whether the coeï¬cients of the dummy variables used to represent the levels can all be set to zero. Here we use the F statistic, F = (RSS0 â RSS1 )/(p1 â p0 ) , RSS1 /(N â p1 â 1) (3.13)

where RSS1 is the residual sum-of-squares for the least squares ï¬t of the bigger model with p1 +1 parameters, and RSS0 the same for the nested smaller model with p0 + 1 parameters, having p1 â p0 parameters constrained to be

3.2 Linear Regression Models and Least Squares

49

zero. The F statistic measures the change in residual sum-of-squares per additional parameter in the bigger model, and it is normalized by an estimate of Ï 2 . Under the Gaussian assumptions, and the null hypothesis that the smaller model is correct, the F statistic will have a Fp1 âp0 ,N âp1 â1 distribution. It can be shown (Exercise 3.1) that the zj in (3.12) are equivalent to the F statistic for dropping the single coeï¬cient Î²j from the model. For large N , the quantiles of the Fp1 âp0 ,N âp1 â1 approach those of the Ï21 âp0 . p Similarly, we can isolate Î²j in (3.10) to obtain a 1â2Î± conï¬dence interval for Î²j : Ë Ë Ë Ë (Î²j â z (1âÎ±) vj2 Ï , Î²j + z (1âÎ±) vj2 Ï ). Here z (1âÎ±) is the 1 â Î± percentile of the normal distribution: z (1â0.025) z (1â.05) = = 1.96, 1.645, etc.
1 1

(3.14)

Ë Ë Hence the standard practice of reporting Î² Â± 2 Â· se(Î²) amounts to an approximate 95% conï¬dence interval. Even if the Gaussian error assumption does not hold, this interval will be approximately correct, with its coverage approaching 1 â 2Î± as the sample size N â â. In a similar fashion we can obtain an approximate conï¬dence set for the entire parameter vector Î², namely Ë Ë CÎ² = {Î²|(Î² â Î²)T XT X(Î² â Î²) â¤ Ï 2 Ï2 Ë p+1
(1âÎ±) (1âÎ±)

},

(3.15)

is the 1 â Î± percentile of the chi-squared distribution on â where Ï2 â (1â0.05) (1â0.1) degrees of freedom: for example, Ï2 = 11.1, Ï2 = 9.2. This 5 5 conï¬dence set for Î² generates a corresponding conï¬dence set for the true function f (x) = xT Î², namely {xT Î²|Î² â CÎ² } (Exercise 3.2; see also Figure 5.4 in Section 5.2.2 for examples of conï¬dence bands for functions).

3.2.1 Example: Prostate Cancer
The data for this example come from a study by Stamey et al. (1989). They examined the correlation between the level of prostate-speciï¬c antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. The variables are log cancer volume (lcavol), log prostate weight (lweight), age, log of the amount of benign prostatic hyperplasia (lbph), seminal vesicle invasion (svi), log of capsular penetration (lcp), Gleason score (gleason), and percent of Gleason scores 4 or 5 (pgg45). The correlation matrix of the predictors given in Table 3.1 shows many strong correlations. Figure 1.1 (page 3) of Chapter 1 is a scatterplot matrix showing every pairwise plot between the variables. We see that svi is a binary variable, and gleason is an ordered categorical variable. We see, for

50

3. Linear Methods for Regression TABLE 3.1. Correlations of predictors in the prostate cancer data. lcavol lweight age lbph svi lcp gleason

lweight age lbph svi lcp gleason pgg45

0.300 0.286 0.063 0.593 0.692 0.426 0.483

0.317 0.437 0.181 0.157 0.024 0.074

0.287 0.129 0.173 0.366 0.276

â0.139 â0.089 0.033 â0.030

0.671 0.307 0.481

0.476 0.663

0.757

TABLE 3.2. Linear model ï¬t to the prostate cancer data. The Z score is the coeï¬cient divided by its standard error (3.12). Roughly a Z score larger than two in absolute value is signiï¬cantly nonzero at the p = 0.05 level.

Term
Intercept lcavol lweight age lbph svi lcp gleason pgg45

Coeï¬cient 2.46 0.68 0.26 â0.14 0.21 0.31 â0.29 â0.02 0.27

Std. Error 0.09 0.13 0.10 0.10 0.10 0.12 0.15 0.15 0.15

Z Score 27.60 5.37 2.75 â1.40 2.06 2.47 â1.87 â0.15 1.74

example, that both lcavol and lcp show a strong relationship with the response lpsa, and with each other. We need to ï¬t the eï¬ects jointly to untangle the relationships between the predictors and the response. We ï¬t a linear model to the log of prostate-speciï¬c antigen, lpsa, after ï¬rst standardizing the predictors to have unit variance. We randomly split the dataset into a training set of size 67 and a test set of size 30. We applied least squares estimation to the training set, producing the estimates, standard errors and Z-scores shown in Table 3.2. The Z-scores are deï¬ned in (3.12), and measure the eï¬ect of dropping that variable from the model. A Z-score greater than 2 in absolute value is approximately signiï¬cant at the 5% level. (For our example, we have nine parameters, and the 0.025 tail quantiles of the t67â9 distribution are Â±2.002!) The predictor lcavol shows the strongest eï¬ect, with lweight and svi also strong. Notice that lcp is not signiï¬cant, once lcavol is in the model (when used in a model without lcavol, lcp is strongly signiï¬cant). We can also test for the exclusion of a number of terms at once, using the F -statistic (3.13). For example, we consider dropping all the non-signiï¬cant terms in Table 3.2, namely age,

3.2 Linear Regression Models and Least Squares lcp, gleason, and pgg45. We get

51

F =

(32.81 â 29.43)/(9 â 5) = 1.67, 29.43/(67 â 9)

(3.16)

which has a p-value of 0.17 (Pr(F4,58 > 1.67) = 0.17), and hence is not signiï¬cant. The mean prediction error on the test data is 0.521. In contrast, prediction using the mean training value of lpsa has a test error of 1.057, which is called the âbase error rate.â Hence the linear model reduces the base error rate by about 50%. We will return to this example later to compare various selection and shrinkage methods.

3.2.2 The GaussâMarkov Theorem
One of the most famous results in statistics asserts that the least squares estimates of the parameters Î² have the smallest variance among all linear unbiased estimates. We will make this precise here, and also make clear that the restriction to unbiased estimates is not necessarily a wise one. This observation will lead us to consider biased estimates such as ridge regression later in the chapter. We focus on estimation of any linear combination of the parameters Î¸ = aT Î²; for example, predictions f (x0 ) = xT Î² are of this 0 form. The least squares estimate of aT Î² is Ë Ë Î¸ = aT Î² = aT (XT X)â1 XT y. (3.17)

Considering X to be ï¬xed, this is a linear function cT y of the response 0 Ë vector y. If we assume that the linear model is correct, aT Î² is unbiased since Ë E(aT Î²) = E(aT (XT X)â1 XT y) = aT (XT X)â1 XT XÎ² = aT Î².

(3.18)

The GaussâMarkov theorem states that if we have any other linear estimaË tor Î¸ = cT y that is unbiased for aT Î², that is, E(cT y) = aT Î², then Ë Var(aT Î²) â¤ Var(cT y). (3.19)

The proof (Exercise 3.3) uses the triangle inequality. For simplicity we have stated the result in terms of estimation of a single parameter aT Î², but with a few more deï¬nitions one can state it in terms of the entire parameter vector Î² (Exercise 3.3). Ë Consider the mean squared error of an estimator Î¸ in estimating Î¸: Ë Ë MSE(Î¸) = E(Î¸ â Î¸)2 Ë Ë = Var(Î¸) + [E(Î¸) â Î¸]2 .

(3.20)

52

3. Linear Methods for Regression

The ï¬rst term is the variance, while the second term is the squared bias. The Gauss-Markov theorem implies that the least squares estimator has the smallest mean squared error of all linear estimators with no bias. However, there may well exist a biased estimator with smaller mean squared error. Such an estimator would trade a little bias for a larger reduction in variance. Biased estimates are commonly used. Any method that shrinks or sets to zero some of the least squares coeï¬cients may result in a biased estimate. We discuss many examples, including variable subset selection and ridge regression, later in this chapter. From a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance. We go into these issues in more detail in Chapter 7. Mean squared error is intimately related to prediction accuracy, as discussed in Chapter 2. Consider the prediction of the new response at input x0 , Y0 = f (x0 ) + Îµ0 . Ë Ë Then the expected prediction error of an estimate f (x0 ) = xT Î² is 0 Ë Ë E(Y0 â f (x0 ))2 = Ï 2 + E(xT Î² â f (x0 ))2 0 Ë = Ï 2 + MSE(f (x0 )). (3.21)

(3.22)

Therefore, expected prediction error and mean squared error diï¬er only by the constant Ï 2 , representing the variance of the new observation y0 .

3.2.3 Multiple Regression from Simple Univariate Regression
The linear model (3.1) with p > 1 inputs is called the multiple linear regression model. The least squares estimates (3.6) for this model are best understood in terms of the estimates for the univariate (p = 1) linear model, as we indicate in this section. Suppose ï¬rst that we have a univariate model with no intercept, that is, Y = XÎ² + Îµ. The least squares estimate and residuals are Ë Î²=
N 1 xi yi , N 2 1 xi

(3.23)

(3.24)

Ë ri = yi â xi Î². In convenient vector notation, we let y = (y1 , . . . , yN )T , x = (x1 , . . . , xN )T and deï¬ne
N

x, y

=
i=1 T

xi yi , (3.25)

= x y,

3.2 Linear Regression Models and Least Squares

53

the inner product between x and y1 . Then we can write x, y Ë , Î²= x, x Ë r = y â xÎ².

(3.26)

As we will see, this simple univariate regression provides the building block for multiple linear regression. Suppose next that the inputs x1 , x2 , . . . , xp (the columns of the data matrix X) are orthogonal; that is xj , xk = 0 for all j = k. Then it is easy to check that the multiple least squares estiË mates Î²j are equal to xj , y / xj , xj âthe univariate estimates. In other words, when the inputs are orthogonal, they have no eï¬ect on each otherâs parameter estimates in the model. Orthogonal inputs occur most often with balanced, designed experiments (where orthogonality is enforced), but almost never with observational data. Hence we will have to orthogonalize them in order to carry this idea further. Suppose next that we have an intercept and a single input x. Then the least squares coeï¬cient of x has the form Ë Î²1 = x â x1, y Â¯ , x â x1, x â x1 Â¯ Â¯ (3.27)

where x = i xi /N , and 1 = x0 , the vector of N ones. We can view the Â¯ estimate (3.27) as the result of two applications of the simple regression (3.26). The steps are: 1. regress x on 1 to produce the residual z = x â x1; Â¯ Ë 2. regress y on the residual z to give the coeï¬cient Î²1 . In this procedure, âregress b on aâ means a simple univariate regression of b on a with no intercept, producing coeï¬cient Î³ = a, b / a, a and residual Ë vector b â Î³ a. We say that b is adjusted for a, or is âorthogonalizedâ with Ë respect to a. Step 1 orthogonalizes x with respect to x0 = 1. Step 2 is just a simple univariate regression, using the orthogonal predictors 1 and z. Figure 3.4 shows this process for two general inputs x1 and x2 . The orthogonalization does not change the subspace spanned by x1 and x2 , it simply produces an orthogonal basis for representing it. This recipe generalizes to the case of p inputs, as shown in Algorithm 3.1. Note that the inputs z0 , . . . , zjâ1 in step 2 are orthogonal, hence the simple regression coeï¬cients computed there are in fact also the multiple regression coeï¬cients.
1 The inner-product notation is suggestive of generalizations of linear regression to diï¬erent metric spaces, as well as to probability spaces.

54

3. Linear Methods for Regression

y

x2
z

y Ë x1
FIGURE 3.4. Least squares regression by orthogonalization of the inputs. The vector x2 is regressed on the vector x1 , leaving the residual vector z. The regression of y on z gives the multiple regression coeï¬cient of x2 . Adding together the Ë projections of y on each of x1 and z gives the least squares ï¬t y.

Algorithm 3.1 Regression by Successive Orthogonalization. 1. Initialize z0 = x0 = 1. 2. For j = 1, 2, . . . , p Regress xj on z0 , z1 , . . . , , zjâ1 to produce coeï¬cients Î³âj = Ë zâ , xj / zâ , zâ , â = 0, . . . , j â 1 and residual vector zj = jâ1 xj â k=0 Î³kj zk . Ë Ë 3. Regress y on the residual zp to give the estimate Î²p .

The result of this algorithm is zp , y Ë Î²p = . zp , zp (3.28)

Re-arranging the residual in step 2, we can see that each of the xj is a linear combination of the zk , k â¤ j. Since the zj are all orthogonal, they form a basis for the column space of X, and hence the least squares projection Ë onto this subspace is y. Since zp alone involves xp (with coeï¬cient 1), we see that the coeï¬cient (3.28) is indeed the multiple regression coeï¬cient of y on xp . This key result exposes the eï¬ect of correlated inputs in multiple regression. Note also that by rearranging the xj , any one of them could be in the last position, and a similar results holds. Hence stated more generally, we have shown that the jth multiple regression coeï¬cient is the univariate regression coeï¬cient of y on xjÂ·012...(jâ1)(j+1)...,p , the residual after regressing xj on x0 , x1 , . . . , xjâ1 , xj+1 , . . . , xp :

3.2 Linear Regression Models and Least Squares

55

Ë The multiple regression coeï¬cient Î²j represents the additional contribution of xj on y, after xj has been adjusted for x0 , x1 , . . . , xjâ1 , xj+1 , . . . , xp . If xp is highly correlated with some of the other xk âs, the residual vector Ë zp will be close to zero, and from (3.28) the coeï¬cient Î²p will be very unstable. This will be true for all the variables in the correlated set. In such situations, we might have all the Z-scores (as in Table 3.2) be smallâ any one of the set can be deletedâyet we cannot delete them all. From (3.28) we also obtain an alternate formula for the variance estimates (3.8), Ë Var(Î²p ) = Ï2 Ï2 = . zp , zp zp 2 (3.29)

Ë In other words, the precision with which we can estimate Î²p depends on the length of the residual vector zp ; this represents how much of xp is unexplained by the other xk âs. Algorithm 3.1 is known as the GramâSchmidt procedure for multiple regression, and is also a useful numerical strategy for computing the estiË mates. We can obtain from it not just Î²p , but also the entire multiple least squares ï¬t, as shown in Exercise 3.4. We can represent step 2 of Algorithm 3.1 in matrix form: X = ZÎ, (3.30)

where Z has as columns the zj (in order), and Î is the upper triangular matrix with entries Î³kj . Introducing the diagonal matrix D with jth diagonal Ë entry Djj = zj , we get X = ZDâ1 DÎ = QR,

(3.31)

the so-called QR decomposition of X. Here Q is an N Ã (p + 1) orthogonal matrix, QT Q = I, and R is a (p + 1) Ã (p + 1) upper triangular matrix. The QR decomposition represents a convenient orthogonal basis for the column space of X. It is easy to see, for example, that the least squares solution is given by Ë Î² = Râ1 QT y, Ë y = QQT y. Equation (3.32) is easy to solve because R is upper triangular (Exercise 3.4). (3.32) (3.33)

56

3. Linear Methods for Regression

3.2.4 Multiple Outputs
Suppose we have multiple outputs Y1 , Y2 , . . . , YK that we wish to predict from our inputs X0 , X1 , X2 , . . . , Xp . We assume a linear model for each output
p

Yk

= Î²0k +
j=1

Xj Î²jk + Îµk

(3.34) (3.35)

= fk (X) + Îµk . With N training cases we can write the model in matrix notation Y = XB + E.

(3.36)

Here Y is the N ÃK response matrix, with ik entry yik , X is the N Ã(p+1) input matrix, B is the (p + 1) Ã K matrix of parameters and E is the N Ã K matrix of errors. A straightforward generalization of the univariate loss function (3.2) is
K N

RSS(B) =
k=1 i=1

(yik â fk (xi ))2

(3.37) (3.38)

=

tr[(Y â XB)T (Y â XB)].

The least squares estimates have exactly the same form as before Ë B = (XT X)â1 XT Y. (3.39)

Hence the coeï¬cients for the kth outcome are just the least squares estimates in the regression of yk on x0 , x1 , . . . , xp . Multiple outputs do not aï¬ect one anotherâs least squares estimates. If the errors Îµ = (Îµ1 , . . . , ÎµK ) in (3.34) are correlated, then it might seem appropriate to modify (3.37) in favor of a multivariate version. Speciï¬cally, suppose Cov(Îµ) = Î£, then the multivariate weighted criterion
N

RSS(B; Î£) =
i=1

(yi â f (xi ))T Î£â1 (yi â f (xi ))

(3.40)

arises naturally from multivariate Gaussian theory. Here f (x) is the vector function (f1 (x), . . . , fK (x)), and yi the vector of K responses for observation i. However, it can be shown that again the solution is given by (3.39); K separate regressions that ignore the correlations (Exercise 3.11). If the Î£i vary among observations, then this is no longer the case, and the solution for B no longer decouples. In Section 3.7 we pursue the multiple outcome problem, and consider situations where it does pay to combine the regressions.

3.3 Subset Selection

57

3.3 Subset Selection
There are two reasons why we are often not satisï¬ed with the least squares estimates (3.6). â¢ The ï¬rst is prediction accuracy: the least squares estimates often have low bias but large variance. Prediction accuracy can sometimes be improved by shrinking or setting some coeï¬cients to zero. By doing so we sacriï¬ce a little bit of bias to reduce the variance of the predicted values, and hence may improve the overall prediction accuracy. â¢ The second reason is interpretation. With a large number of predictors, we often would like to determine a smaller subset that exhibit the strongest eï¬ects. In order to get the âbig picture,â we are willing to sacriï¬ce some of the small details. In this section we describe a number of approaches to variable subset selection with linear regression. In later sections we discuss shrinkage and hybrid approaches for controlling variance, as well as other dimension-reduction strategies. These all fall under the general heading model selection. Model selection is not restricted to linear models; Chapter 7 covers this topic in some detail. With subset selection we retain only a subset of the variables, and eliminate the rest from the model. Least squares regression is used to estimate the coeï¬cients of the inputs that are retained. There are a number of different strategies for choosing the subset.

3.3.1 Best-Subset Selection
Best subset regression ï¬nds for each k â {0, 1, 2, . . . , p} the subset of size k that gives smallest residual sum of squares (3.2). An eï¬cient algorithmâ the leaps and bounds procedure (Furnival and Wilson, 1974)âmakes this feasible for p as large as 30 or 40. Figure 3.5 shows all the subset models for the prostate cancer example. The lower boundary represents the models that are eligible for selection by the best-subsets approach. Note that the best subset of size 2, for example, need not include the variable that was in the best subset of size 1 (for this example all the subsets are nested). The best-subset curve (red lower boundary in Figure 3.5) is necessarily decreasing, so cannot be used to select the subset size k. The question of how to choose k involves the tradeoï¬ between bias and variance, along with the more subjective desire for parsimony. There are a number of criteria that one may use; typically we choose the smallest model that minimizes an estimate of the expected prediction error. Many of the other approaches that we discuss in this chapter are similar, in that they use the training data to produce a sequence of models varying in complexity and indexed by a single parameter. In the next section we use

58

3. Linear Methods for Regression

100

â¢

Residual SumâofâSquares

â¢ â¢ â¢ â¢ â¢ â¢

40

â¢

â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢

80

â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢

â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢

â¢

â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢

60

â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢

â¢ â¢ â¢ â¢ â¢

â¢

0 0

20

1

2

3

4 Subset Size k

5

6

7

8

FIGURE 3.5. All possible subset models for the prostate cancer example. At each subset size is shown the residual sum-of-squares for each model of that size.

cross-validation to estimate prediction error and select k; the AIC criterion is a popular alternative. We defer more detailed discussion of these and other approaches to Chapter 7.

3.3.2 Forward- and Backward-Stepwise Selection
Rather than search through all possible subsets (which becomes infeasible for p much larger than 40), we can seek a good path through them. Forwardstepwise selection starts with the intercept, and then sequentially adds into the model the predictor that most improves the ï¬t. With many candidate predictors, this might seem like a lot of computation; however, clever updating algorithms can exploit the QR decomposition for the current ï¬t to rapidly establish the next candidate (Exercise 3.9). Like best-subset regression, forward stepwise produces a sequence of models indexed by k, the subset size, which must be determined. Forward-stepwise selection is a greedy algorithm, producing a nested sequence of models. In this sense it might seem sub-optimal compared to best-subset selection. However, there are several reasons why it might be preferred:

3.3 Subset Selection

59

â¢ Computational; for large p we cannot compute the best subset sequence, but we can always compute the forward stepwise sequence (even when p â« N ). â¢ Statistical; a price is paid in variance for selecting the best subset of each size; forward stepwise is a more constrained search, and will have lower variance, but perhaps more bias.

Best Subset Forward Stepwise Backward Stepwise Forward Stagewise

Ë E||Î²(k) â Î²||2

0.65 0

0.70

0.75

0.80

0.85

0.90

0.95

5

10

15

20

25

30

Subset Size k FIGURE 3.6. Comparison of four subset-selection techniques on a simulated linear regression problem Y = X T Î² + Îµ. There are N = 300 observations on p = 31 standard Gaussian variables, with pairwise correlations all equal to 0.85. For 10 of the variables, the coeï¬cients are drawn at random from a N (0, 0.4) distribution; the rest are zero. The noise Îµ â¼ N (0, 6.25), resulting in a signal-to-noise ratio of 0.64. Results are averaged over 50 simulations. Shown is the mean-squared error Ë of the estimated coeï¬cient Î²(k) at each step from the true Î².

Backward-stepwise selection starts with the full model, and sequentially deletes the predictor that has the least impact on the ï¬t. The candidate for dropping is the variable with the smallest Z-score (Exercise 3.10). Backward selection can only be used when N > p, while forward stepwise can always be used. Figure 3.6 shows the results of a small simulation study to compare best-subset regression with the simpler alternatives forward and backward selection. Their performance is very similar, as is often the case. Included in the ï¬gure is forward stagewise regression (next section), which takes longer to reach minimum error.

60

3. Linear Methods for Regression

On the prostate cancer example, best-subset, forward and backward selection all gave exactly the same sequence of terms. Some software packages implement hybrid stepwise-selection strategies that consider both forward and backward moves at each step, and select the âbestâ of the two. For example in the R package the step function uses the AIC criterion for weighing the choices, which takes proper account of the number of parameters ï¬t; at each step an add or drop will be performed that minimizes the AIC score. Other more traditional packages base the selection on F -statistics, adding âsigniï¬cantâ terms, and dropping ânon-signiï¬cantâ terms. These are out of fashion, since they do not take proper account of the multiple testing issues. It is also tempting after a model search to print out a summary of the chosen model, such as in Table 3.2; however, the standard errors are not valid, since they do not account for the search process. The bootstrap (Section 8.2) can be useful in such settings. Finally, we note that often variables come in groups (such as the dummy variables that code a multi-level categorical predictor). Smart stepwise procedures (such as step in R) will add or drop whole groups at a time, taking proper account of their degrees-of-freedom.

3.3.3 Forward-Stagewise Regression
Forward-stagewise regression (FS) is even more constrained than forwardstepwise regression. It starts like forward-stepwise regression, with an intercept equal to y , and centered predictors with coeï¬cients initially all 0. Â¯ At each step the algorithm identiï¬es the variable most correlated with the current residual. It then computes the simple linear regression coeï¬cient of the residual on this chosen variable, and then adds it to the current coeï¬cient for that variable. This is continued till none of the variables have correlation with the residualsâi.e. the least-squares ï¬t when N > p. Unlike forward-stepwise regression, none of the other variables are adjusted when a term is added to the model. As a consequence, forward stagewise can take many more than p steps to reach the least squares ï¬t, and historically has been dismissed as being ineï¬cient. It turns out that this âslow ï¬ttingâ can pay dividends in high-dimensional problems. We see in Section 3.8.1 that both forward stagewise and a variant which is slowed down even further are quite competitive, especially in very highdimensional problems. Forward-stagewise regression is included in Figure 3.6. In this example it takes over 1000 steps to get all the correlations below 10â4 . For subset size k, we plotted the error for the last step for which there where k nonzero coeï¬cients. Although it catches up with the best ï¬t, it takes longer to do so.

3.4 Shrinkage Methods

61

3.3.4 Prostate Cancer Data Example (Continued)
Table 3.3 shows the coeï¬cients from a number of diï¬erent selection and shrinkage methods. They are best-subset selection using an all-subsets search, ridge regression, the lasso, principal components regression and partial least squares. Each method has a complexity parameter, and this was chosen to minimize an estimate of prediction error based on tenfold cross-validation; full details are given in Section 7.10. Brieï¬y, cross-validation works by dividing the training data randomly into ten equal parts. The learning method is ï¬tâfor a range of values of the complexity parameterâto nine-tenths of the data, and the prediction error is computed on the remaining one-tenth. This is done in turn for each one-tenth of the data, and the ten prediction error estimates are averaged. From this we obtain an estimated prediction error curve as a function of the complexity parameter. Note that we have already divided these data into a training set of size 67 and a test set of size 30. Cross-validation is applied to the training set, since selecting the shrinkage parameter is part of the training process. The test set is there to judge the performance of the selected model. The estimated prediction error curves are shown in Figure 3.7. Many of the curves are very ï¬at over large ranges near their minimum. Included are estimated standard error bands for each estimated error rate, based on the ten error estimates computed by cross-validation. We have used the âone-standard-errorâ ruleâwe pick the most parsimonious model within one standard error of the minimum (Section 7.10, page 244). Such a rule acknowledges the fact that the tradeoï¬ curve is estimated with error, and hence takes a conservative approach. Best-subset selection chose to use the two predictors lcvol and lweight. The last two lines of the table give the average prediction error (and its estimated standard error) over the test set.

3.4 Shrinkage Methods
By retaining a subset of the predictors and discarding the rest, subset selection produces a model that is interpretable and has possibly lower prediction error than the full model. However, because it is a discrete processâ variables are either retained or discardedâit often exhibits high variance, and so doesnât reduce the prediction error of the full model. Shrinkage methods are more continuous, and donât suï¬er as much from high variability.

3.4.1

Ridge Regression

Ridge regression shrinks the regression coeï¬cients by imposing a penalty on their size. The ridge coeï¬cients minimize a penalized residual sum of

62

3. Linear Methods for Regression
All Subsets
1.8 1.8

Ridge Regression

1.6

1.4

1.2

CV Error

CV Error

1.2

1.4

â¢

1.6

â¢ â¢ â¢ â¢ â¢
4 Degrees of Freedom

1.0

0.8

0.6

â¢
4 Subset Size

â¢
6

0.6

â¢
0

â¢
2

â¢

â¢

0.8

1.0

â¢

â¢
8

â¢

â¢
6

â¢ â¢
8

0

2

Lasso
1.8 1.8

Principal Components Regression

1.6

1.6 1.0

1.4

1.2

CV Error

1.0

â¢ â¢ â¢
0.0 0.2 0.4

CV Error

1.2

1.4

â¢

â¢

0.8

0.8

â¢

â¢

â¢

â¢
0.6

â¢
0.8

â¢

â¢
1.0

0.6

0.6

â¢

â¢
4

â¢

â¢
6

â¢

â¢
8

0

2

Shrinkage Factor s

Number of Directions

Partial Least Squares
1.8 1.6 CV Error 0.8 1.0 1.2 1.4

â¢

0.6

â¢
0

â¢
2

â¢

â¢
4

â¢

â¢
6

â¢

â¢
8

Number of Directions

FIGURE 3.7. Estimated prediction error curves and their standard errors for the various selection and shrinkage methods. Each curve is plotted as a function of the corresponding complexity parameter for that method. The horizontal axis has been chosen so that the model complexity increases as we move from left to right. The estimates of prediction error and their standard errors were obtained by tenfold cross-validation; full details are given in Section 7.10. The least complex model within one standard error of the best is chosen, indicated by the purple vertical broken lines.

3.4 Shrinkage Methods

63

TABLE 3.3. Estimated coeï¬cients and test error results, for diï¬erent subset and shrinkage methods applied to the prostate data. The blank entries correspond to variables omitted. Term Intercept lcavol lweight age lbph svi lcp gleason pgg45 Test Error Std Error LS 2.465 0.680 0.263 â0.141 0.210 0.305 â0.288 â0.021 0.267 0.521 0.179 Best Subset 2.477 0.740 0.316 Ridge 2.452 0.420 0.238 â0.046 0.162 0.227 0.000 0.040 0.133 0.492 0.165 Lasso 2.468 0.533 0.169 0.002 0.094 PCR 2.497 0.543 0.289 â0.152 0.214 0.315 â0.051 0.232 â0.056 0.449 0.105 PLS 2.452 0.419 0.344 â0.026 0.220 0.243 0.079 0.011 0.084 0.528 0.152

0.492 0.143

0.479 0.164

squares,
N p

Ë Î² ridge = argmin
Î² i=1

yi â Î²0 â

xij Î²j
j=1

2

p

+Î»
j=1

2 Î²j .

(3.41)

Here Î» â¥ 0 is a complexity parameter that controls the amount of shrinkage: the larger the value of Î», the greater the amount of shrinkage. The coeï¬cients are shrunk toward zero (and each other). The idea of penalizing by the sum-of-squares of the parameters is also used in neural networks, where it is known as weight decay (Chapter 11). An equivalent way to write the ridge problem is
N p

Ë Î² ridge = argmin
Î² i=1 p

yi â Î²0 â
2 Î²j

xij Î²j
j=1

2

, (3.42)

subject to
j=1

â¤ t,

which makes explicit the size constraint on the parameters. There is a oneto-one correspondence between the parameters Î» in (3.41) and t in (3.42). When there are many correlated variables in a linear regression model, their coeï¬cients can become poorly determined and exhibit high variance. A wildly large positive coeï¬cient on one variable can be canceled by a similarly large negative coeï¬cient on its correlated cousin. By imposing a size constraint on the coeï¬cients, as in (3.42), this problem is alleviated. The ridge solutions are not equivariant under scaling of the inputs, and so one normally standardizes the inputs before solving (3.41). In addition,

64

3. Linear Methods for Regression

notice that the intercept Î²0 has been left out of the penalty term. Penalization of the intercept would make the procedure depend on the origin chosen for Y ; that is, adding a constant c to each of the targets yi would not simply result in a shift of the predictions by the same amount c. It can be shown (Exercise 3.5) that the solution to (3.41) can be separated into two parts, after reparametrization using centered inputs: each xij gets N 1 replaced by xij â xj . We estimate Î²0 by y = N 1 yi . The remaining coÂ¯ Â¯ eï¬cients get estimated by a ridge regression without intercept, using the centered xij . Henceforth we assume that this centering has been done, so that the input matrix X has p (rather than p + 1) columns. Writing the criterion in (3.41) in matrix form, RSS(Î») = (y â XÎ²)T (y â XÎ²) + Î»Î² T Î², the ridge regression solutions are easily seen to be Ë Î² ridge = (XT X + Î»I)â1 XT y, (3.44) (3.43)

where I is the pÃp identity matrix. Notice that with the choice of quadratic penalty Î² T Î², the ridge regression solution is again a linear function of y. The solution adds a positive constant to the diagonal of XT X before inversion. This makes the problem nonsingular, even if XT X is not of full rank, and was the main motivation for ridge regression when it was ï¬rst introduced in statistics (Hoerl and Kennard, 1970). Traditional descriptions of ridge regression start with deï¬nition (3.44). We choose to motivate it via (3.41) and (3.42), as these provide insight into how it works. Figure 3.8 shows the ridge coeï¬cient estimates for the prostate cancer example, plotted as functions of df(Î»), the eï¬ective degrees of freedom implied by the penalty Î» (deï¬ned in (3.50) on page 68). In the case of orthonormal inputs, the ridge estimates are just a scaled version of the least Ë Ë squares estimates, that is, Î² ridge = Î²/(1 + Î»). Ridge regression can also be derived as the mean or mode of a posterior distribution, with a suitably chosen prior distribution. In detail, suppose yi â¼ N (Î²0 + xT Î², Ï 2 ), and the parameters Î²j are each distributed as i N (0, Ï 2 ), independently of one another. Then the (negative) log-posterior density of Î², with Ï 2 and Ï 2 assumed known, is equal to the expression in curly braces in (3.41), with Î» = Ï 2 /Ï 2 (Exercise 3.6). Thus the ridge estimate is the mode of the posterior distribution; since the distribution is Gaussian, it is also the posterior mean. The singular value decomposition (SVD) of the centered input matrix X gives us some additional insight into the nature of ridge regression. This decomposition is extremely useful in the analysis of many statistical methods. The SVD of the N Ã p matrix X has the form X = UDVT . (3.45)

3.4 Shrinkage Methods

65

â¢ lcavol 0.6 â¢ â¢â¢

â¢ â¢ â¢ 0.4 â¢ â¢ â¢ â¢ â¢ Coefficients â¢ 0.2 â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢

â¢

â¢

â¢

â¢

â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢ â¢

â¢ â¢ â¢â¢ â¢â¢ â¢ â¢ â¢â¢

â¢ svi â¢ lweight â¢ pgg45 â¢ lbph

0.0

â¢ â¢ â¢â¢

â¢ gleason

â0.2

â¢ â¢â¢ â¢ â¢ â¢ â¢ â¢

â¢ age

â¢ lcp 0 2 4 6 8

df(Î»)

FIGURE 3.8. Proï¬les of ridge coeï¬cients for the prostate cancer example, as the tuning parameter Î» is varied. Coeï¬cients are plotted versus df(Î»), the eï¬ective degrees of freedom. A vertical line is drawn at df = 5.0, the value chosen by cross-validation.

66

3. Linear Methods for Regression

Here U and V are N Ã p and p Ã p orthogonal matrices, with the columns of U spanning the column space of X, and the columns of V spanning the row space. D is a p Ã p diagonal matrix, with diagonal entries d1 â¥ d2 â¥ Â· Â· Â· â¥ dp â¥ 0 called the singular values of X. If one or more values dj = 0, X is singular. Using the singular value decomposition we can write the least squares ï¬tted vector as Ë XÎ² ls = X(XT X)â1 XT y = UUT y,

(3.46)

after some simpliï¬cation. Note that UT y are the coordinates of y with respect to the orthonormal basis U. Note also the similarity with (3.33); Q and U are generally diï¬erent orthogonal bases for the column space of X (Exercise 3.8). Now the ridge solutions are Ë XÎ² ridge = X(XT X + Î»I)â1 XT y = U D(D2 + Î»I)â1 D UT y
p

=
j=1

uj

d2 j

d2 j uT y, +Î» j

(3.47)

where the uj are the columns of U. Note that since Î» â¥ 0, we have d2 /(d2 + j j Î») â¤ 1. Like linear regression, ridge regression computes the coordinates of y with respect to the orthonormal basis U. It then shrinks these coordinates by the factors d2 /(d2 + Î»). This means that a greater amount of shrinkage j j is applied to the coordinates of basis vectors with smaller d2 . j What does a small value of d2 mean? The SVD of the centered matrix j X is another way of expressing the principal components of the variables in X. The sample covariance matrix is given by S = XT X/N , and from (3.45) we have XT X = VD2 VT , (3.48)

which is the eigen decomposition of XT X (and of S, up to a factor N ). The eigenvectors vj (columns of V) are also called the principal components (or KarhunenâLoeve) directions of X. The ï¬rst principal component direction v1 has the property that z1 = Xv1 has the largest sample variance amongst all normalized linear combinations of the columns of X. This sample variance is easily seen to be Var(z1 ) = Var(Xv1 ) = d2 1 , N (3.49)

and in fact z1 = Xv1 = u1 d1 . The derived variable z1 is called the ï¬rst principal component of X, and hence u1 is the normalized ï¬rst principal

3.4 Shrinkage Methods

67

4

Largest Principal Component o o o o o o o o o o o o o o oooo o oo o o o o o o oo o o oo o o o oo o o o o o o o o o oo o o o o oo o o o o o o o o o oo o o o o o o oo o oo o o o oo o o oo o o oo o o o oo o oo o o o o o ooo o o o o o o o o o o o o ooo o o o oo o o o o ooo oooo oo oo o o o o o oo o o o o o o oo o o o o oo o o o o o oo o o o o o o o o o o oo o o o o o Smallest Principal o Component o -4 o 2

o

X2

-2 -4

0

-2

0

2

4

X1

FIGURE 3.9. Principal components of some input data points. The largest principal component is the direction that maximizes the variance of the projected data, and the smallest principal component minimizes that variance. Ridge regression projects y onto these components, and then shrinks the coeï¬cients of the lowâ variance components more than the high-variance components.

component. Subsequent principal components zj have maximum variance d2 /N , subject to being orthogonal to the earlier ones. Conversely the last j principal component has minimum variance. Hence the small singular values dj correspond to directions in the column space of X having small variance, and ridge regression shrinks these directions the most. Figure 3.9 illustrates the principal components of some data points in two dimensions. If we consider ï¬tting a linear surface over this domain (the Y -axis is sticking out of the page), the conï¬guration of the data allow us to determine its gradient more accurately in the long direction than the short. Ridge regression protects against the potentially high variance of gradients estimated in the short directions. The implicit assumption is that the response will tend to vary most in the directions of high variance of the inputs. This is often a reasonable assumption, since predictors are often chosen for study because they vary with the response variable, but need not hold in general.

68

3. Linear Methods for Regression

In Figure 3.7 we have plotted the estimated prediction error versus the quantity df(Î») = tr[X(XT X + Î»I)â1 XT ], = tr(HÎ» )
p

=
j=1

d2 j . d2 + Î» j

(3.50)

This monotone decreasing function of Î» is the eï¬ective degrees of freedom of the ridge regression ï¬t. Usually in a linear-regression ï¬t with p variables, the degrees-of-freedom of the ï¬t is p, the number of free parameters. The idea is that although all p coeï¬cients in a ridge ï¬t will be non-zero, they are ï¬t in a restricted fashion controlled by Î». Note that df(Î») = p when Î» = 0 (no regularization) and df(Î») â 0 as Î» â â. Of course there is always an additional one degree of freedom for the intercept, which was removed apriori. This deï¬nition is motivated in more detail in Section 3.4.4 and Sections 7.4â7.6. In Figure 3.7 the minimum occurs at df(Î») = 5.0. Table 3.3 shows that ridge regression reduces the test error of the full least squares estimates by a small amount.

3.4.2 The Lasso
The lasso is a shrinkage method like ridge, with subtle but important differences. The lasso estimate is deï¬ned by
N p

Ë Î² lasso

=

argmin
Î² i=1

yi â Î²0 â
p

xij Î²j
j=1

2

subject to
j=1

|Î²j | â¤ t.

(3.51)

Just as in ridge regression, we can re-parametrize the constant Î²0 by stanË dardizing the predictors; the solution for Î²0 is y , and thereafter we ï¬t a Â¯ model without an intercept (Exercise 3.5). In the signal processing literature, the lasso is also known as basis pursuit (Chen et al., 1998). We can also write the lasso problem in the equivalent Lagrangian form 1 Ë Î² lasso = argmin 2 Î²
N i=1 p

yi â Î²0 â

xij Î²j
j=1

2

p

+Î»
j=1

|Î²j | .

(3.52)

Notice the similarity to the ridge regression problem (3.42) or (3.41): the p 2 p L2 ridge penalty 1 Î²j is replaced by the L1 lasso penalty 1 |Î²j |. This latter constraint makes the solutions nonlinear in the yi , and there is no closed form expression as in ridge regression. Computing the lasso solution

3.4 Shrinkage Methods

69

is a quadratic programming problem, although we see in Section 3.4.4 that eï¬cient algorithms are available for computing the entire path of solutions as Î» is varied, with the same computational cost as for ridge regression. Because of the nature of the constraint, making t suï¬ciently small will cause some of the coeï¬cients to be exactly zero. Thus the lasso does a kind p Ë of continuous subset selection. If t is chosen larger than t0 = 1 |Î²j | (where ls Ë Ë Ë Î²j = Î²j , the least squares estimates), then the lasso estimates are the Î²j âs. On the other hand, for t = t0 /2 say, then the least squares coeï¬cients are shrunk by about 50% on average. However, the nature of the shrinkage is not obvious, and we investigate it further in Section 3.4.4 below. Like the subset size in variable subset selection, or the penalty parameter in ridge regression, t should be adaptively chosen to minimize an estimate of expected prediction error. In Figure 3.7, for ease of interpretation, we have plotted the lasso prep Ë diction error estimates versus the standardized parameter s = t/ 1 |Î²j |. A value s â 0.36 was chosen by 10-fold cross-validation; this caused four Ë coeï¬cients to be set to zero (ï¬fth column of Table 3.3). The resulting model has the second lowest test error, slightly lower than the full least squares model, but the standard errors of the test error estimates (last line of Table 3.3) are fairly large. Figure 3.10 shows the lasso coeï¬cients as the standardized tuning pap Ë rameter s = t/ 1 |Î²j | is varied. At s = 1.0 these are the least squares estimates; they decrease to 0 as s â 0. This decrease is not always strictly monotonic, although it is in this example. A vertical line is drawn at s = 0.36, the value chosen by cross-validation.

3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso
In this section we discuss and compare the three approaches discussed so far for restricting the linear regression model: subset selection, ridge regression and the lasso. In the case of an orthonormal input matrix X the three procedures have explicit solutions. Each method applies a simple transformation to the least Ë squares estimate Î²j , as detailed in Table 3.4. Ridge regression does a proportional shrinkage. Lasso translates each coeï¬cient by a constant factor Î», truncating at zero. This is called âsoft thresholding,â and is used in the context of wavelet-based smoothing in Section 5.9. Best-subset selection drops all variables with coeï¬cients smaller than the M th largest; this is a form of âhard-thresholding.â Back to the nonorthogonal case; some pictures help understand their relationship. Figure 3.11 depicts the lasso (left) and ridge regression (right) when there are only two parameters. The residual sum of squares has elliptical contours, centered at the full least squares estimate. The constraint

70

3. Linear Methods for Regression

lcavol

0.4

0.6

Coefficients

svi lweight pgg45 0.2 lbph

0.0

gleason

age â0.2

lcp

0.0

0.2

0.4

0.6

0.8

1.0

Shrinkage Factor s

FIGURE 3.10. Proï¬les of lasso coeï¬cients, as the tuning parameter t is varied. P Ë Coeï¬cients are plotted versus s = t/ p |Î²j |. A vertical line is drawn at s = 0.36, 1 the value chosen by cross-validation. Compare Figure 3.8 on page 65; the lasso proï¬les hit zero, while those for ridge do not. The proï¬les are piece-wise linear, and so are computed only at the points displayed; see Section 3.4.4 for details.

3.4 Shrinkage Methods

71

TABLE 3.4. Estimators of Î²j in the case of orthonormal columns of X. M and Î» are constants chosen by the corresponding techniques; sign denotes the sign of its argument (Â±1), and x+ denotes âpositive partâ of x. Below the table, estimators are shown by broken red lines. The 45â¦ line in gray shows the unrestricted estimate for reference.

Estimator Best subset (size M ) Ridge Lasso
Best Subset Ridge

Formula Ë Ë Ë Î²j Â· I(|Î²j | â¥ |Î²(M ) |) Ë Î²j /(1 + Î») Ë Ë sign(Î²j )(|Î²j | â Î»)+
Lasso Î»

Ë |Î²(M ) |
(0,0) (0,0) (0,0)

Î²2

^ Î²

.

Î²2

^ Î²

.

Î²1

Î²1

FIGURE 3.11. Estimation picture for the lasso (left) and ridge regression (right). Shown are contours of the error and constraint functions. The solid blue 2 2 areas are the constraint regions |Î²1 | + |Î²2 | â¤ t and Î²1 + Î²2 â¤ t2 , respectively, while the red ellipses are the contours of the least squares error function.

72

3. Linear Methods for Regression

2 2 region for ridge regression is the disk Î²1 + Î²2 â¤ t, while that for lasso is the diamond |Î²1 | + |Î²2 | â¤ t. Both methods ï¬nd the ï¬rst point where the elliptical contours hit the constraint region. Unlike the disk, the diamond has corners; if the solution occurs at a corner, then it has one parameter Î²j equal to zero. When p > 2, the diamond becomes a rhomboid, and has many corners, ï¬at edges and faces; there are many more opportunities for the estimated parameters to be zero. We can generalize ridge regression and the lasso, and view them as Bayes estimates. Consider the criterion N p

Ë Î² = argmin
Î² i=1

yi â Î²0 â

xij Î²j
j=1

2

p

+Î»
j=1

|Î²j |q

(3.53)

for q â¥ 0. The contours of constant value of j |Î²j |q are shown in Figure 3.12, for the case of two inputs. Thinking of |Î²j |q as the log-prior density for Î²j , these are also the equicontours of the prior distribution of the parameters. The value q = 0 corresponds to variable subset selection, as the penalty simply counts the number of nonzero parameters; q = 1 corresponds to the lasso, while q = 2 to ridge regression. Notice that for q â¤ 1, the prior is not uniform in direction, but concentrates more mass in the coordinate directions. The prior corresponding to the q = 1 case is an independent double exponential (or Laplace) distribution for each input, with density (1/2Ï ) exp(â|Î²|/Ï ) and Ï = 1/Î». The case q = 1 (lasso) is the smallest q such that the constraint region is convex; non-convex constraint regions make the optimization problem more diï¬cult. In this view, the lasso, ridge regression and best subset selection are Bayes estimates with diï¬erent priors. Note, however, that they are derived as posterior modes, that is, maximizers of the posterior. It is more common to use the mean of the posterior as the Bayes estimate. Ridge regression is also the posterior mean, but the lasso and best subset selection are not. Looking again at the criterion (3.53), we might try using other values of q besides 0, 1, or 2. Although one might consider estimating q from the data, our experience is that it is not worth the eï¬ort for the extra variance incurred. Values of q â (1, 2) suggest a compromise between the lasso and ridge regression. Although this is the case, with q > 1, |Î²j |q is diï¬erentiable at 0, and so does not share the ability of lasso (q = 1) for
q=4 q=2 q=1 q = 0.5 q = 0.1

FIGURE 3.12. Contours of constant value of

P

j

|Î²j |q for given values of q.

3.4 Shrinkage Methods
q = 1.2 Î± = 0.2

73

Lq

Elastic Net

P FIGURE 3.13. Contours of constant value of j |Î²j |q for q = 1.2 (left plot), P 2 and the elastic-net penalty j (Î±Î²j +(1âÎ±)|Î²j |) for Î± = 0.2 (right plot). Although visually very similar, the elastic-net has sharp (non-diï¬erentiable) corners, while the q = 1.2 penalty does not.

setting coeï¬cients exactly to zero. Partly for this reason as well as for computational tractability, Zou and Hastie (2005) introduced the elasticnet penalty
p

Î»
j=1

2 Î±Î²j + (1 â Î±)|Î²j | ,

(3.54)

a diï¬erent compromise between ridge and lasso. Figure 3.13 compares the Lq penalty with q = 1.2 and the elastic-net penalty with Î± = 0.2; it is hard to detect the diï¬erence by eye. The elastic-net selects variables like the lasso, and shrinks together the coeï¬cients of correlated predictors like ridge. It also has considerable computational advantages over the Lq penalties. We discuss the elastic-net further in Section 18.4.

3.4.4 Least Angle Regression
Least angle regression (LAR) is a relative newcomer (Efron et al., 2004), and can be viewed as a kind of âdemocraticâ version of forward stepwise regression (Section 3.3.2). As we will see, LAR is intimately connected with the lasso, and in fact provides an extremely eï¬cient algorithm for computing the entire lasso path as in Figure 3.10. Forward stepwise regression builds a model sequentially, adding one variable at a time. At each step, it identiï¬es the best variable to include in the active set, and then updates the least squares ï¬t to include all the active variables. Least angle regression uses a similar strategy, but only enters âas muchâ of a predictor as it deserves. At the ï¬rst step it identiï¬es the variable most correlated with the response. Rather than ï¬t this variable completely, LAR moves the coeï¬cient of this variable continuously toward its leastsquares value (causing its correlation with the evolving residual to decrease in absolute value). As soon as another variable âcatches upâ in terms of correlation with the residual, the process is paused. The second variable then joins the active set, and their coeï¬cients are moved together in a way that keeps their correlations tied and decreasing. This process is continued

74

3. Linear Methods for Regression

until all the variables are in the model, and ends at the full least-squares ï¬t. Algorithm 3.2 provides the details. The termination condition in step 5 requires some explanation. If p > N â 1, the LAR algorithm reaches a zero residual solution after N â 1 steps (the â1 is because we have centered the data). Algorithm 3.2 Least Angle Regression. 1. Standardize the predictors to have mean zero and unit norm. Start Â¯ with the residual r = y â y, Î²1 , Î²2 , . . . , Î²p = 0. 2. Find the predictor xj most correlated with r. 3. Move Î²j from 0 towards its least-squares coeï¬cient xj , r , until some other competitor xk has as much correlation with the current residual as does xj . 4. Move Î²j and Î²k in the direction deï¬ned by their joint least squares coeï¬cient of the current residual on (xj , xk ), until some other competitor xl has as much correlation with the current residual. 5. Continue in this way until all p predictors have been entered. After min(N â 1, p) steps, we arrive at the full least-squares solution. Suppose Ak is the active set of variables at the beginning of the kth step, and let Î²Ak be the coeï¬cient vector for these variables at this step; there will be k â 1 nonzero values, and the one just entered will be zero. If rk = y â XAk Î²Ak is the current residual, then the direction for this step is Î´k = (XT k XAk )â1 XT k rk . A A (3.55) The coeï¬cient proï¬le then evolves as Î²Ak (Î±) = Î²Ak + Î± Â· Î´k . Exercise 3.23 veriï¬es that the directions chosen in this fashion do what is claimed: keep the correlations tied and decreasing. If the ï¬t vector at the beginning of this step is Ëk , then it evolves as Ëk (Î±) = Ëk + Î± Â· uk , where uk = XAk Î´k f f f is the new ï¬t direction. The name âleast angleâ arises from a geometrical interpretation of this process; uk makes the smallest (and equal) angle with each of the predictors in Ak (Exercise 3.24). Figure 3.14 shows the absolute correlations decreasing and joining ranks with each step of the LAR algorithm, using simulated data. By construction the coeï¬cients in LAR change in a piecewise linear fashion. Figure 3.15 [left panel] shows the LAR coeï¬cient proï¬le evolving as a function of their L1 arc length 2 . Note that we do not need to take small
L1 arc-length of a diï¬erentiable curve Î²(s) for s â [0, S] is given by TV(Î², S) = Ë Ë ||Î²(s)||1 ds, where Î²(s) = âÎ²(s)/âs. For the piecewise-linear LAR coeï¬cient proï¬le, 0 this amounts to summing the L1 norms of the changes in coeï¬cients from step to step. RS
2 The

3.4 Shrinkage Methods

75

v2

v6

v4

v5

v3

v1

Absolute Correlations

0.0 0

0.1

0.2

0.3

0.4

5

10

15

L1 Arc Length

FIGURE 3.14. Progression of the absolute correlations during each step of the LAR procedure, using a simulated data set with six predictors. The labels at the top of the plot indicate which variables enter the active set at each step. The step length are measured in units of L1 arc length.

Least Angle Regression

Lasso

0.5

0.0

Coeï¬cients

Coeï¬cients
0 5 10 15

â0.5

â1.0

â1.5

â1.5 0

â1.0

â0.5

0.0

0.5

5

10

15

L1 Arc Length

L1 Arc Length

FIGURE 3.15. Left panel shows the LAR coeï¬cient proï¬les on the simulated data, as a function of the L1 arc length. The right panel shows the Lasso proï¬le. They are identical until the dark-blue coeï¬cient crosses zero at an arc length of about 18.

76

3. Linear Methods for Regression

steps and recheck the correlations in step 3; using knowledge of the covariance of the predictors and the piecewise linearity of the algorithm, we can work out the exact step length at the beginning of each step (Exercise 3.25). The right panel of Figure 3.15 shows the lasso coeï¬cient proï¬les on the same data. They are almost identical to those in the left panel, and diï¬er for the ï¬rst time when the blue coeï¬cient passes back through zero. For the prostate data, the LAR coeï¬cient proï¬le turns out to be identical to the lasso proï¬le in Figure 3.10, which never crosses zero. These observations lead to a simple modiï¬cation of the LAR algorithm that gives the entire lasso path, which is also piecewise-linear. Algorithm 3.2a Least Angle Regression: Lasso Modiï¬cation. 4a. If a non-zero coeï¬cient hits zero, drop its variable from the active set of variables and recompute the current joint least squares direction. The LAR(lasso) algorithm is extremely eï¬cient, requiring the same order of computation as that of a single least squares ï¬t using the p predictors. Least angle regression always takes p steps to get to the full least squares estimates. The lasso path can have more than p steps, although the two are often quite similar. Algorithm 3.2 with the lasso modiï¬cation 3.2a is an eï¬cient way of computing the solution to any lasso problem, especially when p â« N . Osborne et al. (2000a) also discovered a piecewise-linear path for computing the lasso, which they called a homotopy algorithm. We now give a heuristic argument for why these procedures are so similar. Although the LAR algorithm is stated in terms of correlations, if the input features are standardized, it is equivalent and easier to work with innerproducts. Suppose A is the active set of variables at some stage in the algorithm, tied in their absolute inner-product with the current residuals y â XÎ². We can express this as xT (y â XÎ²) = Î³ Â· sj , âj â A j (3.56)

where sj â {â1, 1} indicates the sign of the inner-product, and Î³ is the common value. Also |xT (y â XÎ²)| â¤ Î³ âk â A. Now consider the lasso k criterion (3.52), which we write in vector form
1 R(Î²) = 2 ||y â XÎ²||2 + Î»||Î²||1 . 2

(3.57)

Let B be the active set of variables in the solution for a given value of Î». For these variables R(Î²) is diï¬erentiable, and the stationarity conditions give (3.58) xT (y â XÎ²) = Î» Â· sign(Î²j ), âj â B j Comparing (3.58) with (3.56), we see that they are identical only if the sign of Î²j matches the sign of the inner product. That is why the LAR

3.4 Shrinkage Methods

77

algorithm and lasso start to diï¬er when an active coeï¬cient passes through zero; condition (3.58) is violated for that variable, and it is kicked out of the active set B. Exercise 3.23 shows that these equations imply a piecewiselinear coeï¬cient proï¬le as Î» decreases. The stationarity conditions for the non-active variables require that |xT (y â XÎ²)| â¤ Î», âk â B, k (3.59)

which again agrees with the LAR algorithm. Figure 3.16 compares LAR and lasso to forward stepwise and stagewise regression. The setup is the same as in Figure 3.6 on page 59, except here N = 100 here rather than 300, so the problem is more diï¬cult. We see that the more aggressive forward stepwise starts to overï¬t quite early (well before the 10 true variables can enter the model), and ultimately performs worse than the slower forward stagewise regression. The behavior of LAR and lasso is similar to that of forward stagewise regression. Incremental forward stagewise is similar to LAR and lasso, and is described in Section 3.8.1. Degrees-of-Freedom Formula for LAR and Lasso Suppose that we ï¬t a linear model via the least angle regression procedure, stopping at some number of steps k < p, or equivalently using a lasso bound t that produces a constrained version of the full least squares ï¬t. How many parameters, or âdegrees of freedomâ have we used? Consider ï¬rst a linear regression using a subset of k features. If this subset is prespeciï¬ed in advance without reference to the training data, then the degrees of freedom used in the ï¬tted model is deï¬ned to be k. Indeed, in classical statistics, the number of linearly independent parameters is what is meant by âdegrees of freedom.â Alternatively, suppose that we carry out a best subset selection to determine the âoptimalâ set of k predictors. Then the resulting model has k parameters, but in some sense we have used up more than k degrees of freedom. We need a more general deï¬nition for the eï¬ective degrees of freedom of an adaptively ï¬tted model. We deï¬ne the degrees of freedom of the ï¬tted Ë vector y = (Ë1 , y2 , . . . , yN ) as y Ë Ë df(Ë ) = y 1 Ï2
N

Cov(Ëi , yi ). y
i=1

(3.60)

Here Cov(Ëi , yi ) refers to the sampling covariance between the predicted y value yi and its corresponding outcome value yi . This makes intuitive sense: Ë the harder that we ï¬t to the data, the larger this covariance and hence df(Ë ). Expression (3.60) is a useful notion of degrees of freedom, one that y Ë can be applied to any model prediction y. This includes models that are

78

3. Linear Methods for Regression

Ë E||Î²(k) â Î²||2

0.55

0.60

0.65

Forward Stepwise LAR Lasso Forward Stagewise Incremental Forward Stagewise

0.0

0.2

0.4

0.6

0.8

1.0

Fraction of L1 arc-length FIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward stagewise (FS) and incremental forward stagewise (FS0 ) regression. The setup is the same as in Figure 3.6, except N = 100 here rather than 300. Here the slower FS regression ultimately outperforms forward stepwise. LAR and lasso show similar behavior to FS and FS0 . Since the procedures take diï¬erent numbers of steps (across simulation replicates and methods), we plot the MSE as a function of the fraction of total L1 arc-length toward the least-squares ï¬t.

adaptively ï¬tted to the training data. This deï¬nition is motivated and discussed further in Sections 7.4â7.6. Now for a linear regression with k ï¬xed predictors, it is easy to show that df(Ë ) = k. Likewise for ridge regression, this deï¬nition leads to the y closed-form expression (3.50) on page 68: df(Ë ) = tr(SÎ» ). In both these y Ë cases, (3.60) is simple to evaluate because the ï¬t y = HÎ» y is linear in y. If we think about deï¬nition (3.60) in the context of a best subset selection of size k, it seems clear that df(Ë ) will be larger than k, and this can be y veriï¬ed by estimating Cov(Ëi , yi )/Ï 2 directly by simulation. However there y is no closed form method for estimating df(Ë ) for best subset selection. y For LAR and lasso, something magical happens. These techniques are adaptive in a smoother way than best subset selection, and hence estimation of degrees of freedom is more tractable. Speciï¬cally it can be shown that after the kth step of the LAR procedure, the eï¬ective degrees of freedom of the ï¬t vector is exactly k. Now for the lasso, the (modiï¬ed) LAR procedure

3.5 Methods Using Derived Input Directions

79

often takes more than p steps, since predictors can drop out. Hence the deï¬nition is a little diï¬erent; for the lasso, at any stage df(Ë ) approximately y equals the number of predictors in the model. While this approximation works reasonably well anywhere in the lasso path, for each k it works best at the last model in the sequence that contains k predictors. A detailed study of the degrees of freedom for the lasso may be found in Zou et al. (2007).

3.5 Methods Using Derived Input Directions
In many situations we have a large number of inputs, often very correlated. The methods in this section produce a small number of linear combinations Zm , m = 1, . . . , M of the original inputs Xj , and the Zm are then used in place of the Xj as inputs in the regression. The methods diï¬er in how the linear combinations are constructed.

3.5.1 Principal Components Regression
In this approach the linear combinations Zm used are the principal components as deï¬ned in Section 3.4.1 above. Principal component regression forms the derived input columns zm = Xvm , and then regresses y on z1 , z2 , . . . , zM for some M â¤ p. Since the zm are orthogonal, this regression is just a sum of univariate regressions:
M

Ë pcr y(M ) = y 1 + Â¯
m=1

Ë Î¸ m zm ,

(3.61)

Ë where Î¸m = zm , y / zm , zm . Since the zm are each linear combinations of the original xj , we can express the solution (3.61) in terms of coeï¬cients of the xj (Exercise 3.13):
M

Ë Î² pcr (M ) =
m=1

Ë Î¸ m vm .

(3.62)

As with ridge regression, principal components depend on the scaling of the inputs, so typically we ï¬rst standardize them. Note that if M = p, we would just get back the usual least squares estimates, since the columns of Z = UD span the column space of X. For M < p we get a reduced regression. We see that principal components regression is very similar to ridge regression: both operate via the principal components of the input matrix. Ridge regression shrinks the coeï¬cients of the principal components (Figure 3.17), shrinking more depending on the size of the corresponding eigenvalue; principal components regression discards the p â M smallest eigenvalue components. Figure 3.17 illustrates this.

80

3. Linear Methods for Regression
1.0

0.8

â¢ â¢

â¢ â¢
ridge pcr

â¢ â¢

â¢

â¢

â¢

â¢

Shrinkage Factor

0.6

â¢

0.4

â¢

â¢

â¢ â¢

0.0

0.2

â¢ â¢
8

2

4 Index

6

FIGURE 3.17. Ridge regression shrinks the regression coeï¬cients of the principal components, using shrinkage factors d2 /(d2 + Î») as in (3.47). Principal j j component regression truncates them. Shown are the shrinkage and truncation patterns corresponding to Figure 3.7, as a function of the principal component index.

In Figure 3.7 we see that cross-validation suggests seven terms; the resulting model has the lowest test error in Table 3.3.

3.5.2 Partial Least Squares
This technique also constructs a set of linear combinations of the inputs for regression, but unlike principal components regression it uses y (in addition to X) for this construction. Like principal component regression, partial least squares (PLS) is not scale invariant, so we assume that each xj is standardized to have mean 0 and variance 1. PLS begins by computing Ï1j = xj , y for each j. From this we construct the derived input Ë z1 = Ë j Ï1j xj , which is the ï¬rst partial least squares direction. Hence in the construction of each zm , the inputs are weighted by the strength of their univariate eï¬ect on y3 . The outcome y is regressed on z1 giving Ë coeï¬cient Î¸1 , and then we orthogonalize x1 , . . . , xp with respect to z1 . We continue this process, until M â¤ p directions have been obtained. In this manner, partial least squares produces a sequence of derived, orthogonal inputs or directions z1 , z2 , . . . , zM . As with principal-component regression, if we were to construct all M = p directions, we would get back a solution equivalent to the usual least squares estimates; using M < p directions produces a reduced regression. The procedure is described fully in Algorithm 3.3.
3 Since the x are standardized, the ï¬rst directions Ï Ë1j are the univariate regression j coeï¬cients (up to an irrelevant constant); this is not the case for subsequent directions.

3.5 Methods Using Derived Input Directions

81

Algorithm 3.3 Partial Least Squares. Ë 1. Standardize each xj to have mean zero and variance one. Set y(0) = (0) y 1, and xj = xj , j = 1, . . . , p. Â¯ 2. For m = 1, 2, . . . , p (a) zm =
p j=1

Ïmj xj Ë

(mâ1)

, where Ïmj = xj Ë

(mâ1)

,y .

Ë (b) Î¸m = zm , y / zm , zm . Ë Ë Ë (c) y(m) = y(mâ1) + Î¸m zm . (d) Orthogonalize each xj [
(mâ1) z m , xj (mâ1)

with respect to zm : xj

(m)

= xj

(mâ1)

/ zm , zm ]zm , j = 1, 2, . . . , p.

â

3. Output the sequence of ï¬tted vectors {Ë (m) }p . Since the {zâ }m are y 1 1 (m) Ë Ë linear in the original xj , so is y = XÎ² pls (m). These linear coeï¬cients can be recovered from the sequence of PLS transformations.

In the prostate cancer example, cross-validation chose M = 2 PLS directions in Figure 3.7. This produced the model given in the rightmost column of Table 3.3. What optimization problem is partial least squares solving? Since it uses the response y to construct its directions, its solution path is a nonlinear function of y. It can be shown (Exercise 3.15) that partial least squares seeks directions that have high variance and have high correlation with the response, in contrast to principal components regression which keys only on high variance (Stone and Brooks, 1990; Frank and Friedman, 1993). In particular, the mth principal component direction vm solves: maxÎ± Var(XÎ±) subject to ||Î±|| = 1, Î±T Svâ = 0, â = 1, . . . , m â 1, (3.63)

where S is the sample covariance matrix of the xj . The conditions Î±T Svâ = 0 ensures that zm = XÎ± is uncorrelated with all the previous linear combinations zâ = Xvâ . The mth PLS direction Ïm solves: Ë maxÎ± Corr2 (y, XÎ±)Var(XÎ±) subject to ||Î±|| = 1, Î±T SÏâ = 0, â = 1, . . . , m â 1. Ë (3.64)

Further analysis reveals that the variance aspect tends to dominate, and so partial least squares behaves much like ridge regression and principal components regression. We discuss this further in the next section. If the input matrix X is orthogonal, then partial least squares ï¬nds the least squares estimates after m = 1 steps. Subsequent steps have no eï¬ect

